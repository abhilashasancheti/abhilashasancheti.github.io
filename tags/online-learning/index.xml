<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Online learning on Desh Raj</title>
    <link>https://desh2608.github.io/tags/online-learning/</link>
    <description>Recent content in Online learning on Desh Raj</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 14 Mar 2018 13:40:57 +0530</lastBuildDate>
    
	<atom:link href="https://desh2608.github.io/tags/online-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Online Learning of Word Embeddings</title>
      <link>https://desh2608.github.io/post/online-learning-word-embeddings/</link>
      <pubDate>Wed, 14 Mar 2018 13:40:57 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/online-learning-word-embeddings/</guid>
      <description>Word vectors have become the building blocks for all natural language processing systems. I have earlier written an overview of popular algorithms for learning word embeddings here. One limitation with all these methods (namely SVD, skip-gram, and GloVe) is that they are all “batch” techniques. In this post, I will discuss two recent papers (which are very similar but were developed independently) which aim to provide an online approximation for the skip-gram algorithm.</description>
    </item>
    
    <item>
      <title>Sparsity in Online Learning with Lasso Regularization</title>
      <link>https://desh2608.github.io/post/sparse-online-learning-lasso-regularization/</link>
      <pubDate>Sat, 24 Feb 2018 13:40:42 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/sparse-online-learning-lasso-regularization/</guid>
      <description>Sparse vectors have become popular recently for 2 reasons:
 Sparse matrices require much less storage since they can be stored using various space-saving methods. Sparse vectors are much more interpretable than dense vectors. For instance, the non-zero non-negative components of a sparse word vector may be taken to denote the weights for certain features. In contrast, there is no interpretation for a value like $-0.1347$.  Sparsity is often induced through the use of L1 (or Lasso) regularization.</description>
    </item>
    
  </channel>
</rss>