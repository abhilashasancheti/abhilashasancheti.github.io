<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Abhilasha Sancheti on Abhilasha Sancheti</title>
    <link>https://abhilashasancheti.github.io/</link>
    <description>Recent content in Abhilasha Sancheti on Abhilasha Sancheti</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Tue, 03 Jul 2018 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Award-winning classic papers in ML and NLP</title>
      <link>https://abhilashasancheti.github.io/post/classic-papers/</link>
      <pubDate>Thu, 30 Aug 2018 19:57:44 -0400</pubDate>
      
      <guid>https://abhilashasancheti.github.io/post/classic-papers/</guid>
      <description>

&lt;p&gt;I was trying to find a consolidated list of papers in machine learning (ICML, NIPS, AAAI, SIGIR) and natural language processing (ACL, EMNLP, NAACL) published after 2000, which are held in some regard, perhaps by winning prizes such as Test-of-time paper at these major conferences. However, there seems to be no such list, or if it is, it&amp;rsquo;s hidden too deep and it may just be quicker to prepare a similar list of my own. I will add the papers in reverse chronological order of their publication year.&lt;/p&gt;

&lt;h2 id=&#34;2009&#34;&gt;2009&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.aclweb.org/anthology/E/E09/E09-1081.pdf&#34; target=&#34;_blank&#34;&gt;A General, Abstract Model of Incremental Dialogue Processing&lt;/a&gt;. David Schlangen and Gabriel Skantze. EACL 2009. &lt;em&gt;Honorable mention at NAACL 2018&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;2008&#34;&gt;2008&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf&#34; target=&#34;_blank&#34;&gt;A unified architecture for natural language processing: deep neural networks with multitask learning&lt;/a&gt;. Ronan Collobert and Jason Weston. ICML 2008. &lt;em&gt;Test-of-time award at ICML 2018&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.aclweb.org/anthology/D08-1027&#34; target=&#34;_blank&#34;&gt;Cheap and Fast—But is it Good?: Evaluating Non-Expert Annotations for Natural Language Tasks&lt;/a&gt;. Snow, O&amp;rsquo;Connor, Jurafsky, and Ng. EMNLP 2008. &lt;em&gt;Honorable mention at NAACL 2018&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.aclweb.org/anthology/J/J08/J08-1001.pdf&#34; target=&#34;_blank&#34;&gt;Modeling Local Coherence: An entity-based approach&lt;/a&gt;. Regina Barzilay and Mirella Lapata. Transactions of ACL (2008). &lt;em&gt;Honorable mention at NAACL 2018&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;2007&#34;&gt;2007&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf&#34; target=&#34;_blank&#34;&gt;Random features for large scale kernel machines&lt;/a&gt;. Ali Rahimi and Ben Recht. NIPS 2007. &lt;em&gt;Test-of-time award at NIPS 2017&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Applications_files/combining_uct.pdf&#34; target=&#34;_blank&#34;&gt;Combining Online and Offline Knowledge in UCT&lt;/a&gt;. Sylvain Gelly and David Silver. ICML 2007. &lt;em&gt;Test-of-time award at ICML 2017&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://ttic.uchicago.edu/~nati/Publications/PegasosMPB.pdf&#34; target=&#34;_blank&#34;&gt;Pegasos: Primal estimated sub-gradient solver for SVM&lt;/a&gt;. Shalev-Shwartz et al. ICML 2007. &lt;em&gt;Honorable mention at ICML 2017&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.83.719&amp;amp;rep=rep1&amp;amp;type=pdf&#34; target=&#34;_blank&#34;&gt;A Bound on the Label Complexity of Agnostic Active Learning&lt;/a&gt;. Steve Hanneke. ICML 2007. &lt;em&gt;Honorable mention at ICML 2017&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://aclweb.org/anthology/J/J09/J09-4008.pdf&#34; target=&#34;_blank&#34;&gt;An Investigation into the Validity of Some Metrics for Automatically Evaluating Natural Language Generation Systems&lt;/a&gt;. Ehud Reiter and Anja Belz. Transactions of ACL 2009. &lt;em&gt;Honorable mention at NAACL 2018&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.aclweb.org/anthology/P07-1033&#34; target=&#34;_blank&#34;&gt;Frustratingly Easy Domain Adaptation&lt;/a&gt;. Hal Daume III. ACL 2007. &lt;em&gt;Honorable mention at NAACL 2018&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;2006&#34;&gt;2006&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://icml.cc/2016/awards/dtm.pdf&#34; target=&#34;_blank&#34;&gt;Dynamic topic models&lt;/a&gt;. David Blei and John Lafferty. ICML 2006. &lt;em&gt;Test-of-time award at ICML 2016&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://dl.acm.org/citation.cfm?id=1148177&#34; target=&#34;_blank&#34;&gt;Improving web search ranking by incorporating user behavior information&lt;/a&gt;. Agichtein et al. SIGIR 2006. &lt;em&gt;Test-of-time award at SIGIR 2018&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;2005&#34;&gt;2005&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://icml.cc/2015/wp-content/uploads/2015/06/icml_ranking.pdf&#34; target=&#34;_blank&#34;&gt;Learning to Rank Using Gradient Descent&lt;/a&gt;. Burges et al. ICML 2005. &lt;em&gt;Test-of-time award at ICML 2015&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.aclweb.org/anthology/H/H05/H05-1044.pdf&#34; target=&#34;_blank&#34;&gt;Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis&lt;/a&gt;. Wilson, Weibi, and Hoffman. EMNLP 2005. &lt;em&gt;Honorable mention at NAACL 2018&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;2004&#34;&gt;2004&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.di.ens.fr/~fbach/skm_icml.pdf&#34; target=&#34;_blank&#34;&gt;Multiple kernel learning, conic duality, and the SMO algorithm&lt;/a&gt;. Michael Jordan&amp;rsquo;s group. ICML 2004. &lt;em&gt;10 year paper award at ICML 2014&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://l2r.cs.uiuc.edu/~danr/Papers/RothYi04.pdf&#34; target=&#34;_blank&#34;&gt;A Linear Programming Formulation for Global Inference in Natural Language Tasks&lt;/a&gt;. Dan Roth and Wen-tau Yih. CoNLL 2004. &lt;em&gt;Honorable mention at NAACL 2018&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.cs.columbia.edu/~ani/papers/pyramid.pdf&#34; target=&#34;_blank&#34;&gt;Evaluating Content Selection in Summarization: The Pyramid Method&lt;/a&gt;. Ani Nenkova and Rebecca Passonneau. NAACL 2004. &lt;em&gt;Honorable mention at NAACL 2018&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.aclweb.org/anthology/W/W04/W04-3252.pdf&#34; target=&#34;_blank&#34;&gt;TextRank: Bringing Order into Texts&lt;/a&gt;. Rada Mihalcea and Paul Tarau. EMNLP 2004. &lt;em&gt;Honorable mention at NAACL 2018&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.aclweb.org/anthology/P/P04/P04-1011.pdf&#34; target=&#34;_blank&#34;&gt;Trainable sentence planning for complex information presentation in spoken dialog systems&lt;/a&gt;. Stent, Prasad, and Walker. ACL 2004. &lt;em&gt;Honorable mention at NAACL 2018&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;2003&#34;&gt;2003&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://mlg.eng.cam.ac.uk/zoubin/papers/zgl.pdf&#34; target=&#34;_blank&#34;&gt;Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions&lt;/a&gt;. Zhu, Ghahramani, and Lafferty. ICML 2003. &lt;em&gt;Classic paper prize at ICML 2013&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://people.eecs.berkeley.edu/~brecht/cs294docs/week1/03.Zinkevich.pdf&#34; target=&#34;_blank&#34;&gt;Online Convex Programming and Generalized Infinitesimal Gradient Ascent&lt;/a&gt;. Martin Zinkevich. ICML 2003. &lt;em&gt;Classic paper prize at ICML 2013&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.aclweb.org/anthology/J/J03/J03-4002.pdf&#34; target=&#34;_blank&#34;&gt;Anaphora and Discourse Structure&lt;/a&gt;. Webber et al. Computational Linguistics (2003). &lt;em&gt;Honorable mention at NAACL 2018&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.aclweb.org/anthology/P03-1021&#34; target=&#34;_blank&#34;&gt;Minimum Error Rate Training In Statistical Machine Translation&lt;/a&gt;. Franz Och. ACL 2003. &lt;em&gt;Honorable mention at NAACL 2018&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.aclweb.org/anthology/P03-1069&#34; target=&#34;_blank&#34;&gt;Probabilistic Text Structuring: Experiments with Sentence Ordering&lt;/a&gt;. Mirella Lapata. ACL 2003. &lt;em&gt;Honorable mention at NAACL 2018&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.aclweb.org/anthology/N/N03/N03-1030.pdf&#34; target=&#34;_blank&#34;&gt;Sentence Level Discourse Parsing using Syntactic and Lexical Information&lt;/a&gt;. Radu Soricut and Daniel Marcu. NAACL 2003. &lt;em&gt;Honorable mention at NAACL 2018&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;2002&#34;&gt;2002&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.aclweb.org/anthology/P02-1033&#34; target=&#34;_blank&#34;&gt;An Unsupervised Method for Word Sense Tagging using Parallel Corpora&lt;/a&gt;. Mona Diab and Philip Resnik. ACL 2002. &lt;em&gt;Honorable mention at NAACL 2018&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/P02-1040.pdf&#34; target=&#34;_blank&#34;&gt;BLEU: a Method for Automatic Evaluation of Machine Translation&lt;/a&gt;. Papineni et al. ACL 2002. &lt;em&gt;Test-of-time award at NAACL 2018&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.aclweb.org/anthology/W02-1001&#34; target=&#34;_blank&#34;&gt;Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms&lt;/a&gt;. Michael Collins. EMNLP 2002. &lt;em&gt;Test-of-time award at NAACL 2018&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.aclweb.org/anthology/W02-1011&#34; target=&#34;_blank&#34;&gt;Thumbs up?: Sentiment Classification using Machine Learning Techniques&lt;/a&gt;. Pang, Lee, and Vaithyanathan. EMNLP 2002. &lt;em&gt;Test-of-time award at NAACL 2018&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.aclweb.org/anthology/W/W02/W02-0603.pdf&#34; target=&#34;_blank&#34;&gt;Unsupervised Discovery of Morphemes&lt;/a&gt;. Mathia Creutz and Krista Laguz. SIGPHON 2002. &lt;em&gt;Honorable mention at NAACL 2018&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;2001&#34;&gt;2001&lt;/h2&gt;

&lt;h2 id=&#34;2000&#34;&gt;2000&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf&#34; target=&#34;_blank&#34;&gt;Algorithms for non-negative matrix factorization&lt;/a&gt;. Daniel Lee and H. Sebastian Seung. NIPS 2000. &lt;em&gt;Classic paper award at NIPS 2013&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.jmlr.org/papers/volume1/allwein00a/allwein00a.pdf&#34; target=&#34;_blank&#34;&gt;Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers&lt;/a&gt;. Erin Allwein, Robert Schapire, and Yoram Singer. ICML 2000. &lt;em&gt;Best 10 year paper award at ICML 2000&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.aaai.org/Papers/AAAI/2000/AAAI00-069.pdf&#34; target=&#34;_blank&#34;&gt;PROMPT: Algorithm and Tool for Automated Ontology Merging and Alignment&lt;/a&gt;. Natalya Roy and Mark Musen. AAAI 2000. &lt;em&gt;Classic paper award at AAAI 2018&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Some random observations:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;NLP venues didn&amp;rsquo;t really have a classic paper section until this year&amp;rsquo;s NAACL, which is probably why so many papers were nominated.&lt;/li&gt;
&lt;li&gt;2001 seems to have been a dismal year for NLP, with no good papers in the long run. By contrast, the community appears to have bounced back next year, with all 3 NAACL 2018 test-of-time awards given to papers from 2002.&lt;/li&gt;
&lt;li&gt;I have no idea why BLEU won. It was supposed to be an &amp;ldquo;understudy,&amp;rdquo; which is pretty clear from its name. The fact that it is still being used as an evaluation metric speaks more of a general failure to construct better metrics than of its strength.&lt;/li&gt;
&lt;li&gt;Since the papers are from before 2010, deep learning is conspicuous by its absence. In fact, Collobert and Weston&amp;rsquo;s ICML&amp;rsquo;08 paper on a unified architecture for language is the only such paper.&lt;/li&gt;
&lt;li&gt;Ali Rahimi&amp;rsquo;s &lt;a href=&#34;https://www.livescience.com/62495-rahimi-machine-learning-ai-alchemy.html&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;ML is alchemy&amp;rdquo; talk at NIPS&amp;rsquo;17&lt;/a&gt; got a lot of attention, probably much more than his paper on random features.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;other-similar-lists&#34;&gt;Other similar lists&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://jeffhuang.com/best_paper_awards.html&#34; target=&#34;_blank&#34;&gt;Best paper award winners in Computer Science&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Theory of Deep Learning: An Illustration with Embeddings</title>
      <link>https://abhilashasancheti.github.io/post/deep-learning-theory-5/</link>
      <pubDate>Tue, 31 Jul 2018 18:45:15 +0530</pubDate>
      
      <guid>https://abhilashasancheti.github.io/post/deep-learning-theory-5/</guid>
      <description>

&lt;p&gt;We have discussed several aspects of deep learning theory, ranging from &lt;a href=&#34;https://abhilashasancheti.github.io/post/deep-learning-theory-1/&#34; target=&#34;_blank&#34;&gt;optimization&lt;/a&gt; and &lt;a href=&#34;https://abhilashasancheti.github.io/post/deep-learning-theory-2/&#34; target=&#34;_blank&#34;&gt;generalization guarantees&lt;/a&gt; to &lt;a href=&#34;https://abhilashasancheti.github.io/post/deep-learning-theory-3/&#34; target=&#34;_blank&#34;&gt;role of depth&lt;/a&gt; and &lt;a href=&#34;https://abhilashasancheti.github.io/post/deep-learning-theory-4/&#34; target=&#34;_blank&#34;&gt;generative models&lt;/a&gt;. In this final post of this series, I will illustrate how theory can motivate simple solutions to problems, which can then outperform complex techniques. For this, we will consider a field where deep learning has done exceptionally well, namely, word and sentence embeddings.&lt;/p&gt;

&lt;p&gt;If you need a refresher on word embeddings, I have previously explained them, along with the most popular methods, in &lt;a href=&#34;https://abhilashasancheti.github.io/post/understanding-word-vectors/&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt;. The &lt;em&gt;distributional hypothesis&lt;/em&gt; forms the basis for all word embedding techniques used at present. Instead of naively taking the co-occurence matrix, though, almost all techniques use some low-rank approximation for the same. This gives rise to low-dimensional ($\sim 300$) dense embeddings for text. An important question, then, is the following: How can low-dimensional embeddings represent the complex linguistic structure in text? We will first look at this question from a theoretical perspective, based on &lt;a href=&#34;http://aclweb.org/anthology/Q16-1028&#34; target=&#34;_blank&#34;&gt;this ACL&amp;rsquo;16 paper&lt;/a&gt; from Arora et al.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;how-do-low-dimensional-embeddings-approximate-co-occurence-matrices&#34;&gt;How do low-dimensional embeddings approximate co-occurence matrices?&lt;/h3&gt;

&lt;p&gt;Formally, we want to see why, for some low-dimensional vector representations $v$, we have&lt;/p&gt;

&lt;p&gt;$$ \langle v_w,v_{w^{\prime}} \rangle \approx \text{PMI}(w,w^{\prime}), $$&lt;/p&gt;

&lt;p&gt;where $\text{PMI}(w,w^{\prime})$ is the pointwise mutual information between $w$ and $w^{\prime}$, defined as $\log \frac{P(w,w^{\prime})}{P(w)P(w^{\prime})}$, where the probabilities are computed empirically from the co-occurence matrix.&lt;/p&gt;

&lt;p&gt;For this, the authors propose a generative model of language, as opposed to the usual discriminative model that is based on predicting the context words given a target word (i.e., multiclass classification). This is based on the random walk of a discourse vector $c_t \in \mathcal{R}^d$, which generates $t$th word in step $t$. Every word has a time-invariant latent vector $v_w \in \mathcal{R}^d$, and the word production model is given as&lt;/p&gt;

&lt;p&gt;$$ \text{Pr}[w ~ \text{emitted at time} ~ t|c_t] \propto \exp(\langle c_t,v_w \rangle). $$&lt;/p&gt;

&lt;p&gt;Here, &lt;em&gt;random walk&lt;/em&gt; means that $c_{t+1}$ is obtained by adding a small random displacement vector to $c_t$. For a theoretic analysis, we make an isotropy assumption about the word vectors.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Isotropy assumption&lt;/strong&gt;: In the bulk, word vectors are distributed uniformly in the $\mathcal{R}^d$ space.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To generate such a dsitribution, we can just sample i.i.d from $v = s \cdot v^{\prime}$, where $s$ is a scalar random variable ($s \leq \kappa$), and $v^{\prime}$ is obtained from a spherical Gaussian distribution. This is a simple Bayesian prior similar to the assumptions commonly used in statistics.&lt;/p&gt;

&lt;p&gt;Let us define $Z_c = \sum_{w}\exp(\langle v_w,c \rangle)$. This is like the normalization factor used with the above equation, but it is very difficult to compute. In the paper, the authors prove that this value is very close to some constant $Z$ for a fixed $c$. This allows us to remove this factor from consideration. Empirically, it has also been seen that some log-linear models have self-normalization properties, and this may be a reason for the observation. Let us now see how to prove this lemma.&lt;/p&gt;

&lt;p&gt;Since $Z_c$ is a sum of random variables, it may be tempting to use concentration inequalities to bound its value. However, we cannot do this since $Z_c$ is neither sub-Gaussian nor sub-exponential. We approach the problem it two parts. First we bound the mean and variance of $Z_c$, and then show that it is concentrated around its mean.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Part 1:&lt;/strong&gt; Suppose there are $n$ vectors in our space. Since they are identically distributed, we have&lt;/p&gt;

&lt;p&gt;$$ \mathbb{E}[Z_c] = n\mathbb{E}[\exp(\langle v_w,c \rangle)] \geq n\mathbb{E}[1 + \langle v_w,c \rangle] = n. $$&lt;/p&gt;

&lt;p&gt;Here, we have used $\mathbb{E}[\langle v_w,c \rangle] = 0$, since $v_w$&amp;rsquo;s are drawn from a scaled uniform spherical Gaussian. Now, suppose all the scalar variables $s_w$ are equal in distribution to $s$. Then, we can write&lt;/p&gt;

&lt;p&gt;$$ \mathbb{E}[Z_c] = n\mathbb{E}[\exp(\langle v_w,c \rangle)] = n\mathbb{E}\left[ \mathbb{E} [\exp(\langle v_w,c \rangle)|s]\right]. $$&lt;/p&gt;

&lt;p&gt;We can compute the conditional expectation as&lt;/p&gt;

&lt;p&gt;$$ \begin{align} \mathbb{E} [\exp(\langle v_w,c \rangle)|s] &amp;amp;= \int_x \frac{1}{\sigma\sqrt{2\pi}}\exp\left( -\frac{x^2}{2\sigma^2} \right)\exp(x) dx \\\ &amp;amp;= \frac{1}{\sigma\sqrt{2\pi}}\exp\left( -\frac{(x-\sigma^2)^2}{2\sigma^2} + \frac{\sigma^2}{2}\right) dx \\\ &amp;amp;= \exp(\frac{\sigma^2}{2}). \end{align} $$&lt;/p&gt;

&lt;p&gt;Here, the standard deviation is equal to the scaling factor $s$, and so $\sigma^2 = s^2$. It follows that&lt;/p&gt;

&lt;p&gt;$$ \mathbb{E}(Z_c) = n\exp(\frac{s^2}{2}). $$&lt;/p&gt;

&lt;p&gt;Similarly, we can show that the variance&lt;/p&gt;

&lt;p&gt;$$ \mathbb{V}(Z_c) \leq n\mathbb{E}[\exp(2s^2)]. $$&lt;/p&gt;

&lt;p&gt;Since $\langle v_w,c \rangle|s$ has a Gaussian distribution with variance $s^2 \leq \kappa^2$, we have using Chernoff bounds that&lt;/p&gt;

&lt;p&gt;$$ \text{Pr}[|\langle v_w,c \rangle| \geq \eta \log n |s] \leq \exp \left( - \frac{\eta^2 \log^2 n}{2\kappa^2} \right) = \exp (-\Omega(\log^2 n)). $$&lt;/p&gt;

&lt;p&gt;Here we have removed $\eta$ and $\kappa$ since they are constants. We can now write the converse of this inequality, by taking expectation over all $s_w$, as&lt;/p&gt;

&lt;p&gt;$$ \text{Pr}[|\langle v_w,c \rangle| \leq \frac{1}{2}\log n] \geq 1 - \exp(-\Omega(\log^2 n)). $$&lt;/p&gt;

&lt;p&gt;This means that, with high probability, $|\langle v_w,c \rangle| \leq \frac{1}{2}\log n$, or equivalently, $\exp(\langle v_w,c \rangle) \leq \sqrt{n}$. Now, let the random variable $X_w$ have the same distribution as $\exp(\langle v_w,c \rangle)$ when the above holds.&lt;/p&gt;

&lt;p&gt;Let us take a minute to understand what we are doing here. We do not know how to bound the original $Z_c$, since $\exp(\langle v_w,c \rangle)$ has no known concentration bounds. So we approximate it by a new random variable with high probability, so that we can compute bounds on the sum. Now, let $Z_{c}^{\prime} = \sum_{w}X_w$. We will now try to bound the mean and variance for this random variable.&lt;/p&gt;

&lt;p&gt;Computing the lower bound for the mean is simple since the mean of $\exp(\langle v_w,c \rangle)$ is zero, and so $\mathbb{E}[Z_c^{\prime}] \leq n$. We can similarly bound the variance as $\mathbb{V}[Z_c^{\prime}] \leq 1.1 \Lambda n$, where $\Lambda$ is a constant. Now, using Bernstein&amp;rsquo;s inequality, we get&lt;/p&gt;

&lt;p&gt;$$ \text{Pr}\left[ | Z_c^{\prime} - \mathbb{E}[Z_c^{\prime}] | \geq \epsilon n \right] \leq \exp(-\Omega(\log^2 n)). $$&lt;/p&gt;

&lt;p&gt;Since $Z_c$ has the same distribution as $Z_c^{\prime}$, the above inequality also holds for the former. This means that the probability of $Z_c$ deviating from its mean is very low, and so we can say with high probability that&lt;/p&gt;

&lt;p&gt;$$ (1-\epsilon_z)Z \leq Z_c \leq (1+\epsilon_z)Z. $$&lt;/p&gt;

&lt;p&gt;The above proof was just to remove the normalization factor as a constant from the original problem, so that analysis becomes easier. We now come to the main result itself. Suppose $c$ and $c^{\prime}$ are consecutive discourse vectors and $w$ and $w^{\prime}$ are words generated from them. We have&lt;/p&gt;

&lt;p&gt;$$ \begin{align} p(w,w^{\prime}) &amp;amp;= \mathbb{E}_{c,c^{\prime}}[\text{Pr}[w,w^{\prime}|c,c^{\prime}]] \\\ &amp;amp;= \mathbb{E}_{c,c^{\prime}}[p(w|c)p(w^{\prime}|c^{\prime})] \\\ &amp;amp;= \mathbb{E}_{c,c^{\prime}}\left[ \frac{\exp(\langle v_w,c \rangle)}{Z_c}\right] \frac{\exp(\langle v_{w^{\prime}},c^{\prime} \rangle)}{Z_{c^{\prime}}}. \end{align} $$&lt;/p&gt;

&lt;p&gt;As proved above, we can approximate the denominators to $Z$ and take them out of the expectation. This gives&lt;/p&gt;

&lt;p&gt;$$ \begin{align} p(w,w^{\prime}) &amp;amp;= \frac{1}{Z^2}\mathbb{E}_{c,c^{\prime}}[\exp(\langle v_w,c \rangle)\exp(\langle v_{w^{\prime}},c^{\prime} \rangle))] \\\ &amp;amp;= \frac{1}{Z^2}\mathbb{E}_c [\exp(\langle v_w,c \rangle)\mathbb{E}_{c^{\prime}|c}[\exp(\langle v_{w^{\prime}},c^{\prime} \rangle)]]. \end{align}. $$&lt;/p&gt;

&lt;p&gt;We can compute the internal expectation term as&lt;/p&gt;

&lt;p&gt;$$ \begin{align} \mathbb{E}_{c^{\prime}|c}[\exp(\langle v_{w^{\prime}},c^{\prime} \rangle)] &amp;amp;= \mathbb{E}_{c^{\prime}|c}[\exp(\langle v_{w^{\prime}},c^{\prime} - c + c \rangle)] \\\ &amp;amp;= \mathbb{E}_{c^{\prime}|c}[\exp(\langle v_{w^{\prime}},c^{\prime} -c \rangle)]\exp(\langle v_{w^{\prime}},c \rangle) \\\ &amp;amp;\approx \exp(\langle v_{w^{\prime}},c \rangle). \end{align}$$&lt;/p&gt;

&lt;p&gt;Here, the last approximation can be done because we have assumed that our random walk has small steps, i.e., $|c^{\prime} - c|$ is small. Using this in above, we get&lt;/p&gt;

&lt;p&gt;$$ p(w,w^{\prime}) = \frac{1}{Z^2}\mathbb{E}[\exp(\langle v_w + v_{w^{\prime}},c \rangle)]. $$&lt;/p&gt;

&lt;p&gt;Since $c$ has uniform distribution over the sphere, the above resembles a Gaussian centered at 0 and variance $\frac{\lVert  v_w + v_{w^{\prime}} \rVert^2}{d}$. Since $\mathbb{E}[\exp(X)] = \exp(\frac{\sigma^2}{2})$ for $X \sim \mathcal{N}(0,\sigma^2)$, we get the closed form expression as&lt;/p&gt;

&lt;p&gt;$$ p(w,w^{\prime}) = \frac{1}{Z^2}\exp\left( \frac{\lVert  v_w + v_{w^{\prime}} \rVert^2}{2d} \right), $$&lt;/p&gt;

&lt;p&gt;which is the desired result. Note that I have ignored some technicalities for error bounds in this proof. We have now shown the original result that we wanted, but how did dimensionality help?&lt;/p&gt;

&lt;p&gt;The answer lies in the &lt;em&gt;isotropy assumption&lt;/em&gt; that we made at the very beginning. Having $n$ vectors be isotropic in $d$ dimensions requires $d &amp;lt;&amp;lt; n$, which is indeed what is observed empirically. Hence, theory justifies experimental findings.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;an-algorithm-for-sentence-embeddings&#34;&gt;An algorithm for sentence embeddings&lt;/h3&gt;

&lt;p&gt;In a previous part of this series, I echoed Prof. Arora&amp;rsquo;s concern that theoretical analysis at present is like a postmortem analysis, where we try to find properties of the model that can explain certain empirical findings. The ideal scenario would be where we can use this understanding to guide future learning models. In this section, I will look at &lt;a href=&#34;https://openreview.net/pdf?id=SyK00v5xx&#34; target=&#34;_blank&#34;&gt;this paper from ICLR&amp;rsquo;17&lt;/a&gt; which uses the understanding from the previous section to build simple but strong word embeddings.&lt;/p&gt;

&lt;p&gt;Suppose we want to obtain the vector for a piece of text, say, a sentence. From our generative model defined in the previous section, it would be reasonable to say that this can be approximated by a &lt;em&gt;max a priori&lt;/em&gt; (MAP) estimate of the discourse vector that generated the sentence, i.e.,&lt;/p&gt;

&lt;p&gt;$$ \text{Pr}[w ~ \text{emitted in sentence} ~ s | c_s] = \frac{\exp(\langle c_s,v_w \rangle)}{Z_{c_s}}, $$&lt;/p&gt;

&lt;p&gt;where $c_s$ is the discourse vector that remains approximately constant for the sentence. However, we need to modify this slightly to account for two real situations.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Some words often appear out of context, and some stop words appear regardless of discourse. To approximate this, we add a term $\alpha p(w)$ to the log-linear model, where $p(w)$ is the unigram probability of the word. This makes probability of appearance of some words high even if they have low correlation with the discourse vector.&lt;/li&gt;
&lt;li&gt;Generation of words depends not just on current sentence, but on entire history of discourse. To model this, we use discourse vector $\tilde{c}_s = \beta c_0 + (1-\beta)c_s$, where $c_0$ is the common discourse vector.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Finally, the modified log-linear objective is as follows.&lt;/p&gt;

&lt;p&gt;$$ \text{Pr}[w ~ \text{emitted in sentence} ~ s | c_s] = \alpha p(w) + (1-\alpha) \frac{\exp(\langle \tilde{c}_s,v_w \rangle)}{Z_{\tilde{c}_s}} $$&lt;/p&gt;

&lt;p&gt;After the word embeddings have been trained using this objective, we can model the likelihood for obtaining sentence $s$ given discourse vector $c_s$ as&lt;/p&gt;

&lt;p&gt;$$ p[s|c_s] = \prod_{w\in s}p(w|c_s) = \prod_{w\in s}\left[ \alpha p(w) + (1-\alpha) \frac{\exp(\langle \tilde{c}_s,v_w \rangle)}{Z} \right]. $$&lt;/p&gt;

&lt;p&gt;Here, we have taken $Z_{\tilde{c}_s} = Z$, in accordance with the result we proved earlier. To maximize this expression, we just need to maximize the term inside the product. Taking $f_w(\tilde{c}_s)$ to denote the term inside the product, we can easily compute its derivative, and then use Taylor expansion, $f_w(\tilde{c}_s) = f_w(0) + \nabla f_w(\tilde{c}_s)^T \tilde{c}_s$, to get an expression for $f_w(\tilde{c}_s)$. Finally, we have&lt;/p&gt;

&lt;p&gt;$$ \text{arg}\max\sum_{w\in s}f_w(\tilde{c}_s) \propto \sum_{w\in s}\frac{a}{p(w)+a}v_w, $$&lt;/p&gt;

&lt;p&gt;where $a = \frac{1-\alpha}{\alpha Z}$. If we analyze this expression, this is simply a weighted sum of the word vectors in the sentence, which is one of the most common bag-of-words technique to obtain sentence embeddings. Furthermore, the weight is low if the unigram frequency of the word is high. This is similar to Tf-idf weighting of words. Now, this theory gives rise to the following algorithm, taken from the original paper.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/25/sif.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This is a striking illustration of how rigorously developed theoretical results can guide construction of simple algorithms in deep learning.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Final note:&lt;/strong&gt; This series was based on the ICML 2018 tutorial on &amp;ldquo;&lt;a href=&#34;http://unsupervised.cs.princeton.edu/deeplearningtutorial.html&#34; target=&#34;_blank&#34;&gt;Toward a Theory for Deep Learning&lt;/a&gt;&amp;rdquo; by &lt;a href=&#34;https://www.cs.princeton.edu/~arora/&#34; target=&#34;_blank&#34;&gt;Prof. Sanjeev Arora&lt;/a&gt;, which is why the discussion revolved mostly around the work done by his group. The papers themselves are not very trivial to understand, but the &lt;a href=&#34;www.offconvex.org&#34; target=&#34;_blank&#34;&gt;blog posts&lt;/a&gt; are more beginner friendly, and highly recommended. Several people criticize deep learning for being purely intuition-based, but I believe that will change soon, given that so much good research is being done to develop a theory for it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Theory of Deep Learning: Generative Models</title>
      <link>https://abhilashasancheti.github.io/post/deep-learning-theory-4/</link>
      <pubDate>Mon, 30 Jul 2018 23:39:37 +0530</pubDate>
      
      <guid>https://abhilashasancheti.github.io/post/deep-learning-theory-4/</guid>
      <description>

&lt;p&gt;Till now, in this series based on the ICML 2018 tutorial on &amp;ldquo;&lt;a href=&#34;http://unsupervised.cs.princeton.edu/deeplearningtutorial.html&#34; target=&#34;_blank&#34;&gt;Toward a Theory for Deep Learning&lt;/a&gt;&amp;rdquo; by &lt;a href=&#34;https://www.cs.princeton.edu/~arora/&#34; target=&#34;_blank&#34;&gt;Prof. Sanjeev Arora&lt;/a&gt;, we have limited our discussion to the theory of supervised discriminative neural models, i.e., those models which learn the conditional probability $P(y|x)$ from a set of given $(x_i,y_i)$ samples. In particular, we saw &lt;a href=&#34;https://abhilashasancheti.github.io/post/deep-learning-theory-1/&#34; target=&#34;_blank&#34;&gt;how deep networks find good solutions&lt;/a&gt;, &lt;a href=&#34;https://abhilashasancheti.github.io/post/deep-learning-theory-2/&#34; target=&#34;_blank&#34;&gt;why they generalize well&lt;/a&gt; despite being overparametrized, and &lt;a href=&#34;https://abhilashasancheti.github.io/post/deep-learning-theory-3/&#34; target=&#34;_blank&#34;&gt;what role depth plays&lt;/a&gt; in all of this.&lt;/p&gt;

&lt;p&gt;We now turn our attention towards the theory of unsupervised learning and generative models, with special emphasis on variational autoencoders and generative adversarial networks (GANs). But first, &lt;em&gt;what is unsupervised learning&lt;/em&gt;?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Evidently, unsupervised learning is much more abstract than its supervised counterpart. In the latter, our objective was essentially to find a function that approximates the original mapping of the distribution $\mathcal{X}\times\mathcal{Y}$. In the unsupervised domain, there is no such objective. We are given input data, and we want to learn &amp;ldquo;structure&amp;rdquo;. The most obvious way to understand why this is more difficult is to realize that &lt;em&gt;drawing a picture of a lion is much more difficult than identifying a lion in a picture&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Why is learning structures important? Creating large annotated datasets is an expensive task, and may even be infeasible for some problems such as parsing, which require significant domain knowledge. Let&amp;rsquo;s consider the simplest problem of image classification. The largest dataset for this problem, ImageNet, contains 14 million images, with 20000 distinct output labels. However, the number of images freely available online far exceeds 14 million, which means that we can probably learn something from them. This kind of &lt;strong&gt;transfer learning&lt;/strong&gt; is the most important motivation for unsupervised learning.&lt;/p&gt;

&lt;p&gt;For instance, while training a machine translation model, obtaining a parallel corpus may be difficult, but we always have access to unilateral text corpora in different languages. If we then try to learn some underlying structure present in these languages, it can assist the downstream translation task. In fact, recent advances in &lt;a href=&#34;https://abhilashasancheti.github.io/post/transfer-learning-nlp/&#34; target=&#34;_blank&#34;&gt;transfer learning for NLP&lt;/a&gt; have empirically proven that huge performance gains are possible using such a technique.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/pdf/1206.5538.pdf&#34; target=&#34;_blank&#34;&gt;Representation learning&lt;/a&gt;&lt;/strong&gt; is perhaps the most widely studied aspect of unsupervised learning. A &amp;ldquo;good representation&amp;rdquo; often means one which disentangles factors of variation, i.e, each coordinate in the representation corresponds to one meaningful factor of variation. For example, if we consider word embeddings, an ideal vector representing a word would depict different features of the word along each dimension. However, this is easier said than done, since learning representations require an objective function, and it is still unknown how to translate these notions of &amp;ldquo;good representation&amp;rdquo; into training criteria. For this reason, representation learning is often criticized for getting too much attention for transfer learning. The essence of the criticism, taken from &lt;a href=&#34;https://www.inference.vc/goals-and-principles-of-representation-learning/&#34; target=&#34;_blank&#34;&gt;this post by Ferenc Huszár&lt;/a&gt; is this:&lt;/p&gt;

&lt;p&gt;If we identified transfer learning as the primary task representation learning is supposed to solve, are we actually sure that representation learning is the way to solve it? One can argue that there may be many ways to transfer information from some dataset over to a novel task. Learning a representation and transferring that is just one approach. Meta-learning, for example, might provide another approach.&lt;/p&gt;

&lt;p&gt;In the discussion so far, we have blindly assumed that the data indeed contains structures that can be learnt. This is not an oversight; it is actually based on the &lt;strong&gt;manifold assumption&lt;/strong&gt; which we will discuss next.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;the-manifold-assumption&#34;&gt;The manifold assumption&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;A manifold is a topological space that locally resembles Euclidean space near each point.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This means that globally, a manifold may not be a Euclidean space. The only requirement for an $n$-manifold, i.e., a manifold in $n$ dimensions, is that each point of the manifold must have a neighborhood that is homeomorphic to the Euclidean space of $n$ dimensions. There are three technicalities in this definition.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;A &lt;em&gt;neighborhood&lt;/em&gt; of a point $p$ in $X$ is a $V \subset X$ which contains an open set $U$ containing $p$, i.e., $p$ must be in the interior of $V$.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A function $f: X \rightarrow Y$ between two topological spaces $X$ and $Y$ is called a &lt;em&gt;homeomorphism&lt;/em&gt; if it has the following properties:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$f$ is a bijection,&lt;/li&gt;
&lt;li&gt;$f$ is continuous,&lt;/li&gt;
&lt;li&gt;$f^{-1}$ is continuous.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;Euclidean space&lt;/em&gt; is a topological space such that&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;it is in 2 or 3 dimensions and obeys Euclidean postulates, or&lt;/li&gt;
&lt;li&gt;it is in any dimension such that points are given by coordinates and satisfy Euclidean distance.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note that the dimension of a manifold may not always be the same as the dimension of the space in which the manifold is embedded. Dimension here simply means the degree of freedom of the underlying process that generated the manifold. As such, lines and curves, even if embedded in $\mathbb{R}^3$, are one-dimensional manifolds.&lt;/p&gt;

&lt;p&gt;With this definition in place, we can now state the manifold assumption. It hypothesizes that the intrinsic dimensionality of the data is much smaller than the ambient space in which the data is embedded. This means that if we have some data in $N$ dimensions, there must be an underlying manifold $\mathcal{M}$ of dimension $n &amp;lt;&amp;lt; N$, from which the data is drawn based on some probability distribution $f$. The goal of unsupervised learning in most cases, is to identify such a manifold.&lt;/p&gt;

&lt;p&gt;It is easy to see that the manifold assumption is, as the name suggests, just an assumption, and does not hold universally. Otherwise, applying the assumption consecutively, we would be able to represent any high-dimensional data using a one-dimensional manifold, which, of course, is not possible.&lt;/p&gt;

&lt;p&gt;The task of manifold learning is modeled as approximating the joint probability density $p(x,z)$, where $x$ is the data point and $z$ is its underlying &amp;ldquo;code&amp;rdquo; on the manifold. Deep generative models have come to be accepted as the standard for estimating this probability, because of two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Deep models promote reuse of features. We have already seen in the previous post that depth is analogous to composition whereas width is analogous to addition. Composition offers more representation capability than addition using the same number of parameters.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Deep models are conjectured to lead to progressively more abstract features at higher levels of representation. An example of this is the commonly known phenomenon in training deep convolutional networks on image data, where it is found that the first few layers learn lines, blobs, and other local features, and higher level layers learn more abstract features. This is done explicitly using the pooling mechanism.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;theory-of-variational-autoencoders&#34;&gt;Theory of Variational Autoencoders&lt;/h3&gt;

&lt;p&gt;Deep learning models often face some flak for being purely intution-based. &lt;a href=&#34;https://arxiv.org/pdf/1606.05908.pdf&#34; target=&#34;_blank&#34;&gt;Variational autoencoders (VAEs)&lt;/a&gt; are the practitioner&amp;rsquo;s answer to such criticisms, since they are rooted in the theory of Bayesian inference, and also perform well empirically. In this section, we will look at the theory that forms VAEs.&lt;/p&gt;

&lt;p&gt;First, we formalize the notion of the &amp;ldquo;code&amp;rdquo; that we mentioned earlier using the concept of a &lt;strong&gt;latent variable&lt;/strong&gt;. These are those variables that are not directly observed but are inferred from the observable variables. For instance, if the model is drawing a picture of an MNIST digit, it would make sense to first have a variable choose a digit from $[0,\ldots,9]$, and then draw the strokes corresponding to the digit.&lt;/p&gt;

&lt;p&gt;Formally, suppose we have a vector of latent variables $z$ in a high-dimensional space $\mathcal{Z}$ which can be sampled using a probability distribution $P(z)$. Then, suppose we have a family of deterministic functions $f(z;\theta)$ parametrized by $\theta \in \Theta$, such that $f:\mathcal{Z}\times \Theta \rightarrow \mathcal{X}$. The task, then, is to optimize $\theta$ such that we can sample $z$ from $P(z)$ and with high probability, $f(z;\theta)$ will be like the $X$&amp;rsquo;s in our dataset. As such, we can write the expression for the generated data as&lt;/p&gt;

&lt;p&gt;$$ X^{\prime} = f(z;\theta). $$&lt;/p&gt;

&lt;p&gt;Now, since we have no idea how to check if randomly generated images are &amp;ldquo;like&amp;rdquo; our dataset, we use the notion of &amp;ldquo;maximum likelihood&amp;rdquo;, i.e., if the model is likely to produce training set samples, then it is also likely to produce similar samples and unlikely to produce dissimilar ones. With this assumption, we want to maximize the probability of each $X$ in the training process. We can now replace $f(z;\theta)$ by the conditional probability $P(X|z;\theta)$, and we get&lt;/p&gt;

&lt;p&gt;$$ P(X) = \int P(X|z;\theta)P(z)dz. $$&lt;/p&gt;

&lt;p&gt;In VAEs, we usually have $P(X|z;\theta) = \mathcal{N}(X|f(z;\theta),\sigma^2 I)$, which is a Gaussian. Using this formalism, we can use gradient descent to increase $P(X)$ by making $f(z;\theta)$ approach $X$ for some $z$. So essentially, VAEs do the following steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Sample $z$ from some known distribution.&lt;/li&gt;
&lt;li&gt;Feed $z$ into some parametrized function to get $X$.&lt;/li&gt;
&lt;li&gt;Tune the parameters of the function such that generated $X$ resemble those in dataset.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this process, two questions arise:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How do we define $z$?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;VAEs simply sample $z$ from $\mathcal{N}(0,I)$, where $I$ is the identity matrix. The motivation for this choice is that any distribution in $d$ dimensions can be generated by taking a set of $d$ variables that are normally distributed and mapping them through a sufficiently complicated function. I do not prove this here, but the proof is based on taking the composition of the inverse cumulative distribution function (CDF) of the desired distribution with the CDF of a Gaussian.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How do we deal with $\int dz$?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We need to understand that the space $\mathcal{Z}$ is very large, and there are only few $z$ which generate realistic $X$, which makes it very difficult to sample &amp;ldquo;good&amp;rdquo; values of $z$ from $P(z)$ . Suppose we have a function $Q(z|X)$ which, given some $X$, gives a distribution over $z$ values that are likely to produce $X$. Now to compute $P(X)$, we need to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;relate $P(X)$ with $\mathbb{E}_{z\sim Q}P(X|z)$, and&lt;/li&gt;
&lt;li&gt;estimate $\mathbb{E}_{z\sim Q}P(X|z)$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the first, we use KL-divergence (that we saw in the previous post) between the probability distribution estimated by $Q$ to the actual conditional probability distribution as follows.&lt;/p&gt;

&lt;p&gt;$$ \begin{align} &amp;amp; \mathcal{D}_{KL}[Q(z|X)||P(z|X)] = \mathbb{E}_{z\sim Q}[\log Q(z|X) - \log P(z|X)] \\\ &amp;amp;= \mathbb{E}_{z\sim Q}\left[ \log Q(z|X) - \log \frac{P(X|z)P(z)}{P(X)} \right] \\\ &amp;amp;= \mathbb{E}_{z\sim Q} [ \log Q(z|X) - \log P(X|z) - \log P(z) ] + \log P(X) \\\ \Rightarrow &amp;amp; \log P(X) - \mathcal{D}_{KL}[Q(z|X)||P(z|X)] = \mathbb{E}_{z\sim Q}[\log P(X|z)] - \mathcal{D}_{KL}[Q(z|X)||P(z)] \end{align} $$&lt;/p&gt;

&lt;p&gt;In the LHS of the above equation, we have an expression that we want to maximize, since we want $P(X)$ to be large and we want $Q$ to approximate the conditional probability distribution (this was our objective of using KL-divergence). If we use a sufficiently high-capacity model for $Q$, the $\mathcal{D}_{KL}$ term will approximate $0$, in which case we will directly be optimizing $P(X)$.&lt;/p&gt;

&lt;p&gt;Now we are just left with finding some way to optimize the RHS in the equation. For this, we will have to choose some model for $Q$. An obvious (and usual) choice is to take the multivariate Gaussian, i.e., $Q(z|X) = \mathcal{N}(z|\mu(X),\Sigma(X))$. Since $P(z) = \mathcal{N}(0,I)$, the KL-divergence term on the RHS can now be written as&lt;/p&gt;

&lt;p&gt;$$ \mathcal{D}_{KL}[\mathcal{N}(\mu(X),\sum(X))||\mathcal{N}(0,I)] = \frac{1}{2}\left( \text{tr}(\Sigma(X)) + (\mu(X))^T (\mu(X)) - k - \log \text{det}(\Sigma(X)) \right). $$&lt;/p&gt;

&lt;p&gt;To estimate the first term on the RHS, we just compute the term for one sample of $z$, instead of iterating over several samples. This is because during stochastic gradient descent, different values of $X$ will automatically require us to sample $z$ several times. With this approximation, the optimization objective for a single sample $X$ becomes&lt;/p&gt;

&lt;p&gt;$$ J = \log P(X|z) - \mathcal{D}_{KL}[Q(z|X)||P(z)]. $$&lt;/p&gt;

&lt;p&gt;This can be represented in the form of a feedforward network by the figure on the left below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/24/vae.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There is, however, a caveat. The network is not trainable using backpropagation because the red box is a stochastic step, which means that it is not differentiable. To solve this problem, we use the &lt;strong&gt;reparametrization trick&lt;/strong&gt; as follows.&lt;/p&gt;

&lt;p&gt;$$ z = \mu(X) + \Sigma^{\frac{1}{2}}(X)  \epsilon \quad \text{where} \quad \epsilon \sim \mathcal{N}(0,I) $$&lt;/p&gt;

&lt;p&gt;After this trick, we get the final network as shown in the right in the above figure. Furthermore, we must have $\mathcal{D}_{KL}[Q(z|X)||P(z|X)]$ approximately equal $0$ in the LHS. Since we have taken $Q$ to be a Gaussian, this means that the original density function $f$ should be such that $P(z|X)$ is a Gaussian. It turns out that such a function, which maximizes $P(X)$ and satisfies the said criteria, provably exists.&lt;/p&gt;

&lt;p&gt;Although VAEs have strong theoretical support, they do not work very well in practice, especially in problems such as face generation. This is because the loss function used for training is log-likelihood, which ultimately leads to fuzzy face images which have high match with several $X$. Instead of using likelihood, we use the power of discriminative deep learning, which is where GANs come into the picture.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;generative-adversarial-networks-new-insights&#34;&gt;Generative adversarial networks: new insights&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1701.00160&#34; target=&#34;_blank&#34;&gt;GANs&lt;/a&gt; were proposed in 2014, and have become immensely popular in computer vision ever since. They are basically motivated from game theory, and I will not get into the details here since the tutorial by Ian Goodfellow is a excellent resource for the same.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/24/gan.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Since the prior learnt by the generator depends upon the discriminative process, an important issue with GANs is that of &lt;strong&gt;mode collapse&lt;/strong&gt;. The problem is that since the discriminator only learns from a few samples, it may be unable to teach the generator to produce $\mathcal{P}_{synth}$ with sufficiently large diversity. In the context of what we have already seen, this can be taken as the problem of generalization for GANs.&lt;/p&gt;

&lt;p&gt;In this section, I will discuss three results from two important papers from Arora et al. which deal with mode collapse in GANs.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1703.00573.pdf&#34; target=&#34;_blank&#34;&gt;Generalization and equilibrium in generative adversarial nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openreview.net/pdf?id=BJehNfW0-&#34; target=&#34;_blank&#34;&gt;Do GANs learn the distribution? Some theory and empirics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For all our discussions in this section, we will consider the Wasserstein GAN objective instead of the usual minimax objective, which is as follows (and arguably more intuitive)&lt;/p&gt;

&lt;p&gt;$$ J = \lvert \mathbb{E}_{x\in \mathcal{P}_{real}}[D(x)] - \mathbb{E}_{x\in \mathcal{P}_{synth}}[D(x)] \rvert, $$&lt;/p&gt;

&lt;p&gt;where $D$ is the discriminator.&lt;/p&gt;

&lt;h4 id=&#34;1-generalization-depends-on-discriminator-size&#34;&gt;1. Generalization depends on discriminator size&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;If the discriminator size is $n$, then there exists a generator supported on $\mathcal{O}(n\log n)$ images, which wins against all possible discriminators.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This means that if we have a discriminator of size $n$, then the best possible generator training is possible using $Cn/\epsilon^2 \log n$ images from the full training set. Any more images will improve the training objective by at most $\epsilon$. I will now give the proof (simplified from the actual proof in the paper).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Suppose $\mu$ denotes the actual distribution learnt by the generator and $\nu$ denotes the actual distribution of real images that the discriminator has access to. Let $\tilde{\mu}$ and $\tilde{\nu}$ be the empirical versions of the above distributions, i.e., the distributions that we actually use for training. Let $d(p,q)$ be some distance measure between the two distributions.&lt;/p&gt;

&lt;p&gt;In the paper, the authors have defined an $\mathcal{F}$-distance that has good generalization properties, but I will not get into the details of that here for sake of simplicity. For this discussion, just assume that the distance measure is $d$. From my earlier post on generalization error in supervised learning, we say that a model generalizes well when, for some $\epsilon$,&lt;/p&gt;

&lt;p&gt;$$ |\text{True error} - \text{Empirical error}| \leq \epsilon. $$&lt;/p&gt;

&lt;p&gt;Here, we don&amp;rsquo;t really know the error, but we can use our distance measure to the same effect. If the size of discriminator is $p$, we want to compute the sample complexity $m$ in terms of $p$ and $\epsilon$ such that the GAN generalizes. For that, we need a few approximations.&lt;/p&gt;

&lt;p&gt;First we approximate the parameter space $\mathcal{V}$ using its $\frac{\epsilon}{8}$-net $\mathcal{X}$. This means that for every $\nu \in \mathcal{V}$, we can find a $\nu^{\prime}\in \mathcal{X}$ which is at a distance of at most $\frac{\epsilon}{8}$ from it. Assuming that the function computed by the discriminator $D$ is 1-Lipschitz, we can then say that $\lvert \mathbb{E}_{x\sim \mu}[D_{\nu}(x)] - \mathbb{E}_{x\sim \mu}[D_{\nu^{\prime}}(x)]  \rvert \leq \frac{\epsilon}{8}$.&lt;/p&gt;

&lt;p&gt;The $\epsilon$-net is taken so that we can apply concentration inequalities in this continuous finite space. You can read more about them &lt;a href=&#34;https://www.ti.inf.ethz.ch/ew/lehre/CG12/lecture/Chapter%2015.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. Now, we can use Hoeffding&amp;rsquo;s inequality to bound the difference between true and empirical errors on this space as&lt;/p&gt;

&lt;p&gt;$$ P\left[ $\lvert \mathbb{E}_{x\sim \mu}[D_{\nu}(x)] - \mathbb{E}_{x\sim \tilde{\mu}}[D_{\nu}(x)]  \rvert \geq \frac{\epsilon}{4} \right] \leq 2\exp \left( -\frac{\epsilon^2 m}{2} \right). $$&lt;/p&gt;

&lt;p&gt;Taking union bound over all $p$ parameters, we get that when $m \geq \frac{Cp\log (p/\epsilon)}{\epsilon^2}$, then the bound holds with high probability. Note that this sample complexity is $m = \mathcal{p\log p}$, which is what we wanted. Now we just need to show that this bound implies that the generalization error is bounded. Since we have taken the $\frac{\epsilon}{8}$-net approximation, we translate both the parameters in $\mathcal{X}$ back to $\mathcal{V}$, paying a cost of $\frac{\epsilon}{8}$ for each. Finally, we get, for every $D$,&lt;/p&gt;

&lt;p&gt;$$ \lvert \mathbb{E}_{x\sim \mu}[D_{\nu}(x)] - \mathbb{E}_{x\sim \tilde{\mu}}[D_{\nu}(x)]  \rvert \leq \frac{\epsilon}{2}. $$&lt;/p&gt;

&lt;p&gt;We can prove a similar upper bound for $\nu$. Finally, with similar approximation arguments, and from the definition of our distance function, we get the desired result.&lt;/p&gt;

&lt;h4 id=&#34;2-existence-of-equilibrium&#34;&gt;2. Existence of equilibrium&lt;/h4&gt;

&lt;p&gt;For GANs to be successful, they must find an equilibrium in the G-D game where the generator wins. In the context of the minimax equation, this means that switching min and max in the objective should not cause any change in the equilibrium. In the paper, the authors prove an $\epsilon$-approximate equilibrium, i.e., one where such a switching affects the expression by at most $\epsilon$.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If a generator net is able to generate a Gaussian distribution, then there exists an $\epsilon$-approximate equilibrium where the generator has capacity $\mathcal{O}(n\log n / \epsilon^2)$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The proof of this result lies in a classical result in statistics, which says that any probability distribution can be approximated by a mixture of infinite Gaussians. For this, we just need to take the standard Gaussian $P(x)\mathcal{N}(x,\sigma^2)$ at every $x \in \mathcal{X}$ such that $\sigma^2 \rightarrow 0$, and take the mixture of all such Gaussians. The remaining proof is similar to the one done for the previous result, so I will not repeat it here.&lt;/p&gt;

&lt;h4 id=&#34;3-empirically-detecting-mode-collapse&#34;&gt;3. Empirically detecting mode collapse&lt;/h4&gt;

&lt;p&gt;We have already seen that GAN training can be successful even if the generator has not learnt a good enough distribution, if the discriminator is small. But suppose we take a really large discriminator and then train our GAN to a minima. How do we still make sure that the generator distribution is good? It could well be the case that the generator has simply memorized the training data, due to which the discriminator cannot make a better guess than random. Researchers have proposed several qualitative checks to test this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Check the similarity of each generated image to the nearest image in the training set.&lt;/li&gt;
&lt;li&gt;If the seed formed by interpolating two seeds $s_1$ and $s_2$ that produce realistic images, also produces realistic images, then the learnt distribution probably has many realistic images.&lt;/li&gt;
&lt;li&gt;Check for semantically important directions in latent space, which cause predictable changes in generated image.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will now see a new empirical measure for the support size of the trained distribution, based on the Birthday Paradox.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;The birthday paradox&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In a room of just 23 people, there&amp;rsquo;s a 50% chance of finding 2 people who share their birthday.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To see why, refer to &lt;a href=&#34;https://betterexplained.com/articles/understanding-the-birthday-paradox/&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt;. It is a simple problem of permutation and combination, followed by using the approximation for $e^x$.&lt;/p&gt;

&lt;p&gt;Since $23 \approx \sqrt{365}$, we can generalize this to mean that if a distribution has support $N$, we are likely to find a duplicate in a batch of about $\sqrt{N}$ samples taken from this distribution. As such, finding the smallest batch size $s$ which ensures duplicate images with good probability almost guarantees that the distribution has support $s^2$. Let us formalize this guarantee.&lt;/p&gt;

&lt;p&gt;Suppose we have a probability distribution $P$ on a set $\Omega$. Also, let $S \subset \Omega$ such that $\sum_{s\in S}P(s)\geq \rho$ and $|S|=N$. Then from calculations similar to the one done for birthday paradox, we can say that the probability of finding at least one collision on drawing $M$ i.i.d samples is at least $1 - \exp\left( -\frac{(M^2 - M)\rho}{2N} \right)$.&lt;/p&gt;

&lt;p&gt;Now, suppose we have empirically found this minimum probability of collision to be $\gamma$. Then it can be shown that under realistic assumptions on parameters, the following holds:&lt;/p&gt;

&lt;p&gt;$$ N \leq \frac{2M\rho^2}{\left(-3 + \sqrt{9+\frac{24}{M}\log \frac{1}{1-\gamma}}\right)-2M(1-\rho)^2} $$&lt;/p&gt;

&lt;p&gt;This gives an upper bound on the support size of the distribution learned by the generator.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Generative models are definitely very promising, especially with the recent interest in transfer learning with unsupervised pretraining. While I have tried to explain the recent insights into GANs as best as possible, it is not possible to explain every detail in the proof in an overview post. Even so, I hope I have been able to at least give a flavor of how veterans in the field approach theoretical guarantees.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Theory of Deep Learning: Role of Depth</title>
      <link>https://abhilashasancheti.github.io/post/deep-learning-theory-3/</link>
      <pubDate>Sat, 28 Jul 2018 23:00:20 +0530</pubDate>
      
      <guid>https://abhilashasancheti.github.io/post/deep-learning-theory-3/</guid>
      <description>

&lt;p&gt;In the previous posts of this series, we have looked at &lt;a href=&#34;https://abhilashasancheti.github.io/post/deep-learning-theory-1/&#34; target=&#34;_blank&#34;&gt;how stochastic gradient descent is able to find a good solution&lt;/a&gt; despite the nonconvex objective, and &lt;a href=&#34;https://abhilashasancheti.github.io/post/deep-learning-theory-2/&#34; target=&#34;_blank&#34;&gt;why overparametrized neural networks generalize so well&lt;/a&gt;. In this post, we will look at the titular property of deep networks, namely depth, and what role they play in the learning ability of the model.&lt;/p&gt;

&lt;p&gt;An ideal result in this regard would be if we can show that there exists a class of natural learning problems (recall the idea of a &amp;ldquo;natural&amp;rdquo; problem from the first post) which cannot be solved with depth $d$ neural networks, but are solvable with at least one model of depth $d+1$. However, such a result is elusive at present, since we have already established that there exists no mathematical formulation of a &amp;ldquo;natural&amp;rdquo; learning problem.&lt;/p&gt;

&lt;p&gt;However, there has been some advancement in establishing similar results in the case of less natural problems, and in this regard, the following papers are worth mentioning.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://proceedings.mlr.press/v49/eldan16.pdf&#34; target=&#34;_blank&#34;&gt;The Power of Depth for Feedforward Neural Networks&lt;/a&gt; by Eldan and Shamir&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://proceedings.mlr.press/v49/telgarsky16.pdf&#34; target=&#34;_blank&#34;&gt;Benefit of Depth in Neural Networks&lt;/a&gt; by Telgarsky&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I will now discuss both of these in some detail.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;role-of-depth-for-less-natural-problems&#34;&gt;Role of depth for &amp;ldquo;less natural&amp;rdquo; problems&lt;/h3&gt;

&lt;h4 id=&#34;1-approximating-radial-functions&#34;&gt;1. Approximating radial functions&lt;/h4&gt;

&lt;p&gt;At the outset, note that if we allow the neural network to be unbounded, i.e., have exponential width, even a 2-layer network can approximate any continuous function. As such, for our study, we only use &amp;ldquo;bounded&amp;rdquo; networks where the number of hidden layer units cannot be exponential in the dimension of input. With this understanding, we will look at the simplest case: we try to find a function (or a family of functions) that are expressible by a 3-layer network but cannot be expressed by any 2-layer network. Before we get into the details, we first look at what a 2-layer and 3-layer networks represent.&lt;/p&gt;

&lt;p&gt;A 2-layer network represents the following function:&lt;/p&gt;

&lt;p&gt;$$ f_2(\mathbf{x}) = \sum_{i=1}^w v_i \sigma(&amp;lt; \mathbf{w}_i,\mathbf{x} &amp;gt;+b_i), $$&lt;/p&gt;

&lt;p&gt;and a 3-layer network represents the following:&lt;/p&gt;

&lt;p&gt;$$ f_3(\mathbf{x}) = \sum_{i=1}^w u_i \sigma\left( \sum_{j=1}^w v_{i,j} \sigma (&amp;lt; \mathbf{w}_{i,j},\mathbf{x} &amp;gt;+ b_{i,j}) + c_i \right). $$&lt;/p&gt;

&lt;p&gt;Here, $w$ is the size of the hidden layer and $\sigma$ is an activation function. The only constraint on $\sigma$ is that it should be &amp;ldquo;universal&amp;rdquo;, i.e., a 2-layer network should be able to approximate any Lipschitz function that is non-constant on a bounded domain for some $w$ (which need not be bounded). This constraint is satisfied by all the standard activation functions such as sigmoid and ReLU.&lt;/p&gt;

&lt;p&gt;Under this assumption, the main result in the paper is as follows:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;There exists a &lt;strong&gt;radial function $g$&lt;/strong&gt; depending only on the norm of the input, which is expressible by a 3-layer network of width polynomial in the input dimension, but not by any 2-layer neural network.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;More importantly, apart from the universal assumption, this result does not depend on any characteristic of $\sigma$. Furthermore, there are no constraints on the size of the parameters $\mathbf{w}$. The only constraint worth noting is that $g$ must be a &lt;em&gt;radial function&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;To prove this result, we need to show 2 things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;$g$ can be approximated by a 3-layer neural network.&lt;/li&gt;
&lt;li&gt;$g$ cannot be approximated by any 2-layer network of bounded width.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Part 1:&lt;/strong&gt; This is trivial to show, since any radial function can be approximated by a 3-layer network. To do this, we compute the Euclidean norm $\lVert \mathbf{x} \rVert^2$ from the input $\mathbf{x}$ in the first layer using a linear combination of neurons. This is possible because the squared norm is just the sum of squares of all the components, and each squared component can be approximated in a finite range, for example, using the step function.&lt;/p&gt;

&lt;p&gt;Once the norm is computed, the second layer can be used to approximate the required radial function using RBF nodes. This completes the construction.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Part 2:&lt;/strong&gt; Let the input be taken from a probability distribution $\mu$, which has a density function $\phi^2(x)$, for some known function $\phi$. Suppose we are trying to approximate a function $f$ using a function $g$. Then, the distance between the functions can be given as&lt;/p&gt;

&lt;p&gt;$$ \begin{align} \mathbb{E}_{\mu}(f(x)-g(x))^2 &amp;amp;= \int (f(x)-g(x))^2 \phi^2(x) dx \\\ &amp;amp;= \int (f(x)\phi (x) - g(x)\phi (x))^2 dx \\\ &amp;amp;= \lVert f\phi - g\phi \rVert_{L_2}^2 \end{align} $$&lt;/p&gt;

&lt;p&gt;Now, we can replace $f\phi$ and $g\phi$ with their respective Fourier transforms since the Fourier transform is an isometric mapping (i.e., distance remains same before and after the mapping). Therefore, we get&lt;/p&gt;

&lt;p&gt;$$ \mathbb{E}_{\mu}(f(x)-g(x))^2 = \lVert \hat{f\phi} - \hat{g\phi} \rVert_{L_2}^2 $$&lt;/p&gt;

&lt;p&gt;While this replacement may seem arbitrary at first, it has a very clear motivation. We have done this because the Fourier transform of functions expressible by a 2-layer network has a very particular form, which we will use here. Specifically, consider the function&lt;/p&gt;

&lt;p&gt;$$ f(x) = \sum_{i=1}^k f_i (&amp;lt; \mathbf{v}_i,\mathbf{x} &amp;gt;), $$&lt;/p&gt;

&lt;p&gt;which is expressible by any 2-layer network. The component function $f_i (&amp;lt; \mathbf{v}_i,\mathbf{x} &amp;gt;)$ is constant in any direction perpendicular to $\mathbf{v}_i$ and so its Fourier transform is non-zero only in the direction of $\mathbf{v}_i$, and so the whole distribution is supported on $\bigcup_i \text{span}(\mathbf{v}_i)$. Now we just need to compute the support of $\hat{\phi}$, and then we can directly use the convolution-multiplication principle.&lt;/p&gt;

&lt;p&gt;Since we haven&amp;rsquo;t yet chosen a density function, we choose $\phi$ to make the computation of support easier. Specifically, we choose $\phi$ to be the inverse Fourier transform of $\mathbb{1}\{x\in B\}$, which is the indicator function of a unit Euclidean ball. Then, $\hat{\phi}$ becomes $\mathbb{1}\{x\in B\}$ itself, and its support is simply the ball $B$. Using these, we get&lt;/p&gt;

&lt;p&gt;$$ \text{Supp}(\hat{f\phi}) \subseteq T = \bigcup_{i=1}^k (\text{span}\{ \mathbf{v}_i \} + B ), $$&lt;/p&gt;

&lt;p&gt;which is basically the union of $k$ tubes passing through origin. This is because $\text{span}\{\mathbf{v}_i\}$ is just a straight line, and $B$ is a ball. Sum here means the direct sum, i.e., for every element $a \in A$ and $b \in B$, form a set of $a+b$. So we just put the Euclidean ball on every point on the line, which gives us a cylinder passing through the origin.&lt;/p&gt;

&lt;p&gt;Since we are looking for a $g$ which cannot be approximated by the neural network, we try to make $\lVert f\phi - g\phi \rVert_{L_2}^2$ as large as possible. We have already seen what the support of $\hat{f\phi}$ looks like. Now, we want a $g$ such that&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;$g$ should have most of its mass away from the origin in all the directions, and&lt;/li&gt;
&lt;li&gt;the Fourier transform $\hat{g}$ should be outside $B$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If $g$ is chosen as a radial function, the first criteria will be satisfied if we just put large mass away from the origin in one direction. To satisfy the second criteria, $g$ should have a high-frequency component. To see why, see the following figure which shows the sine curve and its Fourier transform.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/23/fourier.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The only thing that remains to be shown is that if $\hat{g}$ contains a significant mass away from the origin, then so does $\hat{g\phi}$. But this proof is somewhat technical in nature and I avoid it here for sake of simplicity.&lt;/p&gt;

&lt;p&gt;This completes our proof for the result given in the paper. While this result is an important step in quantifying the role of depth in a neural network, it is still limited in that it only holds for radial functions. This is what I meant earlier by &amp;ldquo;less natural&amp;rdquo; problems, since in most of the common learning problems, the $(x,y)$ pairs are not generated from a simple radial distribution, and are much more complex in nature.&lt;/p&gt;

&lt;h4 id=&#34;2-exponential-separation-between-shallow-and-deep-nets&#34;&gt;2. Exponential separation between shallow and deep nets&lt;/h4&gt;

&lt;p&gt;In the proof of the previous result, the key idea was to have a high-frequency component in the function required to be approximated. This means that the function was highly oscillatory. In this paper as well, a similar oscillation argument is used to prove another important result.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For every positive integer $k$, there exists neural networks with $\theta(k^3)$ layers, $\theta(1)$ nodes per layer, and $\theta(1)$ distinct parameters, which cannot be approximated by networks with $\mathcal{O}(k)$ layers and $o(2^k)$ nodes.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This result is proven using three steps.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Functions with few oscillations poorly approximate functions with many oscillations.&lt;/li&gt;
&lt;li&gt;Functions computed by networks with few layers must have few oscillations.&lt;/li&gt;
&lt;li&gt;Functions computed by networks with many layers can have many oscillations.&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&#34;approximation-via-oscillation-counting&#34;&gt;Approximation via oscillation counting&lt;/h5&gt;

&lt;p&gt;We will first look at a metric to count oscillations of a function. For this, consider the following graph which shows functions $f$ and $g$ which are defined from $\mathbb{R}$ to $[0,1]$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/23/oscillations.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here, the horizontal line denotes $y = \frac{1}{2}$. The classifiers $\tilde{f}$ and $\tilde{g}$ obtained from $f$ and $g$ perform binary classification according to the rule $\tilde{f}(x) = \mathbb{1}[f(x)\geq \frac{1}{2}]$. Let $\mathcal{I}_f$ denote the set of partitions of $\mathbb{R}$ into intervals so that the classifier $\tilde{f}$ is constant in each interval. Then, the crossing number is defined as&lt;/p&gt;

&lt;p&gt;$$ \text{Cr}(f) = |\mathcal{I}_f|. $$&lt;/p&gt;

&lt;p&gt;From our definition of $\tilde{f}$, this clearly means that $\text{Cr}(f)$ counts the number of times that $f$ crosses the line $y = \frac{1}{2}$, and hence the name. In this way, we formalize the notion of counting the number of oscillations of a function.&lt;/p&gt;

&lt;p&gt;With this definition, if $\text{Cr}(f)$ is much larger than $\text{Cr}(g)$, then most piecewise constant regions of $\tilde{g}$ will exhibit many oscillations of $f$, and thus $g$ poorly approximates $f$.&lt;/p&gt;

&lt;p&gt;Now we will prove the following lemma, where the counting number $\text{Cr}(f)$ is denoted by $s_f$ for sake of convenience.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;$$ \frac{\text{No. of regions of }\mathcal{I}_f \text{ where} ~ \tilde{f}\neq \tilde{g}}{s_f} \geq \frac{1}{2} - \frac{s_g}{s_f} $$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now, if $s_f &amp;gt;&amp;gt; s_g$, then the RHS approximately becomes $\frac{1}{2}$, which implies that for more than half of all the regions of $f$, $\tilde{g}$ classifies $x$ incorrectly, and so $g$ is a poor approximation of $f$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; We choose a region $J$ where $\tilde{g}$ is constant but $\tilde{f}$ alternates, such as the region where $g$ is red in the above figure. We denote by $X_J$ all the partitions of $\mathcal{I}_f$ that are contained in $J$. Since $f$ oscillates within $g$, this means that $\tilde{g}$ disagrees with $\tilde{f}$ for half of all $X_J$, i.e., at least $\frac{|X_J|-1}{2}$ in general.&lt;/p&gt;

&lt;p&gt;In the LHS of the claim, we need to count all the regions of $\mathcal{I}_f$ where the classifiers disagree for all points in the region. From above, we have a lower bound on the number of such regions within one $J$. So now we just take sum over all $J \in \mathcal{I}_g$ to get&lt;/p&gt;

&lt;p&gt;$$ \frac{\text{No. of regions of }\mathcal{I}_f \text{ where} ~ \tilde{f}\neq \tilde{g}}{s_f} \geq \frac{1}{s_f}\sum_{J \in \mathcal{I}_g} \frac{|X_J|-1}{2}. $$&lt;/p&gt;

&lt;p&gt;Now we need to bound $s_f$. For this, see that the total number of oscillations of $f$ are at least its number of oscillations within a single partition of $\mathcal{I}_g$ summed over all such partitions. I say &amp;ldquo;at least&amp;rdquo; because this will not include those partitions of $\mathcal{I}_f$ whose interior intersects with the boundary of an interval in $\mathcal{I}_g$. At most, there would be $s_g$ such partitions, and so&lt;/p&gt;

&lt;p&gt;$$ s_f \leq s_g + \sum_{J\in \mathcal{I}_g}|X_J|. $$&lt;/p&gt;

&lt;p&gt;This means that $\sum_{J\in \mathcal{I}_g}|X_J| \geq s_f - s_g$. Using this bound in the previously obtained inequality, we get the desired result.&lt;/p&gt;

&lt;h5 id=&#34;few-layers-few-oscillations&#34;&gt;Few layers, few oscillations&lt;/h5&gt;

&lt;p&gt;Adding more nodes is similar to adding polynomials, while adding layers is like composition of polynomials. Adding polynomials yields a new polynomial with degree equal to the higher of the two and at most twice as many terms, but composing them (i.e. taking product) would yield a polynomial with higher degree and more than the product of terms. Clearly, composition would lead to more number of roots of the new polynomial. This suggests that adding layers should lead to a higher number of oscillations than adding nodes.&lt;/p&gt;

&lt;p&gt;Let $f$ be a function computed by the neural network $\mathcal{N}((m_i,t_i,\alpha_i,\beta_i)_{i=1}^l)$, i.e. a network of $l$ layers where the $i$th layer has $m_i$ nodes, such that the activation function at each node is $(t,\alpha)-poly$ (a piecewise function containing $t$ parts where each piece is a polynomial of degree at most $\alpha$). Then, we claim that&lt;/p&gt;

&lt;p&gt;$$ \text{Cr}(f) \leq \mathcal{O}\left( \left( \frac{tm\alpha}{l} \right)^l \beta^{l^2} \right). $$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; We will prove this in two parts. First, we bound the counting number of a $(t,\alpha)-poly$ function, and then we will show that the function $f$ as computed by the above network is $(t,\alpha)-poly$.&lt;/p&gt;

&lt;p&gt;For the first part, see that each piece of the function $f$ is a polynomial of degree at most $\alpha$, which means that each piece oscillates at most $1 + \alpha$ times. Since there are $t$ such pieces&lt;/p&gt;

&lt;p&gt;$$ \text{Cr}(f) \leq t(1+\alpha). $$&lt;/p&gt;

&lt;p&gt;Now it remains to show that the function $f$ computed by the network is indeed $(t,\alpha)-poly$. To see this, consider the function computed by a single layer. Each node in the layer computes a $(t,\alpha)-poly$ function, say $g_i$, and we apply a composition function, say $f$, on these $g_i$&amp;rsquo;s, which is a polynomial with degree at most $\gamma$. The final function computed by this layer is&lt;/p&gt;

&lt;p&gt;$$ h(x) = f(g_1(x),\ldots,g_k(x)). $$&lt;/p&gt;

&lt;p&gt;To visualize such a composition, consider the following figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/23/poly.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here, each horizontal line denotes one node&amp;rsquo;s partition function, i.e., $\tilde{g_i}$. There are $k$ such lines with at most $t$ intervals each. The composition takes the union of all the partitions of all these lines. As such, the maximum number of intervals after composition will be equal to $kt$. Within each such interval, since we are taking a composition of a degree $\gamma$ polynomial with one with degree at most $\alpha$, the resulting polynomial has degree at most $\alpha \gamma$. Hence, $h$ is $(tk,\alpha\gamma)-poly$.&lt;/p&gt;

&lt;p&gt;Since there are $l$ layers and the total number of nodes in the network is $m$, it implies there are $\frac{m}{l}$ nodes on average in each layer, and each node has at most $t$ intervals. So after every layer, the number of intervals gets multiplied by a factor of $\frac{mt}{l}$. Finally, the total number of intervals will be of the order $\left(\frac{mt}{l}\right)^l$.&lt;/p&gt;

&lt;p&gt;Similarly, the degree of resulting function gets multiplied by $\alpha$ after every layer, so the final degree is of the order $\alpha^l$. Using the result shown in the first part, the resulting function will have a counting number bounded by $\mathcal{O}\left(\frac{tm\alpha}{l} \right)^l$.&lt;/p&gt;

&lt;p&gt;The $\beta$ term comes due to technicalities associated with taking an activation function which is semi-algebraic rather than piecewise polynomial, but the proof technique remains the same.&lt;/p&gt;

&lt;h5 id=&#34;many-layers-many-oscillations&#34;&gt;Many layers, many oscillations&lt;/h5&gt;

&lt;p&gt;In the figure that I showed for explaining counting number, notice that oscillations usually (always?) mean repetitions of a triangle-like function (strictly increasing till some point and then strictly decreasing thereafter). Also, the usual functions computed by a single layer of most of the common neural networks are like these triangular functions.&lt;/p&gt;

&lt;p&gt;In the last result, we used the composition of $(t,\alpha)-poly$ functions across several layers to bound the counting number of a network. Similarly in this section, we will use the concept of a $(t,[a,b])$-triangle. It represents a function which is continuous in $[a,b]$ and consists of $t$ triangle-like pieces. Also, since this function oscillates $2t$ times, its counting number is $2t+1$.&lt;/p&gt;

&lt;p&gt;Now it remains to show that the composition of 2 such functions gives a similar function (which is a similar technique to what we used earlier). More formally, we will prove this claim.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Claim:&lt;/strong&gt; If $f$ is a $(s,[0,1])$-triangle and $g$ is a $(t,[0,1])$-triangle, then $f \circ g$ is a $(2st,[0,1])$-triangle.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; First, we note that $f \circ g$ is continuous in $[0,1]$ since a composition of continuous functions is continuous in the same domain.&lt;/p&gt;

&lt;p&gt;Now, consider any odd (i.e., strictly increasing) interval $g_j$ of $g$. Suppose $(a_1,\ldots,a_{2s+1})$ are the interval boundaries of $f$. Since the range of $g_j$ is $[0,1]$, $g_j^{-1}(a_i)$ exists for all $i$ and is unique, since $g_j$ is strictly increasing. Let $a_i^{\prime}=g_j^{-1}(a_i)$, i.e., $g_j(a_i^{\prime})=a_i$. If $i$ is odd, the composition $f \circ g_j(a_i^{\prime}) = f(a_i)=0$, and $f \circ g_j$ is strictly increasing in $[a_i^{\prime},a_{i+1}^{\prime}]$, since $g_j$ is strictly increasing everywhere and $f$ is strictly increasing in $[a_i,a_{i+1}]$. By a similar argument, if $i$ is even, $f \circ g_j$ is strictly decreasing along $[a_i^{\prime},a_{i+1}^{\prime}]$. In this way, we get $2s$ triangular pieces for a single $g_j$, and so the overall composition $f \circ g$ has $2st$ triangular pieces.&lt;/p&gt;

&lt;p&gt;Having shown this, it is easy to see that if there are $l$ layers and each layer computes a $(t,[0,1])$-triangle, the final layer will output a $((2t)^l,[0,1])$-triangle. In this way, the counting number of the overall function becomes $(2t)^l + 1$.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;implicit-acceleration-by-overparametrization&#34;&gt;Implicit acceleration by overparametrization&lt;/h3&gt;

&lt;p&gt;In the previous section, we have seen some results which show that depth plays a role in the expressive capacity of neural networks. Specifically, we saw that:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Radial functions can be approximated by depth-3 networks but not with depth-2 networks.&lt;/li&gt;
&lt;li&gt;Functions expressible by $\theta(k^3)$-depth networks of constant width cannot be approximated by $\mathcal{O}(k)$-depth networks with polynomial width.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this section, we will look at a &lt;a href=&#34;https://arxiv.org/pdf/1802.06509.pdf&#34; target=&#34;_blank&#34;&gt;new paper from Arora, Cohen, and Hazan&lt;/a&gt; that suggests that, sometimes, increasing depth can speed up optimization (which is rather counterintuitive given the consensus on expressiveness vs. optimization trade-off), i.e., depth plays some role in convergence. Furthermore, this acceleration is more than what could be obtained by commonly used techniques, and is theoretically shown to be a combination of momentum and adaptive regularization (which we will discuss later).&lt;/p&gt;

&lt;p&gt;To isloate convergence from expressiveness, the authors focus solely on linear neural networks, where increasing depth has no impact on the expressiveness of the network. This is because in such networks, adding layers manifests itself only in the replacement of a matrix parameter by a product of matrices – an
overparameterization.&lt;/p&gt;

&lt;h4 id=&#34;equivalence-to-adaptive-learning-rate-and-momentum&#34;&gt;Equivalence to adaptive learning rate and momentum&lt;/h4&gt;

&lt;p&gt;The first result that we prove is the following.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Overparametrized gradient descent with small learning rate and near-zero initialization is equivalent to GD with adaptive learning rate and momentum terms.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; This can be seen by simple analysis of gradients for an $l_p$-regression with parameter $\mathbf{w}\in \mathbb{R}^d$. The loss function can be given as&lt;/p&gt;

&lt;p&gt;$$ L(\mathbf{w}) = \mathbb{E}_{(\mathbf{x},y)\sim S}\left[ \frac{1}{p}(\mathbf{x}^T\mathbf{w} - y)^p \right]. $$&lt;/p&gt;

&lt;p&gt;Now, if we add a scalar parameter, the new parameters are $\mathbf{w}_1$ and $w_2 \in \mathbb{R}$, i.e., $\mathbf{w} = w_2 \mathbf{w}_1$, and we can write the new loss function as&lt;/p&gt;

&lt;p&gt;$$ L(\mathbf{w}_1,w_2) = \mathbb{E}_{(\mathbf{x},y)\sim S}\left[ \frac{1}{p}(\mathbf{x}^T\mathbf{w}_1 w_2 - y)^p \right]. $$&lt;/p&gt;

&lt;p&gt;We can now compute the gradients of the objective with respect to the parameters as&lt;/p&gt;

&lt;p&gt;$$ \nabla_{\mathbf{w}} = \mathbb{E}_{(\mathbf{x},y)\sim S}\left[ (\mathbf{x}^T\mathbf{w} - y)^{p-1}\mathbf{x} \right] $$&lt;/p&gt;

&lt;p&gt;$$ \nabla_{\mathbf{w}_1} = \mathbb{E}_{(\mathbf{x},y)\sim S}\left[ (\mathbf{x}^T\mathbf{w}_1 w_2 - y)^{p-1}w_2\mathbf{x} \right] = w_2 \nabla_{\mathbf{w}} $$&lt;/p&gt;

&lt;p&gt;$$ \nabla_{w_2} = \mathbb{E}_{(\mathbf{x},y)\sim S}\left[ (\mathbf{x}^T\mathbf{w} - y)^{p-1}\mathbf{w}_1^T \mathbf{x} \right] $$&lt;/p&gt;

&lt;p&gt;The update rules for $\mathbf{w}_1$ and $w_2$ can be given as&lt;/p&gt;

&lt;p&gt;$$ \mathbf{w}_1^{(t+1)} = \mathbf{w}_1^{(t)} - \eta \nabla_{\mathbf{w}_1}^{(t)} \quad \text{and} \quad w_2^{(t+1)} = w_2^{(t)} - \eta \nabla_{w_2}^{(t)}, $$&lt;/p&gt;

&lt;p&gt;and the updated parameter $\mathbf{w}$ is&lt;/p&gt;

&lt;p&gt;$$ \begin{align} \mathbf{w}^{(t+1)} &amp;amp;= \mathbf{w}_1^{(t+1)} w_2^{(t)} \\\ &amp;amp;= \left( \mathbf{w}_1^{(t)} - \eta \nabla_{\mathbf{w}_1}^{(t)} \right) \left( w_2^{(t)} - \eta \nabla_{w_2}^{(t)} \right) \\\ &amp;amp;= \mathbf{w}_1^{(t)}w_2^{(t)} - \eta w_2^{(t)}\nabla_{\mathbf{w}_1^{(t)}} - \eta \nabla_{w_2^{(t)}}\mathbf{w}_1^{(t)} + \mathcal{O}(\eta^2) \\\ &amp;amp;= \mathbf{w}^{(t)} - \eta \left( w_2^{(t)} \right)^2 \nabla_{\mathbf{w}^{(t)}} -\eta \left( w_2^{(t)} \right)^{-1} \nabla_{w_2^{(t)}} \mathbf{w}^{(t)} + \mathcal{O}(\eta^2). \end{align}$$&lt;/p&gt;

&lt;p&gt;We can ignore $\mathcal{O}(\eta^2)$ since the learning rate is assumed to be low. Also, we take $\rho^{(t)} = \eta(w_2^{(t)})^2$ and $\gamma^{(t)}=\eta(w_2^{(t)})^{-1}\nabla_{w_2^{(t)}}$, so the update becomes&lt;/p&gt;

&lt;p&gt;$$ \mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - \rho^{(t)}\nabla_{\mathbf{w}^{(t)}} - \gamma^{(t)}\mathbf{w}^{(t)}. $$&lt;/p&gt;

&lt;p&gt;Since $\mathbf{w}$ is initialized near $0$, it is essentially a weighted combination of the past gradients at any given time, i.e., $\gamma^{(t)}\mathbf{w}^{(t)} = \sum_{\tau=1}^{t-1}\mu^{(t,\tau)}\nabla_{\mathbf{w}^{(\tau)}}$.&lt;/p&gt;

&lt;p&gt;This is similar to the momentum term in the popular momentum algorithm for optimization (see &lt;a href=&#34;https://abhilashasancheti.github.io/post/short-note-sgd-algorithms/&#34; target=&#34;_blank&#34;&gt;this earlier post&lt;/a&gt; for an overview), and the learning rate term $\rho^{(t)}$ is time-varying and adaptive.&lt;/p&gt;

&lt;h4 id=&#34;update-rule-for-end-to-end-matrix&#34;&gt;Update rule for end-to-end matrix&lt;/h4&gt;

&lt;p&gt;The next derivation is a little more involved, and I defer the reader to the actual paper for the detailed proof. I will give a brief outline here.&lt;/p&gt;

&lt;p&gt;Suppose we have a depth-$N$ linear network such that the weight matrices are given by $W_1,\ldots,W_N$. Let $W_e$ denote the final end-to-end update matrix. The authors use differential techniques to compute an update rule for $W_e$. For this, the important assumption is that $\eta^2 \approx 0$. When step sizes are taken to be small, trajectories of discrete optimization algorithms converge to smooth curves modeled by continuous-time differential equations.&lt;/p&gt;

&lt;p&gt;After obtaining such a differential equation, integration over the $N$ layers gives the derivative of $W_e$, which is then transformed back to the discrete update rule given as&lt;/p&gt;

&lt;p&gt;$$ W_e^{(t+1)} = (1 - \eta\lambda N)W_e^{(t)} - \eta \sum_{i=1}^N \left[ W_e^{(t)} (W_e^{(t)})^T \right]^{\frac{j-1}{N}} \frac{\partial L^1}{\partial W}(W_e^{(t)}) \cdot \left[ (W_e^{(t)})^T W_e^{(t)} \right]^{\frac{N-j}{N}}. $$&lt;/p&gt;

&lt;p&gt;Let us break down this expression. The first part is similar to a weight-decay term for a 1-layer update. The second part also has the derivative w.r.t parameters, but it is multiplied by some preconditioning terms. On further inspection of these terms, it is found that their eigenvalues and eigenvectors depend on the singular value decomposition of $W_e$. Qualitatively, this means that these multipliers favor the gradient along those directions that correspond to singular values whose presence in $W_e$ is stronger. If we assume, as is usually the case in deep learning, that the initialization was near 0, this means that these multipliers act similar to acceleration and push the gradient along the direction already taken by the optimization.&lt;/p&gt;

&lt;p&gt;For further reading, check out the author&amp;rsquo;s &lt;a href=&#34;http://www.offconvex.org/2018/03/02/acceleration-overparameterization/&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt; about the paper.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;To summarize, we looked at three recent papers which prove results on the role of depth in expressibility and optimization of neural networks. People often think that working on the mathematics of deep learning would require complex group theory formalisms and difficult techniques in high-dimensional probability, but as we saw in the proofs of some of these results (especially in Telgarsky&amp;rsquo;s paper), a lot can be achieved using simple counting logic and concentration inequalities.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Theory of Deep Learning: Generalization</title>
      <link>https://abhilashasancheti.github.io/post/deep-learning-theory-2/</link>
      <pubDate>Fri, 27 Jul 2018 13:45:11 +0530</pubDate>
      
      <guid>https://abhilashasancheti.github.io/post/deep-learning-theory-2/</guid>
      <description>

&lt;p&gt;In &lt;a href=&#34;https://abhilashasancheti.github.io/post/deep-learning-theory-1/&#34; target=&#34;_blank&#34;&gt;Part 1&lt;/a&gt; of this series, based on the ICML 2018 tutorial on &amp;ldquo;&lt;a href=&#34;http://unsupervised.cs.princeton.edu/deeplearningtutorial.html&#34; target=&#34;_blank&#34;&gt;Toward a Theory for Deep Learning&lt;/a&gt;&amp;rdquo; by &lt;a href=&#34;https://www.cs.princeton.edu/~arora/&#34; target=&#34;_blank&#34;&gt;Prof. Sanjeev Arora&lt;/a&gt;, we looked at several aspects of optimization of the nonconvex objective function that is a part of most deep learning models. In this article, we will turn our attention to another important aspect, namely generalization.&lt;/p&gt;

&lt;p&gt;A distinguishing feature of most modern deep learning architectures is that they generalize to test cases exceptionally well, even though the number of parameters is far greater than the number of training samples. VGG19, for instance, which has approximately 20 million weights to be tuned, gives $\sim 93\%$ classification accuracy on CIFAR-10, which has only 50000 training images. If you have studied statistical learning theory (see my &lt;a href=&#34;https://abhilashasancheti.github.io/post/intro-learning-theory-1/&#34; target=&#34;_blank&#34;&gt;previous&lt;/a&gt; &lt;a href=&#34;https://abhilashasancheti.github.io/post/intro-learning-theory-2/&#34; target=&#34;_blank&#34;&gt;blogs&lt;/a&gt; on the topic), this behavior is extremely counter-intuitive, and begs the question: &lt;em&gt;why don&amp;rsquo;t deep neural networks overfit even with small number of training samples?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Before we try to understand the reason, let us look at a popular folklore experiment that is described in &lt;a href=&#34;http://www.cs.princeton.edu/~rlivni/files/papers/LivnComputational.pdf&#34; target=&#34;_blank&#34;&gt;Livni et al &amp;lsquo;14&lt;/a&gt; related to over-specification.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For sufficiently over-specified networks, global optima are ubiquitous and in general computationally easy to find.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To see this, we fix a depth-2 neural network (i.e. a network with 1 hidden layer) consisting of $n$ hidden nodes. We provide random inputs to the network and obtain their corresponding output. Now, take a randomly initialized neural network with the same architecture as the above, and train it using the input-output pairs obtained earlier. It is found that this is really difficult to achieve. However, if we take a large number of hidden nodes, the training becomes easier.&lt;/p&gt;

&lt;p&gt;Although this result has been known and verified empirically for some time, it remains to be proven theoretically. This is a striking example of the difficulty of proving generalization guarantees in deep learning.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;effective-capacity-of-learning&#34;&gt;Effective capacity of learning&lt;/h3&gt;

&lt;p&gt;The capacity of a learning model, in an abstract sense, means the complexity of training samples that it can fit. For instance, a quadratic regression has inherently more capacity than linear regression, but is also more prone to overfitting. Furthermore, the effective capacity can be thought of as analogous to the number of bits required to represent all possible states that the hypothesis class contains. For this reason, the capacity is approximately the log of the number of apriori functions in the hypothesis class.&lt;/p&gt;

&lt;p&gt;We will now see a general result that is true for learning models including deep neural networks.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Claim:&lt;/strong&gt; Test loss - training loss $\leq \sqrt{\frac{N}{m}}$, where $N$ is the effective capacity and $m$ is the number of training samples.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; First let us fix our neural network $\theta$ and its parameters. Suppose we take an i.i.d sample $S$ containing $m$ data points. Consider &lt;em&gt;Hoeffding&amp;rsquo;s inequality&lt;/em&gt;: If $x_1,\ldots,x_m$ are $m$ i.i.d samples of a random variable $X$ distributed by $P$, and $a\leq x_i \leq b$ for every $i$, then for a small postive non-zero value $\epsilon$:&lt;/p&gt;

&lt;p&gt;$$ P\left( \mathbb{E}_{X \sim P} - \frac{1}{m}\sum_{i=1}^m x_i \right) \leq 2\exp \left( \frac{-2m\epsilon^2}{(b-a)^2} \right) $$&lt;/p&gt;

&lt;p&gt;We can apply this inequality to our generalization probability, assuming that our errors are bounded between 0 and 1 (which is a reasonable assumption, as we can get that using a 0/1 loss function or by squashing any other loss between 0 and 1) and get for a single hypothesis $h$:&lt;/p&gt;

&lt;p&gt;$$ P(|R(h) - \hat{R}(h)| &amp;gt; \epsilon) \leq 2\exp (-2m\epsilon^2), $$&lt;/p&gt;

&lt;p&gt;where $R(h)$ denotes generalization error and $\hat{R}(h)$ denotes empirical error on the sample.&lt;/p&gt;

&lt;p&gt;However, this is not the true generalization bound. This is because we have first fixed out network and we are then choosing the sample i.i.d. However, in a real learning problem, we are given the sample $S$ and we have to learn the parameters to best fit this sample. Therefore, to obtain the actual generalization bound, we take the union bound over all possible neural net configurations $\mathcal{W}$. Now, equating the RHS with the confidence $\delta$, we get&lt;/p&gt;

&lt;p&gt;$$ \begin{align} &amp;amp; 2\mathcal{W}\exp(-2m\epsilon^2) \leq \delta \\\ \Rightarrow &amp;amp; -2m\epsilon^2 \leq \log \frac{\delta}{2\mathcal{W}} \\\ \Rightarrow &amp;amp; \epsilon \geq \sqrt{\frac{\log \frac{2\mathcal{W}}{\delta}}{2m}}, \end{align} $$&lt;/p&gt;

&lt;p&gt;which completes the proof.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In statistical learning theory, the most popular metrics for measuring the capacity of a model are Rademacher complexity and VC dimension, which I have explained in &lt;a href=&#34;https://abhilashasancheti.github.io/post/intro-learning-theory-2/&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt;. I will quickly summarize them here.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Rademacher complexity:&lt;/strong&gt; It is a measure of how well the model can fit a random assignment of labels. Its mathematical formulation is:&lt;/p&gt;

&lt;p&gt;$$ \hat{\mathcal{R}_S}(G) = \mathbb{E}_{\sigma}[\text{sup}_{g\in G}\frac{1}{m}\sigma_i g(z_i)] $$&lt;/p&gt;

&lt;p&gt;Essentially, it denotes an expectation of the best possible average correlation that the random labels have with any function present in the hypothesis class $G$. Therefore, a higher Rademacher complexity would imply that the function class $G$ is able to fit a random assignment of labels well, and vice versa. This is because the more complex a class $G$ is, higher is the probability that it would have some $g$ which correlates well with random noise.&lt;/p&gt;

&lt;p&gt;The generalization error $R(h)$ can be written in terms of R.C. as&lt;/p&gt;

&lt;p&gt;$$ R(h) \leq \hat{R}(h) + \mathcal{R}_m(H) + \sqrt{\frac{\log \frac{1}{\delta}}{2m}}, $$&lt;/p&gt;

&lt;p&gt;where $\hat{R}(h)$ is the empirical error, $\delta$ is the confidence, and $m$ is the number of training samples.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;VC dimension:&lt;/strong&gt; It is the size of the largest set that can be fully shattered by $G$. By shattering, we mean that $G$ can classify the given set in all possible ways. As such, higher the VC-dimension, more is the capacity of the hypothesis class. We can bound the generalization error in terms of the VC-dimension of the hypothesis class as&lt;/p&gt;

&lt;p&gt;$$ R(h) \leq \hat{R}(h) + \mathcal{O}\left( \sqrt{\frac{\log(m/d)}{m/d}} \right) $$&lt;/p&gt;

&lt;p&gt;Although these metrics are well established in learning theory, they fail for deep neural networks since they are usually equally vacuous, i.e, the upper bound is greater than 1. This means that the bounds are so large that they are meaningless, since error can never exceed 1, and in practice the generalization error of the networks is many orders of magnitude less than these bounds.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;deep-networks-have-excess-capacity&#34;&gt;Deep networks have &amp;ldquo;excess capacity&amp;rdquo;&lt;/h3&gt;

&lt;p&gt;As mentioned earlier, deep neural networks generalize surprisingly well despite having a huge number of parameters. They can be shown by the dotted red line (figure taken from tutorial slides) in the following popular figure which is often found in textbooks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/22/generalize.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Other learning models with a &amp;ldquo;high capacity&amp;rdquo; would follow the general trend and fail to generalize well, which may be evidence that somehow, the large number of parameters in deep networks is not necessarily translating to a high capacity. For a long time, it was believed that a combination of stochastic gradient descent and regularization eliminates the &amp;ldquo;excess capacity&amp;rdquo; of the neural network.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;But this belief is wrong!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In their &lt;a href=&#34;https://arxiv.org/abs/1611.03530&#34; target=&#34;_blank&#34;&gt;ICLR &amp;lsquo;17 paper&lt;/a&gt; (which I have previously discussed in &lt;a href=&#34;https://abhilashasancheti.github.io/post/best-papers-at-iclr-17/&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt;), Zhang et. al., in a series of well-designed experiments, showed that deep networks do retain this excess capacity. From &lt;a href=&#34;http://www.offconvex.org/2017/12/08/generalization1/&#34; target=&#34;_blank&#34;&gt;Prof. Arora&amp;rsquo;s blog post&lt;/a&gt; on the subject: &amp;ldquo;Their main experimental finding is that if you take a classic convnet architecture, say Alexnet, and train it on images with random labels, then you can still achieve very high accuracy on the training data. (Furthermore, usual regularization strategies, which are believed to promote better generalization, do not help much.) Needless to say, the trained net is subsequently unable to predict the (random) labels of still-unseen images, which means it doesn’t generalize.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/22/iclr-17.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;An interesting (and provable) guarantee that the paper contains is the following theorem: &lt;em&gt;There exists a two-layer neural network with ReLU activations and $2n+d$ weights that can represent any function on a sample of size $n$ in $d$ dimensions.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In a &lt;a href=&#34;https://arxiv.org/abs/1802.01396&#34; target=&#34;_blank&#34;&gt;related paper&lt;/a&gt; published recently, it was shown that the &amp;ldquo;excess capacity&amp;rdquo; is not just limited to deep networks, since even linear models possess this feature. Furthermore, when it comes to fitting noise, there are some interesting similarities between Laplacian kernel machines and ReLU networks. But before we get to that, I will briefly define Laplacian and Gaussian kernels. (For an overview of several kernel functions, check out &lt;a href=&#34;http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/&#34; target=&#34;_blank&#34;&gt;this article&lt;/a&gt;.)&lt;/p&gt;

&lt;blockquote&gt;
&lt;h4 id=&#34;kernel-methods&#34;&gt;Kernel Methods&lt;/h4&gt;

&lt;p&gt;Kernel methods map the data into higher-dimensional spaces, in the hope that in this higher-dimensional space the data could become more easily separated or better structured. However, when we talk about transforming data to a higher dimension, called a $z$-space, an actual transformation would involve paying computation costs. To avoid this, we need to look at what we actually want from the $z$-space.&lt;/p&gt;

&lt;p&gt;Support Vector Machines (SVMs), which are among the most popular kernel-based methods for classification, involve solving for the following Lagrangian.&lt;/p&gt;

&lt;p&gt;$$ \mathcal{L}(\alpha) = \sum_{n=1}^N \alpha_n - \frac{1}{2}\sum_{n=1}^N \sum_{m=1}^M y_n y_m \alpha_n \alpha_m z_n^T z_m $$&lt;/p&gt;

&lt;p&gt;under the constraints $\alpha_n \geq 0 \forall n$ and $\sum_{n=1}^N \alpha_n y_n = 0$. On solving this, we get the boundary as&lt;/p&gt;

&lt;p&gt;$$ g(x) = \text{sgn}(w^T z + b) $$&lt;/p&gt;

&lt;p&gt;where $w = \sum_{z_n \in SV} \alpha_n y_n z_n$.&lt;/p&gt;

&lt;p&gt;We can see from this that the only value we need from the $z$-space is the inner product $z^T z^{\prime}$. If we can show that obtaining this inner product is possible without actually going to the $z$-space, we are done.&lt;/p&gt;

&lt;p&gt;It turns out that this is indeed possible, and there are several such functions, known as &lt;strong&gt;kernel functions&lt;/strong&gt;, which can be written as the inner product in some space. The only constraint on the $z$-space is that it should exist. Interestingly, kernels such as the radial basis function (RBF) kernel exist in an $\infty$-dimensional space. Furthermore, in order for the problem to be convex and have a unique solution, it is important to select a positive semi-definite kernel, i.e., whose kernel matrix contain only non-negative eigenvalues. Such a kernel is said to obey &lt;a href=&#34;https://en.wikipedia.org/wiki/Mercer&#39;s_theorem&#34; target=&#34;_blank&#34;&gt;Mercer&amp;rsquo;s theorem&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now that we have some idea what kernels are, let us look at Laplacian and Gaussian kernels.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Laplacian kernel:&lt;/strong&gt; It is mathematically defined as&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ K(x,y) = \exp \left( - \frac{\lVert x-y \rVert}{\sigma} \right). $$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gaussian kernel:&lt;/strong&gt; Its mathematical formulation is&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ K(x,y) = \exp \left( - \frac{\lVert x-y \rVert^2}{2\sigma^2} \right). $$&lt;/p&gt;

&lt;p&gt;Both the Laplacian and Gaussian kernels are examples of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Radial_basis_function&#34; target=&#34;_blank&#34;&gt;radial basis function&lt;/a&gt; kernels. The difference lies only in the parameter $\sigma$. Since the Gaussian depends on the square of this parameter, it is more sensitive to changes in $\sigma$ than the Laplacian.&lt;/p&gt;

&lt;p&gt;The authors found in their empirical evaluations that Laplacian kernels were much more adept at fitting random labels than Gaussian kernels. This property may be attributed to the inherent non-smoothness of Laplacians as opposed to the Gaussians being smooth. This discontinuity in derivative is reminiscent of that for ReLU units, which, as we saw above, were found to fit random labels exceptionally well. As such, the conjecture is that the radial structure of the kernels, as opposed to the specifics of optimization, plays a key role in ensuring strong classification performance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/22/laplace.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Another take-away from this paper is that they establish stronger bounds for classification performance of kernel methods. If understanding kernels can indeed lead to a better understanding of deep learning, then maybe these bounds will lead to tighter bounds for the effective capactity of deep neural networks.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;other-notions-of-generalizability&#34;&gt;Other notions of generalizability&lt;/h3&gt;

&lt;p&gt;We now look at 2 other concepts that seek to explain why deep neural networks generalize well: flat minima, and noise stability.&lt;/p&gt;

&lt;h4 id=&#34;flat-minima&#34;&gt;Flat minima&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/22/minima.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://papers.nips.cc/paper/899-simplifying-neural-nets-by-discovering-flat-minima.pdf&#34; target=&#34;_blank&#34;&gt;Hochreiter and Schmidhuber&lt;/a&gt; first conjectured that the flatness of the local minima found by the stochastic gradient descent may be an indicator of its generalization performance. Sharpness of a minimizer can be characterized by the magnitude of the eigenvalues of $\nabla^2 f(x)$, but since the computation of this quantity is expensive, &lt;a href=&#34;https://arxiv.org/pdf/1609.04836.pdf&#34; target=&#34;_blank&#34;&gt;Keskar et. al.&lt;/a&gt; defined a new metric for sharpness that is easier to compute.&lt;/p&gt;

&lt;p&gt;Given $x \in \mathbb{R}^n$, $\epsilon &amp;gt; 0$, and $A \in \mathbb{R}^{n \times p}$, the $(C_{\epsilon},A)$-sharpness of $f$ at $x$ is defined as&lt;/p&gt;

&lt;p&gt;$$ \phi_{x,f}(\epsilon,A) = \frac{(\max_{y\in C_{\epsilon}} f(x+Ay))-f(x)}{1+f(x)}\times 100 $$&lt;/p&gt;

&lt;p&gt;The metric is based on exploring a small neighborhood of a solution and computing the largest value that $f$ can attain in that neighborhood. We use that value to measure the sensitivity of the training function at the given local minimizer.&lt;/p&gt;

&lt;p&gt;Intuitively, flat minima have lower description lengths (since less information is required to represent a flat surface), and consequently, fewer number of models are possible with this length. The effective capactiy thus becomes less, and so the hypothesis is able to generalize well.&lt;/p&gt;

&lt;p&gt;However, &lt;a href=&#34;https://arxiv.org/abs/1703.04933&#34; target=&#34;_blank&#34;&gt;recent research&lt;/a&gt; suggests that flatness is sensitive to reparametrizations of the neural network: we can reparametrize a neural network without changing its outputs while making sharp minima look arbitrarily flat and vice versa. As a consequence the flatness alone cannot explain or predict good generalization.&lt;/p&gt;

&lt;p&gt;As Prof. Arora pointed out in his talk, most of the existing theory that tries to explain generalization is only doing a &amp;ldquo;postmortem analysis&amp;rdquo;. This means that they look at some property $\phi$ that is seemingly possessed by a few neural networks that generalize well, and they argue that the generalization is due to this property. The notion of &amp;ldquo;flat minima&amp;rdquo; is a prime example of this. However, &lt;em&gt;correlation is not causation.&lt;/em&gt; Instead of such a qualitative check, the theoretical approach would be to use the property $\phi$ to compute an upper bound on the number of possible neural networks that would generalize well with this property. This computation is very nontrivial and is therefore ignored.&lt;/p&gt;

&lt;h4 id=&#34;noise-stability&#34;&gt;Noise stability&lt;/h4&gt;

&lt;p&gt;While flat minima was an old concept, the notion of noise stability is a very recent formalization for the same, proposed in &lt;a href=&#34;https://arxiv.org/abs/1802.05296&#34; target=&#34;_blank&#34;&gt;Prof. Arora&amp;rsquo;s ICML&amp;rsquo;18 paper&lt;/a&gt;. Essentially, it means that if we add some zero-mean Gaussian noise at an intermediate output of a neural network, the noise gets attenuated as the signal moves to higher layers. Therefore, the capacity of a network to fit random noise can be measured by adding a Gaussian noise at an intermediate layer and measuring the change in output at higher layers.&lt;/p&gt;

&lt;p&gt;This is also biologically inspired, since neurologists believe that single neurons are extremely susceptible to errors. However, the fact that we still function well suggests that there must be some mechanism to attenuate these errors.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Noise stability implies compressibility.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;First, what is meant by compression of a neural network? Given a network $C$ with $N$ parameters and some training loss, compression means obtaining a new network $C^{\prime}$ containing $N^{\prime}$ parameters ($N^{\prime} &amp;lt; N$), such that the training loss effectively remains the same. From the generalization claim proved earlier, this compression would mean better generalization capability for the network $C^{\prime}$.&lt;/p&gt;

&lt;p&gt;Now, let us consider a depth-2 network consisting only of linear transformations. This network can be represented by some matrix $M$, which transforms input $x$ to output $Mx$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/22/compression.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In the above figure, $\eta$ is a zero-mean Gaussian noise that is added to the input. We say that the matrix $M$ is noise stable, i.e. $M(x+\eta)\approx Mx$. This means that $\frac{|Mx|}{|x|} &amp;gt;&amp;gt; \frac{|M\eta|}{|\eta|}$. Here, the value $\frac{|Mx|}{|x|}$ is at most equal to the largest singular value of $M$, which we denote by $\sigma_{\max}(M)$. The RHS is approximately $\frac{(\sum_i (\sigma_i (M))^2)^{\frac{1}{2}}}{\sqrt{n}}$ where $\sigma_i(M)$ is the $i$th singular value of $M$ and $n$ is dimension of $Mx$. The reason is that gaussian noise divides itself evenly across all directions, with variance in each direction $1/n$. Thus,&lt;/p&gt;

&lt;p&gt;$$ (\sigma_{max}(M))^2 \gg \frac{1}{h} \sum_i (\sigma_i(M)^2) $$&lt;/p&gt;

&lt;p&gt;The ratio of the LHS to the RHS in the above inequality is known as the &lt;em&gt;stable rank&lt;/em&gt;. Higher the stable rank, more uneven is the distribution of singular values in the matrix. This is easily seen since the highest singular value is much larger than the RMS of all the singular values, something similar to the following figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/22/singular.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The actual signal $x$ is usually correlated with the eigenvectors corresponding to the larger singular values, and as such, the other directions can be ignored without any loss in performance. This is similar to feature selection by a principal component analysis approach.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;nonvacuous-bounds-for-true-capacity&#34;&gt;Nonvacuous bounds for true capacity&lt;/h3&gt;

&lt;p&gt;We have earlier seen that most of the classical metrics used for bounding the generalization error in learning systems prove to be vacuous in case of deep neural networks. The following blog posts by Prof. Arora discuss this issue in some detail and also introduce a new generalization bound based on the compressibility of neural networks explained in the previous section.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.offconvex.org/2017/12/08/generalization1/&#34; target=&#34;_blank&#34;&gt;Generalization theory and deep nets, an introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.offconvex.org/2018/02/17/generalization2/&#34; target=&#34;_blank&#34;&gt;Proving generalization of deep nets via compression&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this section, I will discuss two approaches for computing nonvacuous bounds for deep networks. The first is from &lt;a href=&#34;https://arxiv.org/pdf/1703.11008.pdf&#34; target=&#34;_blank&#34;&gt;Dziugaite and Roy&lt;/a&gt;, and the second is from &lt;a href=&#34;https://arxiv.org/pdf/1802.05296.pdf&#34; target=&#34;_blank&#34;&gt;Prof. Arora&amp;rsquo;s ICML&amp;rsquo;18 paper&lt;/a&gt; mentioned previously.&lt;/p&gt;

&lt;p&gt;As discussed earlier, a common framework for addressing this problem would involve showing under certain assumptions that either SGD performs implicit regularization, or that it finds a solution with some known structure connected to regularization. Once this is found, a nonvacuous bound for the generalization error of such models would have to be determined.&lt;/p&gt;

&lt;h4 id=&#34;1-pac-bayes-approach&#34;&gt;1. PAC-Bayes approach&lt;/h4&gt;

&lt;p&gt;The first question is how to identify structure in the solutions found by SGD? For this, we again turn to the old notion of flat minima. If SGD finds a flat minima, it means that the solution is surrounded by a large volume of solutions that are nearly as good. If we then represent these nearby solutions by some distribution and pick an average classifier from this distribution, it would be very likely that its generalization error is very close to that of the true solution.&lt;/p&gt;

&lt;p&gt;This concept is very similar to the PAC-Bayes theorem, which informally bounds the expected error of a classifier chosen from a distribution $Q$ in terms of its KL divergence from a priori fixed distribution $P$. But first, &lt;em&gt;what is KL divergence?&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;h5 id=&#34;kullback-leibler-divergence&#34;&gt;Kullback-Leibler divergence&lt;/h5&gt;

&lt;p&gt;It is a metric that compares the similarity between two probability distributions. Mathematically, it is the expectation of the log difference between the probability of data in the original distribution $p$ and the approximating distribution $q$.&lt;/p&gt;

&lt;p&gt;$$ \begin{align} KL(p||q) &amp;amp;= \mathbb{E}(\log p(x) - \log q(x)) \\\ &amp;amp;= \sum_{i=1}^N p(x_i)(\log p(x_i) - \log q(x_i)) \end{align}$$&lt;/p&gt;

&lt;p&gt;In information theory, the most important notion is that of &lt;strong&gt;entropy&lt;/strong&gt;, which represents the minimum number of bits required to encode some information, and is mathematically represented as&lt;/p&gt;

&lt;p&gt;$$ H = -\sum_{i=1}^N p(x_i)\log p(x_i). $$&lt;/p&gt;

&lt;p&gt;As such, the KL divergence can be seen to compute how many bits of information will be lost in approximating a distribution $p$ with another distribution $q$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The PAC-Bayes bound is given as&lt;/p&gt;

&lt;p&gt;$$ KL(\hat{e}(Q,S_m)||e(Q)) \leq \frac{KL(Q||P)+\log \frac{m}{\delta}}{m-1}, $$&lt;/p&gt;

&lt;p&gt;where $\hat{e}(Q,S_m)$ is the empirical loss of $Q$ w.r.t some i.i.d sample $S_m$, and $e(Q)$ is the expected loss. If we now find a $Q$ that minimizes this value, we are likely to find a minima that generalizes well and has a nonvacuous bound. This is exactly what is proposed in the paper.&lt;/p&gt;

&lt;p&gt;On a binary variant of MNIST, the computed PAC-Bayes bounds on the test error are in the range 16-22%. While this is a loose bound (actual bounds are around 3%), it is still surprising to find a non-trivial numerical bound for a model with such a large capacity on so few training examples. The authors comment that these are, in all likelihood, &amp;ldquo;the first explicit and nonvacuous numerical bounds computed for trained neural networks in the deep learning regime&amp;rdquo;.&lt;/p&gt;

&lt;h4 id=&#34;2-compressibility-approach&#34;&gt;2. Compressibility approach&lt;/h4&gt;

&lt;p&gt;Although the PAC-Bayes bound is nonvacuous, it is still looser than actual sample complexity bounds computed empirically. Instead, Arora et al. introduce a new &lt;em&gt;compression framework&lt;/em&gt; to address this problem. Earlier while discussing noise stability, we have already seen that if we can compress a classifier $f$ without decreasing the empirical loss, it becomes much more generalizable according to the fundamental theorem proved earlier.&lt;/p&gt;

&lt;p&gt;We say that $f$ is $(\gamma,S)$-compressible using helper string $s$ if there exists some other classifier $g_{A,s}$ on a class of parameters $A$ such that the classification loss of $f$ on every $x \in S$ differs from that of $g_{A,s}$ by at most $\gamma$. Here, $s$ is fixed before looking at the training sample, and is often just for randomization.&lt;/p&gt;

&lt;p&gt;Then, the main theorem in the paper is as follows: If $f$ is $(\gamma,S)$-compressible using helper string $s$, then with high probability,&lt;/p&gt;

&lt;p&gt;$$ L_0 (g_A) \leq \hat{L}_{\gamma}(f) + \mathcal{O}\left( \sqrt{\frac{q \log r}{m}} \right), $$&lt;/p&gt;

&lt;p&gt;where $A$ is a set of $q$ parameters each having at most $r$ discrete values, $L_0 (g_A)$ is the generalization loss of compressed classifier, and $\hat{L}_{\gamma}(f)$ is the empirical estimate of the marginal loss of original classifier. Note that the bound is for the compressed classifier, but the same is also true for earlier works (like the PAC-Bayes approach). The proof is very elementary and uses just simple concentration inequalities.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; First, using Hoeffding&amp;rsquo;s inequality, we can write&lt;/p&gt;

&lt;p&gt;$$ P(L_0 (g_A) - \hat{L}_0 (g_A) \geq \epsilon) \leq 2\exp(-2m\epsilon^2). $$&lt;/p&gt;

&lt;p&gt;Taking $\epsilon = \sqrt{\frac{q \log r}{m}}$, we get, with probability at least $1 - \exp(-2q\log r)$,&lt;/p&gt;

&lt;p&gt;$$ L_0 (g_A) \leq \hat{L}_0 (g_A) + \mathcal{O}\left( \sqrt{\frac{q \log r}{m}} \right). $$&lt;/p&gt;

&lt;p&gt;Next, by definition of $(\gamma,S)$-compressibility, we can write&lt;/p&gt;

&lt;p&gt;$$ \lvert f(x)[y] - g_A(x)[y] \rvert \leq \gamma. $$&lt;/p&gt;

&lt;p&gt;This means that as long as the original function has margin at least $\gamma$, the new function classifies the example correctly. Therefore,&lt;/p&gt;

&lt;p&gt;$$ \hat{L}_0 (g_A) \leq \hat{L}_{\gamma}(f). $$&lt;/p&gt;

&lt;p&gt;Combining this with the earlier inequality, we immediately get the result.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In addition to providing a tighter generalization bound for fully connected networks, the paper also proposes some theory for convolutional nets, which have been notoroiusly difficult to theorize. For details, readers are suggested to refer to the paper.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Theory of Deep Learning: Optimization</title>
      <link>https://abhilashasancheti.github.io/post/deep-learning-theory-1/</link>
      <pubDate>Thu, 26 Jul 2018 11:15:18 +0530</pubDate>
      
      <guid>https://abhilashasancheti.github.io/post/deep-learning-theory-1/</guid>
      <description>

&lt;p&gt;I only just got around to watching the ICML 2018 tutorial on &amp;ldquo;&lt;a href=&#34;http://unsupervised.cs.princeton.edu/deeplearningtutorial.html&#34; target=&#34;_blank&#34;&gt;Toward a Theory for Deep Learning&lt;/a&gt;&amp;rdquo; by &lt;a href=&#34;https://www.cs.princeton.edu/~arora/&#34; target=&#34;_blank&#34;&gt;Prof. Sanjeev Arora&lt;/a&gt;. In this and the next few posts, I will discuss the subject in some detail, including the referenced papers and blogs. Very conveniently, the talk itself was divided into 5 parts, and I will structure this series accordingly.&lt;/p&gt;

&lt;p&gt;At the outset, we should understand that a number of important concepts in deep learning are already shaped by optimization theory. Backpropagation, for instance, is basically just a linear time dynamic programming algorithm to compute gradient. Recent methods for gradient descent, such as momentum, Adagrad, etc. (see &lt;a href=&#34;https://abhilashasancheti.github.io/post/short-note-sgd-algorithms/&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt; for a quick overview) are obtained from convex optimization techniques. However, over the last decade, the deep learning community has come up with several models based on intuition mostly, that do not have any theoretical support yet.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The goal, then, is to find theorems that support these intuitions, leading to new insights and concepts.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this first part of the series, we will try to understand why (and how) deep learning almost always finds decent solutions to problems that are highly nonconvex.&lt;/p&gt;

&lt;h3 id=&#34;possible-goals-for-optimization&#34;&gt;Possible goals for optimization&lt;/h3&gt;

&lt;p&gt;Any neural network essentially tries to minimize a loss function. However, in almost all cases, this loss function is highly nonconvex (and sometimes NP-hard), which means that no provably polytime algorithm exists for its optimization. Even so, deep networks are quite adept at finding an approximately good solution.&lt;/p&gt;

&lt;p&gt;Whenever the gradient $\nabla$ is non-zero, there exists a descent direction. As such, a possible goal for the network may be any of the following (in increasing order of difficulty):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Finding a critical point, i.e. $\nabla = 0$.&lt;/li&gt;
&lt;li&gt;Finding a local optimum, i.e. $\nabla = 0$ and $\nabla^2$ is positive semi-definite.&lt;/li&gt;
&lt;li&gt;Finding a global optimum.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Furthermore, this descent may be from several possible initializations, namely all points, random points, or specially-chosen points. Now, if there are $d$ parameters (weights) to be optimized, we say that the problem is in $\mathbb{R}^d$ space. It is usually visualized by the following sea-urchin figure (or a $d$-urchin figure, according to Prof. Arora).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/21/high-dim-space.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In $\mathbb{R}^d$ space, there exit exp($d$) directions which can be explored to find the optimal solution, which makes the naive approach infeasible. Also, we cannot use non black box approaches to prune the number of explorations, since there is no clean mathematical formulation for the problem.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;But what does this mean?&lt;/em&gt; This means that problems in deep learning are usually of the kind where, given pixels of an image, you have to label it as a cat or a dog. Such an $(x_i,y_i)$ has no mathematical meaning. This means that we do not understand the inherent landscape of the problem we are trying to solve, and so no special pruning can be done.&lt;/p&gt;

&lt;p&gt;This, combined with the nonconvex nature of the loss function, also means that it becomes infeasible to find a global optimum for the optimization problem. As such, we have to settle for goals 1 and 2, i.e. a critical point or a local optimum.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;finding-critical-points&#34;&gt;Finding critical points&lt;/h3&gt;

&lt;p&gt;The update function for a parameter $\theta$ is given as&lt;/p&gt;

&lt;p&gt;$$ \theta_{t+1} = \theta_t - \eta \nabla f(\theta_t) $$&lt;/p&gt;

&lt;p&gt;If the second derivative $\nabla^2$ is high, $\nabla f(\theta_t)$ will vary a lot, and we may miss the actual critical point. To prevent this, it is advisable to take &lt;em&gt;small&lt;/em&gt; steps.&lt;/p&gt;

&lt;p&gt;But how do we quantify small? In other words, &lt;em&gt;how do we determine a good learning rate for the optimization problem&lt;/em&gt;? For this, we again look at $\nabla^2$, which will determine the smoothness of the function. Suppose there exists a $\beta$ such that the Hessian $-\beta I \leq \nabla^2 f(\theta) \leq \beta I$, where $I$ is the identity matrix. Essentially, a higher $\beta$ means that $\nabla^2$ varies more, and so the learning rate should be lower. From this understanding, we can prove the following claim.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Claim (&lt;a href=&#34;https://rd.springer.com/book/10.1007%2F978-1-4419-8853-9&#34; target=&#34;_blank&#34;&gt;Nesterov 1998&lt;/a&gt;):&lt;/strong&gt; If we choose $\eta = \frac{1}{2\beta}$, we can achieve $|\nabla f|&amp;lt;\epsilon$ in number of steps proportional to $\frac{\beta}{\epsilon^2}$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; See the proof of Lemma 2.8 &lt;a href=&#34;https://ee227c.github.io/notes/ee227c-notes.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; (see Definition 2.7). So a single update reduces the function value by at least $\frac{\epsilon^2}{2\beta}$. Therefore, it would take $\mathcal{O}(\frac{\beta}{\epsilon^2})$ steps to arrive at a critical point.&lt;/p&gt;

&lt;h4 id=&#34;evading-saddle-points&#34;&gt;Evading saddle points&lt;/h4&gt;

&lt;p&gt;While we have a theoretical upper limit for the time taken for convergence at a critical point, this is still problematic since it may be a saddle point, i.e., the function value is minimum in $d-1$ directions but maximum in one direction. Such a surface literally looks like a saddle as follows.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/21/saddle-point.png&#34; alt=&#34;Saddle point&#34; /&gt;&lt;/p&gt;

&lt;p&gt;An important question, then, is how to evade saddle points while looking for critical points. This question is explored in a series of papers and corresponding blog posts on &lt;a href=&#34;www.offconvex.org&#34; target=&#34;_blank&#34;&gt;Prof. Arora&amp;rsquo;s blog&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.offconvex.org/2016/03/22/saddlepoints/&#34; target=&#34;_blank&#34;&gt;Polynomial time guarantee for GD to escape saddle points&lt;/a&gt; (based on &lt;a href=&#34;http://proceedings.mlr.press/v40/Ge15.pdf&#34; target=&#34;_blank&#34;&gt;this paper&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.offconvex.org/2016/03/24/saddles-again/&#34; target=&#34;_blank&#34;&gt;Random initialization for asymptotically avoiding saddle points&lt;/a&gt; (based on &lt;a href=&#34;https://arxiv.org/pdf/1602.04915.pdf&#34; target=&#34;_blank&#34;&gt;this paper&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.offconvex.org/2017/07/19/saddle-efficiency/&#34; target=&#34;_blank&#34;&gt;Perturbing gradient descent&lt;/a&gt; (based on &lt;a href=&#34;https://arxiv.org/pdf/1703.00887.pdf&#34; target=&#34;_blank&#34;&gt;this paper&lt;/a&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here I will try to summarize these discussions in several bullet points.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Most learning problems have exponentially many saddle points.&lt;/em&gt; Learning problems usually involve searching for $k$ components, for example clustering, $k$-node hidden layer in a neural network, etc. Suppose $(x_1,x_2,\ldots,x_k)$ is an optimal solution. Then, $(x_2,x_1,\ldots,x_k)$ is also an optimal solution, but the mean of these is not an optimal solution. This suffices to show that the learning problem is nonconvex, since for a convex function, the average of optimal solutions is also optimal. Furthermore, we can keep swapping the $k$ components to obtain exponential optimal solutions. Saddle points lie on the paths joining these isolated solutions, and hence, are exponential in number themselves.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;Hessians can be used to evade saddle points.&lt;/em&gt; Consider the second order Taylor expansion given below. If there exists a direction where $\frac{1}{2}(y-x)^T \nabla^2 f(x)(y-x)$ is significantly less than 0, then using this update rule can avoid saddle points. Such saddle points are called &amp;ldquo;strict,&amp;rdquo; and for these, methods such as trust region algorithms and cubic regularization can find the local optimum.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ f(y) = f(x) + &amp;lt;\nabla f(x), y-x&amp;gt; + \frac{1}{2}(y-x)^T \nabla^2 f(x)(y-x) $$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Noisy gradient descent converges to local optimum in polynomial number of steps.&lt;/em&gt; Although the Hessian method provides a theoretical way to escape saddle points, the computation of $\nabla^2$ is still expensive. Suppose we put a ball on a saddle point. Then, giving it only a slight push will move it away from the saddle. This intuition leads to the notion of &amp;ldquo;noisy&amp;rdquo; GD, i.e., $y = x - \eta \nabla f(x) + \epsilon$, where $\epsilon$ is a zero-mean error, which is often cheaper to compute than the true gradient. The authors in also prove the theorem in &lt;a href=&#34;http://proceedings.mlr.press/v40/Ge15.pdf&#34; target=&#34;_blank&#34;&gt;the paper&lt;/a&gt;, but it is very non-trivial.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;It is hard to converge to a saddle point.&lt;/em&gt; Furthermore, a random initialization of GD will asymptotically converge to a local minimum, rather than other stationary points. In (2), &lt;a href=&#34;http://people.eecs.berkeley.edu/~brecht/&#34; target=&#34;_blank&#34;&gt;Ben Recht&lt;/a&gt; emphasized that &amp;ldquo;even simple algorithms like gradient descent with constant step sizes can’t converge to saddle points unless you try really hard.&amp;rdquo; To prove this, they use the Stable Manifold Theorem, taking $x^{\ast}$ to be an arbitrary saddle point and showing that this measure was &lt;em&gt;always&lt;/em&gt; zero.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;The Stable Manifold theorem is concerned with fixed point operations of the form $x^{(k+1)}=\psi(x^{(k)})$. It quantifies that the set of points that locally converge to a fixed point $x^{\ast}$ of such an iteration have measure zero whenever the Jacobian of $\psi$ at $x^{\ast}$ has eigenvalues bigger than 1.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In fact, it has been &lt;a href=&#34;https://www.math.upenn.edu/~pemantle/papers/nonconvergence.pdf&#34; target=&#34;_blank&#34;&gt;shown long back&lt;/a&gt; that additive Gaussian noise is sufficient to prevent convergence to saddles, without even assuming the &amp;ldquo;strictness&amp;rdquo; criteria of (1).&lt;/p&gt;

&lt;p&gt;Now that it is clear that GD can avoid saddle points almost certainly, it remains to be seen whether it is &lt;em&gt;efficient&lt;/em&gt; in doing so. The paper (1), although it did show a poly-time convergence for the noisy GD, was still inefficient because its polynomial dependency on the dimension $n$ and the smallest eigenvalue of the Hessian are impractical. The paper (3) further improves this aspect of the problem.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;A perturbed form of GD, under an additional Hessian-Lipschitz condition, converges to a second-order stationary point in almost the same time required for GD to converge to a first-order stationary point.&lt;/em&gt; Furthermore, the dimensional dependence is only polynomial in $\log(d)$.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Finally, recent work definitely shows that &lt;em&gt;PGD is much better than GD with random initialization&lt;/em&gt;, since the latter can be slowed down by saddle points, taking exponential time to escape. This is because if there are a sequence of closely-spaced saddle points, GD gets closer to the later ones, and takes $e^i$ iterations to escape the $i^{th}$ saddle point. PGD, on the other hand, escapes each saddle point in a small number of steps regardless of history.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt; Although most learning problems have exponentially many saddle points, they are hard to converge to, and even random initializations can escape them. They take a long time for this escape though, which is why using perturbations is more efficient, and actually as efficient as GD for first-order stationary points. Therefore, using information from Hessians is not necessary to escape saddle points efficiently.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;second-order-methods-for-local-optimum&#34;&gt;Second-order methods for local optimum&lt;/h3&gt;

&lt;p&gt;Although we have established that Hessians are unnecessary for finding the local optimum, it would still be enlightening to look at some approaches for the same.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1602.03943.pdf&#34; target=&#34;_blank&#34;&gt;Agarwal et. al &amp;lsquo;17&lt;/a&gt; proposed LiSSA, or Linear (time) Stochastic Second-order Algorithm. The basic update rule is&lt;/p&gt;

&lt;p&gt;$$ x_{t+1} = x_t - \eta [\nabla^2 f(x)]^{-1}\nabla f(x), $$&lt;/p&gt;

&lt;p&gt;i.e. the gradient is scaled by the inverse of the Hessian, which intuitively makes sense as discussed earlier. Although backpropagation can compute the Hessian itself in linear time, we require the inverse. In this paper, the LiSSA algorithm uses the idea that $(\nabla^2)^{-1} = \sum_{i=1}^{\infty}(I - \nabla^2)^i$, but with finite truncation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1611.00756.pdf&#34; target=&#34;_blank&#34;&gt;Carmon et al. &amp;lsquo;17&lt;/a&gt; further improved upon the $\mathcal{O}(\frac{1}{\epsilon^2})$ guarantee provided by gradient descent for $\epsilon$-first-order convergence, without any need for Hessian computation. They use two competing techniques for this purpose. The first has already been discussed above:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If the problem is locally non-convex, the Hessian must have a negative eigenvalue. In this case, under the assumption that the Hessian is Lipschitz continuous, moving in the direction of the corresponding eigenvector must make progress on the objective.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The second technique is more novel. They show that if the Hessian&amp;rsquo;s smallest eigenvalue is at least $-\gamma$, we can apply &lt;a href=&#34;https://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf&#34; target=&#34;_blank&#34;&gt;proximal point techniques&lt;/a&gt; and accelerated gradient descent to a carefully constructed regularized problem to obtain a faster running time.&lt;/p&gt;

&lt;p&gt;While their approach is asymptotically faster than first-order methods, it is still empirically slower. Furthermore, it doesn&amp;rsquo;t seem to find better quality neural networks in practice.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;understanding-the-landscape-matrix-completion&#34;&gt;Understanding the landscape: Matrix completion&lt;/h3&gt;

&lt;p&gt;Very early on in this post, we established that in deep learning problems, the landscape is unknown, i.e. the problem does not have a meaningful mathematical formulation. In this vein, we now look at a &lt;a href=&#34;https://arxiv.org/pdf/1704.00708.pdf&#34; target=&#34;_blank&#34;&gt;paper&lt;/a&gt; that develops a new framework to capture the landscape. In particular, we will approach this problem in the context of matrix completion. (Interestingly, this paper is again from &lt;a href=&#34;https://users.cs.duke.edu/~rongge/index.html&#34; target=&#34;_blank&#34;&gt;Rong Ge&lt;/a&gt;, who first showed polytime convergence to local minimum for noisy GD.)&lt;/p&gt;

&lt;p&gt;But first, what is matrix completion. Matrix completion is a learning problem wherein the objective is to recover a low-rank matrix from partially observed entries. The mathematical formulation of the problem is:&lt;/p&gt;

&lt;p&gt;$$ \min_{X} \text(rank)(X) \quad \text{subject to} \quad X_{ij} = M_{ij} ~~ \forall i,j \in E $$&lt;/p&gt;

&lt;p&gt;where $E$ is the set of observed entries. Most approaches to solve this problem represent it in the form of the following nonconvex objective.&lt;/p&gt;

&lt;p&gt;$$ f(X) = \frac{1}{2}\sum_{i,j\in E}[M_{i,j}-(XX^T)_{i,j}]^2 +R(X) $$&lt;/p&gt;

&lt;p&gt;Here, $R(X)$ is a regularization term which ensures that no single row of $X$ becomes too large, otherwise most observed entries will be 0.&lt;/p&gt;

&lt;p&gt;Ge showed in &lt;a href=&#34;https://arxiv.org/pdf/1605.07272.pdf&#34; target=&#34;_blank&#34;&gt;an earlier paper&lt;/a&gt; that in case of matrix completion (others have shown the same result for other problems like tensor decomposition and dictionary learning), all local minima are also global minima.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For matrix completion, all local minima are also global minima.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the present paper, the authors proposed the new insight that for the case of the matrix completion objective as defined above, the function $f$ is quadratic in $X$, which means that its Hessian w.r.t $X$ is constant. Furthermore, any saddle point has at least one strictly negative eigenvalue in its Hessian. Together, these ensure that simple local search algorithms can find the desired low rank matrix from an arbitrary starting point in polynomial time with high probability.&lt;/p&gt;

&lt;p&gt;These advances, while mathematically involved, show that characterizing the various stationary points of the learning objective can be helpful in providing theoretical guarantees for learning algorithms. While I have avoided proof details for the several important theorems here, I will try to understand and explain them lucidly in some later post.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Waldo: A system for optical character recognition</title>
      <link>https://abhilashasancheti.github.io/project/waldo-ocr/</link>
      <pubDate>Tue, 03 Jul 2018 16:56:46 +0530</pubDate>
      
      <guid>https://abhilashasancheti.github.io/project/waldo-ocr/</guid>
      <description>&lt;p&gt;It is an ongoing project under Prof. Daniel Povey to develop an Optical Character Recognition system that is robust on focused as well as incidental text. My contributions are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Experimenting with the ICDAR 2015 Robust Reading Challenge dataset by modifying training script.&lt;/li&gt;
&lt;li&gt;A visualization and compression module for segmentation mask overlayed on images.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The system consists of a modified UNet first proposed in &lt;a href=&#34;https://arxiv.org/abs/1505.04597&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; paper.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Transfer Learning in NLP</title>
      <link>https://abhilashasancheti.github.io/post/transfer-learning-nlp/</link>
      <pubDate>Fri, 15 Jun 2018 13:42:18 +0530</pubDate>
      
      <guid>https://abhilashasancheti.github.io/post/transfer-learning-nlp/</guid>
      <description>

&lt;p&gt;Transfer learning is undoubtedly the new (well, relatively anyway) hot thing in deep learning right now. In vision, it has been in practice for some time now, with people using models trained to learn features from the huge ImageNet dataset, and then training it further on smaller data for different tasks. In NLP, though, transfer learning was mostly limited to the use of pretrained word embeddings (which, to be fair, improved baselines significantly). Recently, researchers are moving towards transferring entire models from one task to another, and that is the subject of this post.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://ruder.io/&#34; target=&#34;_blank&#34;&gt;Sebastian Ruder&lt;/a&gt; (whose biweekly newsletter inspires a lot of my deep learning reading) and &lt;a href=&#34;https://www.fast.ai/about/#jeremy&#34; target=&#34;_blank&#34;&gt;Jeremy Howard&lt;/a&gt; were perhaps the first to make transfer learning in NLP exciting through their &lt;a href=&#34;http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html&#34; target=&#34;_blank&#34;&gt;ULMFiT method&lt;/a&gt; which surpassed all text classification state-of-the-art. This Monday, &lt;a href=&#34;https://openai.com/&#34; target=&#34;_blank&#34;&gt;OpenAI&lt;/a&gt; &lt;a href=&#34;https://blog.openai.com/language-unsupervised/&#34; target=&#34;_blank&#34;&gt;extended their idea&lt;/a&gt; and outperformed SOTAs on several NLP tasks. At NAACL 2018, the Best Paper award was given to the paper introducing &lt;a href=&#34;https://allennlp.org/elmo&#34; target=&#34;_blank&#34;&gt;ELMo&lt;/a&gt;, a new word embedding technique very similar to the idea behind ULMFiT, from researchers at &lt;a href=&#34;https://allenai.org/&#34; target=&#34;_blank&#34;&gt;AllenAI&lt;/a&gt; and &lt;a href=&#34;https://www.cs.washington.edu/people/faculty/lsz/&#34; target=&#34;_blank&#34;&gt;Luke Zettlemoyer&lt;/a&gt;’s group at UWash (Seattle).&lt;/p&gt;

&lt;p&gt;In this article, I will discuss all of these new work and how they are interrelated. Let’s start with Ruder and Howard’s trend-setting architecture.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;universal-language-model-fine-tuning-for-text-classification-https-arxiv-org-pdf-1801-06146-pdf&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/1801.06146.pdf&#34; target=&#34;_blank&#34;&gt;Universal Language Model Fine-Tuning for Text Classification&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Most datasets for text classification (or any other supervised NLP tasks) are rather small. This makes it very difficult to train deep neural networks, as they would tend to overfit on these small training data and not generalize well in practice.&lt;/p&gt;

&lt;p&gt;In computer vision, for a couple of years now, the trend is to pre-train any model on the huge ImageNet corpus. This is much better than a random initialization because the model learns general image features and that learning can then be used in any vision task (say captioning, or detection).&lt;/p&gt;

&lt;p&gt;Taking inspiration from this idea, Howard and Ruder propose a bi-LSTM model that is trained on a general language modeling (LM) task and then fine tuned on text classification. This would, in principle, perform well because the model would be able to use its knowledge of the semantics of language acquired from the generative pre-training. Ideally, this transfer can be done from any source task $S$ to a target task $T$. The authors use LM as the source task because:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;it is able to capture long-term dependencies in language&lt;/li&gt;
&lt;li&gt;it effectively incorporates hierarchical relations&lt;/li&gt;
&lt;li&gt;it can help the model learn sentiments&lt;/li&gt;
&lt;li&gt;large data corpus is easily available for LM&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Formally, &amp;ldquo;LM introduces a hypothesis space $H$ that should be useful for many other NLP tasks.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;For the architecture, they use the then SOTA &lt;a href=&#34;https://arxiv.org/pdf/1708.02182.pdf&#34; target=&#34;_blank&#34;&gt;AWD-LSTM&lt;/a&gt; (which is, I suppose, a multi-layer bi-LSTM network without attention, but I would urge you to read the details in the paper from Salesforce Research). The model was trained on the WikiText-103 corpus.&lt;/p&gt;

&lt;p&gt;Once the generic LM is trained, it can be used as is for multiple classification tasks, with some fine-tuning. For this fine tuning and subsequent classification, the authors propose 3 implementation tricks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discriminative fine tuning:&lt;/strong&gt; Different learning rates are used for different layers during the fine-tuning phase of LM (on the target task). This is done because the layers capture different types of information.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Slanted triangular learning rates (STLR):&lt;/strong&gt; Learning rates are first increased linearly, and then decreased gradually after a cut, i.e., there is a &amp;ldquo;short increase&amp;rdquo; and a &amp;ldquo;long decay&amp;rdquo;. This is similar to the aggressive cosine annealing learning strategy that is popular now.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/20/stlr.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Gradual unfreezing:&lt;/strong&gt; During the classification training, the LM model is gradually unfreezed starting from the last layer. If all the layers are trained from the beginning, the learning from the LM would be forgotten quickly, and so gradual unfreezing is important to make use of the transfer learning.&lt;/p&gt;

&lt;p&gt;On the 6 text classification tasks that they evaluated, there was a relative improvement of 18–24% on the majority of tasks. Further, the following was observed:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Only 100 labeled samples in classification were sufficient to match the performance of a model trained on 50–100x samples from scratch.&lt;/li&gt;
&lt;li&gt;Pretraining is more useful on small and medium sized data.&lt;/li&gt;
&lt;li&gt;LM quality affects final classification performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The analysis in the paper is very thorough, and I would recommend going through it for details, and also to learn how to design experiments for strong empirical results. They suggest some possible future directions as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The LM pretraining and fine-tuning can be improved.&lt;/li&gt;
&lt;li&gt;The LM can be augmented with other tasks in a multi-task learning setting.&lt;/li&gt;
&lt;li&gt;The pretrained model can be evaluated on tasks other than classification.&lt;/li&gt;
&lt;li&gt;Further analysis can be done to determine what information is captured during pretraining and changed during fine-tuning.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;1 and 3 should be noted, in particular, as that makes up the novelty in OpenAI’s new paper discussed below.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;improving-language-modeling-by-generative-pre-training-https-s3-us-west-2-amazonaws-com-openai-assets-research-covers-language-unsupervised-language-understanding-paper-pdf&#34;&gt;&lt;a href=&#34;https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf&#34; target=&#34;_blank&#34;&gt;Improving Language Modeling by Generative Pre-training&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;This paper was published on ArXiv this Monday (11 June), and my Twitter feed has been inundated with talk about it since then. Jeremy Howard himself tweeted favorably about it, saying that this was exactly the kind of work he was hoping for in his &amp;ldquo;future directions&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;This is exactly where we were hoping our ULMFit work would head - really great work from &lt;a href=&#34;https://twitter.com/OpenAI?ref_src=twsrc%5Etfw&#34;&gt;@OpenAI&lt;/a&gt;! 😊&lt;br&gt;&lt;br&gt;If you&amp;#39;re doing NLP and haven&amp;#39;t tried language model transfer learning yet, then jump in now, because it&amp;#39;s a Really Big Deal. &lt;a href=&#34;https://t.co/0Dj8ChCxvu&#34;&gt;https://t.co/0Dj8ChCxvu&lt;/a&gt;&lt;/p&gt;&amp;mdash; Jeremy Howard (@jeremyphoward) &lt;a href=&#34;https://twitter.com/jeremyphoward/status/1006262925986652161?ref_src=twsrc%5Etfw&#34;&gt;June 11, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;What Alec Radford (the first author) does here is&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;use a Transformer network (explained below in detail) instead of the AWD-LSTM; and&lt;/li&gt;
&lt;li&gt;evaluate the LM on a variety of NLP tasks, ranging from textual entailment to question-answering.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you are already aware of the ULMFiT architecture, you only need to know 2 things to understand this paper: (a) how the Transformer works, and (b) how an LM-trained model can be used to evaluate the different NLP tasks.&lt;/p&gt;

&lt;h4 id=&#34;the-transformer&#34;&gt;The Transformer&lt;/h4&gt;

&lt;p&gt;&lt;a href=&#34;https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.WyH7hIrhXb0&#34; target=&#34;_blank&#34;&gt;This blog&lt;/a&gt; provides an extensive description of the model, originally proposed in &lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34; target=&#34;_blank&#34;&gt;this highly popular paper&lt;/a&gt; from last year. Here I will go over the salient features. For details, you can go through the linked blog post or the paper itself.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/20/transformer.png&#34; alt=&#34;Single layer of Encoder (left) and Decoder (right) that is build out of *N*=6 identical layers.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The problem with RNN-based seq2seq models is that since they are sequential models, they cannot be parallelized. One possible solution that was proposed to remedy this involved the use of fully convolutional networks with positional embeddings, but it required O(nlogn) time to relate 2 words at some distance in the sentence. The Transformer solves this problem by completely doing away with convolutions or recurrence, and relying entirely upon self-attention.&lt;/p&gt;

&lt;p&gt;In a simple &lt;em&gt;scalar dot-product attention&lt;/em&gt;, weight is computed by taking the dot product of the query (Q) and key (K). The weighted sum of all values V is then the required output. In contrast, in a &lt;em&gt;multihead attention&lt;/em&gt;, the input vector itself is divided into chunks and then the scalar dot-product attention is applied on each chunk in parallel. Finally, we compute the average of all the chunk outputs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/20/multiatt.png&#34; alt=&#34;Multi-head attention architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The final step consists of a position-wise FFN, which itself is a combination of 2 linear transformations and a ReLU for each position. The following GIF explains this process very effectively.&lt;/p&gt;

&lt;p&gt;&lt;blockquote class=&#34;imgur-embed-pub&#34; lang=&#34;en&#34; data-id=&#34;a/GUwAEe9&#34;&gt;&lt;a href=&#34;//imgur.com/GUwAEe9&#34;&gt;Transformer&lt;/a&gt;&lt;/blockquote&gt;&lt;script async src=&#34;//s.imgur.com/min/embed.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&#34;task-specific-input-transformations&#34;&gt;Task-specific input transformations&lt;/h4&gt;

&lt;p&gt;The second novelty in the OpenAI paper is how they use the pretrained LM model on several NLP tasks.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Textual entailment&lt;/em&gt;: The text (t) and the hypothesis (h) are cocatenated with a $ in between. This makes it naturally suitable for evaluation on an LM model.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Text similarity&lt;/em&gt;: Since the order is not important here, the texts are concatenated in both orders and then processed independently and added element-wise.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Question-answering and commonsense reasoning&lt;/em&gt;: The text, query, and answer option are concatenated with some differentiation symbol in between and each such sample is processed. They are then normalized via softmax to produce output distribution over possible answers.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The authors trained the Transformer LM on the Book Corpus dataset, and improved SOTA on 9 of the 12 tasks. While the results are indeed amazing, the analysis is not as extensive as that performed by Howard and Ruder, probably because the training required a month on 8 GPUs. This was even pointed out by &lt;a href=&#34;https://www.cs.bgu.ac.il/~yoavg/uni/&#34; target=&#34;_blank&#34;&gt;Yoav Goldberg&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Strong empirical results, but I wish there was more focus on a proper comparison / controlled experiments. &lt;br&gt;&lt;br&gt;Is the improvement due to LSTM -&amp;gt; Transformer or due to Wiki-1B (single sents) -&amp;gt; BooksCorpus (longer context)? &lt;a href=&#34;https://t.co/L3WrJW3z12&#34;&gt;https://t.co/L3WrJW3z12&lt;/a&gt;&lt;/p&gt;&amp;mdash; (((λ()(λ() &amp;#39;yoav)))) (@yoavgo) &lt;a href=&#34;https://twitter.com/yoavgo/status/1006410113547108354?ref_src=twsrc%5Etfw&#34;&gt;June 12, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;deep-contextualized-word-representations-https-arxiv-org-pdf-1802-05365-pdf&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/1802.05365.pdf&#34; target=&#34;_blank&#34;&gt;Deep Contextualized Word Representations&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;The motivation for this paper, which won the Best Paper award at NAACL’18, is that word embeddings should incorporate both word-level characteristics as well as contextual semantics.&lt;/p&gt;

&lt;p&gt;The solution is very simple: instead of taking just the final layer of a deep bi-LSTM language model as the word representation, &lt;strong&gt;obtain the vectors of each of the internal functional states of every layer, and combine them in a weighted fashion&lt;/strong&gt; to get the final embeddings.&lt;/p&gt;

&lt;p&gt;The intuition is that the higher level states of the bi-LSTM capture context, while the lower level captures syntax well. This is also shown empirically by comparing the performance of 1st layer and 2nd layer embeddings. While the 1st layer performs better on POS tagging, the 2nd layer achieves better accuracy for a word-sense disambiguation task.&lt;/p&gt;

&lt;p&gt;For the initial representation, the authors chose to initialize with the embeddings obtained from a character CNN, so as to have character level morphological information incorporated in the embeddings. Finally for an $L$-layer bi-LSTM, $2L+1$ such vectors need to be combined to get the final representation, after performing some layer normalization.&lt;/p&gt;

&lt;p&gt;In the empirical evalutation, the use of ELMo resulted in up to 25% relative increase in performance across several NLP tasks. Moreover, it improves sample efficiency considerably.&lt;/p&gt;

&lt;p&gt;(Interestingly, a Google search revealed that this paper was first submitted at ICLR’18 but later withdrawn. I wonder why.)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;As Jeremy Howard says, transfer learning is indeed the Next Big Thing in NLP, and these trend-setting papers demonstrate why. I am sure we will see a lot of development in this area in the days to come.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Evaluation Methods for Dialog Systems</title>
      <link>https://abhilashasancheti.github.io/post/evaluation-methods-dialog-systems/</link>
      <pubDate>Wed, 06 Jun 2018 13:42:04 +0530</pubDate>
      
      <guid>https://abhilashasancheti.github.io/post/evaluation-methods-dialog-systems/</guid>
      <description>

&lt;p&gt;Spoken Dialog Systems (SDS) have become very popular recently, especially for goal completion tasks on mobile devices. Also, with the increasing use of IoT devices and their associated assistants like Alexa, Google Home, etc., systems that can converse with users in natural language are set to be the primary mode of human-computer interaction in the coming years.&lt;/p&gt;

&lt;p&gt;Early SDSs used to be extremely modular, with components such as automatic speech recognition, natural language understanding, dialog management, response generation, and speech synthesis, each trained separately and then combined. For the last few years, especially after neural models gained popularity in language models and machine translation, researchers are slowly but surely moving towards more end-to-end approaches. Very recently, most of the dialog systems proposed in conferences all seem to be built around reinforcement learning frameworks.&lt;/p&gt;

&lt;p&gt;Although they are critical in industry, SDSs have traditionally been notoriously difficult to evaluate. This is because any evaluation strategy associated with them needs to have at least the following features.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It should provide an estimate of how well the goal is met.&lt;/li&gt;
&lt;li&gt;It should allow for comparative judgement of different systems.&lt;/li&gt;
&lt;li&gt;It should (ideally) identify factors that can be improved.&lt;/li&gt;
&lt;li&gt;It should discover trade-offs or correlations between factors.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For a long time, the most prominent evaluation scheme for SDSs was &lt;strong&gt;PARADISE&lt;/strong&gt; (&lt;a href=&#34;http://www.aclweb.org/anthology/P97-1035&#34; target=&#34;_blank&#34;&gt;Walker’97&lt;/a&gt;), developed at AT&amp;amp;T labs.&lt;/p&gt;

&lt;h3 id=&#34;paradise-paradigm-for-dialog-system-evalutation&#34;&gt;PARADISE: PARAdigm for DIalog System Evalutation&lt;/h3&gt;

&lt;p&gt;The Paradise scheme comprises of 2 objectives:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;What does the agent accomplish?&lt;/strong&gt; i.e., task completion&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;How is it accomplished?&lt;/strong&gt; i.e., agent behavior&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/19/paradise.png&#34; alt=&#34;Paradise’s structure of objectives&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For quantifying these objectives, the task is represented in the form of an &lt;strong&gt;Attribute Value Matrix (AVM)&lt;/strong&gt;, which consists of the information that must be exchanged between the agent and the user during the dialogue, represented as a set of ordered pairs of attributes and their possible values. Essentially, this is a confusion matrix between attributes in the actual dialogue and attributes in the expected dialogue.&lt;/p&gt;

&lt;p&gt;Once the AVM is available, the task completion success is computed by a metric $\kappa$ defined as&lt;/p&gt;

&lt;p&gt;$$ \kappa = \frac{P(A)-P(E)}{1-P(E)} $$&lt;/p&gt;

&lt;p&gt;where $P(A)$ is the proportion of times that the actual dialogue agrees with the scenario keys, and $P(E)$ is the expected proportion of times for the same. If $M$ is the matrix, then&lt;/p&gt;

&lt;p&gt;$$ P(E) = \sum_{i=1}^n \left( \frac{t_i}{T} \right)^2 $$&lt;/p&gt;

&lt;p&gt;where $t_i$ is the sum of frequencies in column $i$, and $T$ is the sum of frequencies in $M$. $P(A)$ is given as&lt;/p&gt;

&lt;p&gt;$$ P(A) = \sum_{i=1}^n \frac{M(i,i)}{T} $$&lt;/p&gt;

&lt;p&gt;Since $\kappa$ includes $P(E)$, it inherently includes the task complexity as well, thereby making it a better metric for task completion than, say, transaction success, concept accuracy, or percent agreement.&lt;/p&gt;

&lt;p&gt;For measuring the second objective, i.e., agent behavior, all the AVM attributes are tagged with respective costs. Some examples of cost attributes are: number of user initiatives, mean word per turn, mean response time, number of missing/inappropriate responses, number of errors, etc. Thereafter, the final performance is defined as&lt;/p&gt;

&lt;p&gt;$$ P = (\alpha \mathcal{N}(\kappa)) - \sum_{i=1}^n (w_i \mathcal{N}(c_i)) $$&lt;/p&gt;

&lt;p&gt;Here, $\mathcal{N}$ is some Z-score normalization factor, such as simple normalization based on mean and standard deviation.&lt;/p&gt;

&lt;p&gt;Although PARADISE was an important evaluation scheme for evaluating older statistical SDS models, I haven’t seen it used in any of the recent papers on the subject. A major factor for this is probably the choice of the regression coefficients (costs and coefficient for Kappa), which would greatly affect the performance statistic.&lt;/p&gt;

&lt;h3 id=&#34;schemes-in-recent-papers&#34;&gt;Schemes in Recent Papers&lt;/h3&gt;

&lt;p&gt;For the last couple years, most papers on SDSs propose end-to-end neural architectures. As such, they prefer an evaluation scheme based on a corpus of dialogues divided into training, validation, and development sets.&lt;/p&gt;

&lt;h4 id=&#34;data-collection&#34;&gt;Data collection&lt;/h4&gt;

&lt;p&gt;&lt;a href=&#34;https://www.mturk.com/&#34; target=&#34;_blank&#34;&gt;Amazon Mechanical Turk (MT)&lt;/a&gt;, which is a crowdsourcing website, is the most popular method for collecting data. &lt;a href=&#34;https://arxiv.org/pdf/1710.11277.pdf&#34; target=&#34;_blank&#34;&gt;Peng’17&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/1703.01008.pdf&#34; target=&#34;_blank&#34;&gt;Li’17&lt;/a&gt;, and &lt;a href=&#34;https://arxiv.org/pdf/1604.04562.pdf&#34; target=&#34;_blank&#34;&gt;Wen’16&lt;/a&gt; all use Amazon MT to source their training dialogue sets. Furthermore, the protocol used for this is usually the Wizard-of-Oz scheme.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Wizard-of-Oz protocol:&lt;/strong&gt; In this scheme, a user converses with an agent, which he believes is autonomous. However, there is actually a human in the loop (called a &amp;ldquo;wizard&amp;rdquo;) which controls some key features of the agent which require tuning. The protocol is implemented as follows.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A metric (e.g., task completion rate) is selected to serve as the objective function.&lt;/li&gt;
&lt;li&gt;Some particular features, called the “repair strategy,” are varied to best match the desired performance claim for the metric.&lt;/li&gt;
&lt;li&gt;All other input, output are kept constant through the interface.&lt;/li&gt;
&lt;li&gt;The process is repeated using different wizards.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In &lt;a href=&#34;https://arxiv.org/pdf/1604.04562.pdf&#34; target=&#34;_blank&#34;&gt;Wen’16,&lt;/a&gt; the authors further expediated this protocol by parallelizing it on Amazon MT, such that there are multiple users and wizards working simultaneously on single-turn dialogues.&lt;/p&gt;

&lt;p&gt;Task completion usually involved slot filling as an intermediate objective. As such, several &amp;ldquo;informable slots&amp;rdquo; (e.g., food, price range, area, for a restaurant search system) and &amp;ldquo;requestable slots&amp;rdquo;(e.g., address, phone, postal code) are identified. Users are provided with the keys for the informable slots and wizards are provided with keys for the requestable slots.&lt;/p&gt;

&lt;p&gt;In some cases, like &lt;a href=&#34;https://arxiv.org/pdf/1805.11762.pdf&#34; target=&#34;_blank&#34;&gt;Liu’18&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/1710.11277.pdf&#34; target=&#34;_blank&#34;&gt;Peng’17,&lt;/a&gt; user simulators are also used to create such a corpus of dialogues. An implementation of such a system can be found &lt;a href=&#34;https://github.com/MiuLab/TC-Bot&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. For &lt;a href=&#34;https://arxiv.org/pdf/1805.11762.pdf&#34; target=&#34;_blank&#34;&gt;Liu’18,&lt;/a&gt; the authors further used a public dataset (&lt;a href=&#34;https://github.com/perezjln/dstc6-goal-oriented-end-to-end&#34; target=&#34;_blank&#34;&gt;DSTC&lt;/a&gt;) for corpus-based training.&lt;/p&gt;

&lt;h4 id=&#34;automatic-evaluation&#34;&gt;Automatic evaluation&lt;/h4&gt;

&lt;p&gt;Automatic evaluation metrices may be unsupervised or supervised in nature. &lt;a href=&#34;https://arxiv.org/pdf/1603.08023.pdf&#34; target=&#34;_blank&#34;&gt;Liu’17&lt;/a&gt; summarizes several unsupervised schemes for evaluation, and I list them here.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Word overlap based metrics&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;These include BLEU, ROUGE, and METEOR, and are inspired from machine translation tasks. I have previously described all of these metrics in &lt;a href=&#34;https://medium.com/explorations-in-language-and-learning/metrics-for-nlg-evaluation-c89b6a781054&#34; target=&#34;_blank&#34;&gt;another blog post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Embedding based metrics&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;These are based on the word embeddings (skip-gram, GloVe, etc.) of the actual response and the expected response. Some of them are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Greedy matching&lt;/li&gt;
&lt;li&gt;Embedding average&lt;/li&gt;
&lt;li&gt;Vector extrema&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Please refer to the linked paper for details of these metrics. They are conceptually very simple so I won’t describe them here.&lt;/p&gt;

&lt;p&gt;A problem with both of these techniques is that they may only be suitable for task completion dialog systems, where there are only a few expected responses. Any open-world dialog will necessarily beat the metric, e.g.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;User: Do you want to watch a movie today?&lt;/p&gt;

&lt;p&gt;Gold-standard: Yeah, let’s catch the new Bond film.&lt;/p&gt;

&lt;p&gt;Actual: No, I am busy with something.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For this reason, several other metrics are employed, such as task success rate, average reward (+1 for each slot filled correctly), average number of turns, entity matching rate, prediction accuracy, etc.&lt;/p&gt;

&lt;h4 id=&#34;human-evaluation&#34;&gt;Human evaluation&lt;/h4&gt;

&lt;p&gt;Most importantly though, all researchers agree that human evaluation can never be replaced by automatic evaluation metrics. Usually, several human users are asked to test a trained system with goal-oriented dialogues, and at the end of the dialogue, they are asked to rate it on several criteria such as:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Task completion success&lt;/li&gt;
&lt;li&gt;Comprehension&lt;/li&gt;
&lt;li&gt;Naturalness&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To avoid extreme scores (due to bias etc.), inter-user agreement is calculated using the Kappa value, and only those users with $\kappa &amp;gt; 0.2$ are retained in the final measure. Their scores are then matched against scores from the automatic evaluations by computing correlation coefficients such as Spearman’s or Pearson’s (like in &lt;a href=&#34;https://arxiv.org/pdf/1603.08023.pdf&#34; target=&#34;_blank&#34;&gt;Liu’16&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&#34;key-takeaways&#34;&gt;Key Takeaways&lt;/h3&gt;

&lt;p&gt;From this entire literature survey, I have extracted the following key points to note if you are working to build a task-oriented dialog system and want to evaluate it:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Choose one specific domain, e.g., restaurant search.&lt;/li&gt;
&lt;li&gt;Use either &lt;a href=&#34;https://github.com/perezjln/dstc6-goal-oriented-end-to-end&#34; target=&#34;_blank&#34;&gt;DSTC&lt;/a&gt; (or an equivalent large corpus of dialogues), or use Amazon MT to create one for your task.&lt;/li&gt;
&lt;li&gt;Train your model on the dataset created above.&lt;/li&gt;
&lt;li&gt;Use a word overlap based and a few task completion based metrics for automatic evaluation statistics. Compare with at least a few popular neural baselines. RL frameworks with LSTMs are in vogue these days, I suppose.&lt;/li&gt;
&lt;li&gt;Definitely do human evaluation for your method.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>The 8 Commandments for Coding Your Research</title>
      <link>https://abhilashasancheti.github.io/post/8-commandments-for-coding-research/</link>
      <pubDate>Wed, 23 May 2018 13:41:50 +0530</pubDate>
      
      <guid>https://abhilashasancheti.github.io/post/8-commandments-for-coding-research/</guid>
      <description>

&lt;p&gt;This article is in a different flavor from the other posts in this publication. This is because I have been reading Roman Vershynin’s &amp;ldquo;High Dimensional Probability&amp;rdquo; for the last few days, and between that and visa formalities, I didn’t get a chance to check out new papers. I do plan to write an article on new methods for object detection (such as RCNN, Faster RCNN, and YOLO) sometime next month.&lt;/p&gt;

&lt;p&gt;I have only been a researcher for a couple of years now but during this period I have gained valuable insights on how to structure a research project. When I started out with research back in 2016, I was too eager to obtain results, a mistake that most beginner researchers make. In my eagerness, I used to cut corners with my code structure and take several liberties, especially because there was no review process. I made several mistakes during the project (on relation classification: &lt;a href=&#34;https://github.com/abhilashasancheti/crnn-relation-classification&#34; target=&#34;_blank&#34;&gt;link to Github repo&lt;/a&gt;), some of which I list here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lack of planning:&lt;/strong&gt; I did not have a research plan to begin with, which showed in my code. It is true that in applied machine learning, much of the progress is dictated by experimental results, but a broad outline still helps. At the beginning, I just read papers, cloned their repositories, and ran them on my GPU. Sometimes this used to eat up a lot of precious time since some repositories had several dependencies.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Haphazard code format:&lt;/strong&gt; This stemmed from the first issue. Since I had not planned in advance, I would work with every dataset differently, depending upon how it was available. Some of the code would be in IPython notebooks, while some would be Shell scripts. I would use plain Tensorflow for some training and a Keras wrapper for others.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Not caring about reproducibility:&lt;/strong&gt; This is perhaps the biggest crime an ML researcher can commit. Although my code is legitimate and publicly available, I highly doubt that anyone could reproduce it (not easily, in any case). This is because at that time, all I cared about was getting a publication (which I did, in the end). I did not have a good README file, nor instructions on how to reproduce the results.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Based on these and several other mistakes, I have come up with some guidelines on how to write good code for a research project. I ascribe much of my learning to working on &lt;a href=&#34;https://github.com/waldo-seg/waldo&#34; target=&#34;_blank&#34;&gt;this project&lt;/a&gt; (which is only in its beginning phase) for the last few months. Here are the 8 commandments, along with examples in Python.&lt;/p&gt;

&lt;h4 id=&#34;1-define-and-validate-data-types-at-the-outset&#34;&gt;1. Define and validate data types at the outset&lt;/h4&gt;

&lt;p&gt;Define data structures which will hold your input and output data. Since Python allows using data structures without declaring them implicitly, this can be done by having validation functions which are invoked whenever the data structure is used. This would ensure that the structure is consistent throughout the project. An example of a data structure validation for an “object” type (which is a dict with just one key) is below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def validate_object(x):
    &amp;quot;&amp;quot;&amp;quot;This function validates an object x that is supposed to represent an object inside an image, and throws an exception on failure. Specifically it is checking that:
      x[&#39;polygon&#39;] is a list of &amp;gt;= 3 integer (x,y) pairs representing the corners of the polygon in clockwise or anticlockwise order.
    &amp;quot;&amp;quot;&amp;quot;
    if type(x) != dict:
        raise ValueError(&#39;dict type input required.&#39;)

    if &#39;polygon&#39; not in x:
        raise ValueError(&#39;polygon object required.&#39;)

    if not isinstance(x[&#39;polygon&#39;], (list,)):
        raise ValueError(&#39;list type polygon object required.&#39;)

    points_list = x[&#39;polygon&#39;]
    if len(points_list) &amp;lt; 3:
        raise ValueError(&#39;More than two points required.&#39;)

    for x, y in points_list:
        if type(x) != int or type(y) != int:
            raise ValueError(&#39;integer (x,y) pairs required.&#39;)

    return
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;2-write-data-loader-scripts-for-all-your-datasets&#34;&gt;2. Write data loader scripts for all your datasets&lt;/h4&gt;

&lt;p&gt;Now that wehave common data structures to use with our model(s), we need to convert all our datasets to that format. There are 2 ways to achieve this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Preprocess the dataset to the required structure and save in a serialized file (e.g. Pickle in Python).&lt;/li&gt;
&lt;li&gt;Have a data loader class to read the dataset from source at the time of running and return in the desired format.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;When should you use the second method?&lt;/em&gt; When the dataset itself is large, or we need to have several additional elements in the structure, such as mask data (for an image), or associate word vectors (for text data).&lt;/p&gt;

&lt;p&gt;Additionally, if you have a decent processor and parallelizable script, the compute time in method 2 should be low enough such that the total runtime in 1 becomes larger due to greater I/O time.&lt;/p&gt;

&lt;h4 id=&#34;3-put-common-methods-in-a-shared-library&#34;&gt;3. Put common methods in a shared library&lt;/h4&gt;

&lt;p&gt;Since all the datasets are in a common structure, several transformation methods may be applicable to many of them. So it would make sense to have these methods in a global shared library and link to this library inside each of the local dataset directories. This achieves 2 things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Reduces clutter and reduplication in the directory.&lt;/li&gt;
&lt;li&gt;Allows for ease in making modifications to the shared functions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;4-write-unit-tests-for-utility-functions&#34;&gt;4. Write unit tests for utility functions&lt;/h4&gt;

&lt;p&gt;Instead of writing a test file and modifying it for testing the utility functions, it would be better to use the &lt;strong&gt;&lt;a href=&#34;https://docs.python.org/3/library/unittest.html&#34; target=&#34;_blank&#34;&gt;unittest&lt;/a&gt;&lt;/strong&gt; package in Python, or analogous packages in other languages. For example, in an object detection project, there may be utilities to visualize the object with a mask, or to compress the image. The unit test file may then look like this.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import unittest


class ImageUtilsTest(unittest.TestCase):
    &amp;quot;&amp;quot;&amp;quot;Testing image utilities: visualization and compression
    &amp;quot;&amp;quot;&amp;quot;
    def setUp(self):
        &amp;quot;&amp;quot;&amp;quot;This method sets up objects for all the test cases.
        &amp;quot;&amp;quot;&amp;quot;
        &amp;lt;code for loading data&amp;gt;


    def test_visualize_object(self):
        &amp;quot;&amp;quot;&amp;quot;Given a dictionary object as follows
        x[&#39;img&#39;]: numpy array of shape (height,width,colors)
        x[&#39;mask&#39;]: numpy array of shape (height,width), with every element categorizing it into one of the object ids
        The method generates an image overlaying a translucent mask on the image.
        &amp;quot;&amp;quot;&amp;quot;
        visualize_mask(self.test_object)


    def test_compress_object(self):
        &amp;quot;&amp;quot;&amp;quot;Given a dictionary object x, the method compresses the object and prints the original and compressed sizes.
        It also asserts that the original size should be greater than the compressed size.
        &amp;quot;&amp;quot;&amp;quot;
        y = compress_image_with_mask(self.test_object,self.c)
        x_mem = sys.getsizeof(self.test_object)
        y_mem = sys.getsizeof(y)
        self.assertTrue(y_mem &amp;lt;= x_mem)


if __name__ == &#39;__main__&#39;:
    unittest.main()
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;5-prepare-installation-and-run-scripts&#34;&gt;5. Prepare installation and run scripts&lt;/h4&gt;

&lt;p&gt;This is key for reproducibility. It is very arrogant to assume that readers would clone your repository, install several dependencies one by one, then download the datasets, preprocess the data using some script in your repo, and only then be able to start training. All of these steps can and should be automated using simple Bash shell scripts, so that the users can just run an &lt;strong&gt;install.sh&lt;/strong&gt; or a &lt;strong&gt;run.sh&lt;/strong&gt; file with certain parameters to get things done.&lt;/p&gt;

&lt;p&gt;If you have built a small library providing some functionality, say a text classification library, it would be best if you package it and make it available for download via a manager such as &lt;strong&gt;pip&lt;/strong&gt; so that the package can be used directly in other projects.&lt;/p&gt;

&lt;p&gt;In any case, installation and run instructions should be documented elaborately in a README file.&lt;/p&gt;

&lt;h4 id=&#34;6-put-parameter-tuning-options-as-command-line-arguments&#34;&gt;6. Put parameter tuning options as command line arguments&lt;/h4&gt;

&lt;p&gt;In continuation with #5, the user should never be expected to open your training script to tune hyperparameters, or provide path to data directories, or other similar stuff. Python has the &lt;strong&gt;argparse&lt;/strong&gt; library which facilitates parsing command line arguments, and it is insanely simple to use. Bash has the parser available by default and the arguments can be accessed using the numbered variables $0, $1, and so on. Similar functionalities are available for almost every programming language.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import argparse

parser = argparse.ArgumentParser(description=&#39;Process some integers.&#39;)
parser.add_argument(&#39;integers&#39;, metavar=&#39;N&#39;, type=int, nargs=&#39;+&#39;,
                    help=&#39;an integer for the accumulator&#39;)
parser.add_argument(&#39;--sum&#39;, dest=&#39;accumulate&#39;, action=&#39;store_const&#39;,
                    const=sum, default=max,
                    help=&#39;sum the integers (default: find the max)&#39;)

args = parser.parse_args()
print(args.accumulate(args.integers))
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;7-have-a-defined-naming-convention-for-saved-models-and-result-files&#34;&gt;7. Have a defined naming convention for saved models and result files&lt;/h4&gt;

&lt;p&gt;Your saved model and result file names should indicate the important hyperparameters that were used in that instance of training. This simplifies testing with several available models later during analysis.&lt;/p&gt;

&lt;h4 id=&#34;8-get-your-code-reviewed-before-merging&#34;&gt;8. Get your code reviewed before merging&lt;/h4&gt;

&lt;p&gt;I can’t stress this enough. Regardless of how sincere you have been in your coding, your commits would still be flawed in some way. Having a reviewer always helps, even if it is to point out some pep8 naming convention.&lt;/p&gt;

&lt;p&gt;In my undergrad thesis project, I was the sole contributor, and so there used to be several weeks in which I didn’t push any code, arguing that it was all there in my local system anyway. I would think of Git as an additional time-consuming formality, instead of the immensely useful tool it is. Don’t make that mistake!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;I hope these guidelines are useful to some researcher who is just starting out on her first project. Yes, it would take some time to get used to following all these rules, but trust me, your research would only be the better for it in the long run!&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Introduction to Speech Recognition using WFSTs</title>
      <link>https://abhilashasancheti.github.io/post/intro-speech-recognition-wfst/</link>
      <pubDate>Mon, 23 Apr 2018 13:41:31 +0530</pubDate>
      
      <guid>https://abhilashasancheti.github.io/post/intro-speech-recognition-wfst/</guid>
      <description>

&lt;p&gt;Until now, all of my blog posts have been about deep learning methods or their application to NLP. Since the last couple of weeks, however, I have started learning about Automatic Speech Recognition (ASR)&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Therefore, I will also include speech-related articles in this publication now.&lt;/p&gt;

&lt;p&gt;The ASR logic is very simple (it’s just Bayes rule, like most other things in machine learning). Essentially, given a speech waveform, the objective is to transcribe it, i.e., identify a text which aligns with the waveform. Suppose $Y$ represents the feature vectors obtained from the waveform (Note: this “feature extraction” itself is an involved procedure, and I will describe it in detail in another post), and $\mathbf{w}$ denotes an arbitrary string of words. Then, we have the following.&lt;/p&gt;

&lt;p&gt;$$ \hat{\mathbf{w}} = \text{arg}\max_{\mathbf{w}} { P(\mathbf{w}|Y)} = \text{arg} \max_{\mathbf{w}} {P(Y|\mathbf{w})P(\mathbf{w}) } $$&lt;/p&gt;

&lt;p&gt;The two likelihoods in the term are trained separately. The first component, known as &lt;em&gt;acoustic modeling&lt;/em&gt;, is trained using a parallel corpus of utterances and speech waveforms. The second component, called &lt;em&gt;language modeling&lt;/em&gt;, is trained in an unsupervised fashion from a large corpus of text.&lt;/p&gt;

&lt;p&gt;Although the ASR training appears simple from this abstract level, the implementation is arguably more complex, and is usually done using Weighted Finite State Transducers (WFSTs). In this post, I’ll describe WFSTs, some of their basic algorithms, and give a brief introduction to how they are used for speech recognition.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;weighted-finite-state-transducers-wfsts&#34;&gt;Weighted Finite State Transducers (WFSTs)&lt;/h4&gt;

&lt;p&gt;If you have taken any Theory of Computation course before, you’d probably already be aware what an &lt;em&gt;automata&lt;/em&gt; is. Essentially, a finite automaton accepts a language (which is a set of strings). They are represented by directed graphs as shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/17/dag.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Each such automaton has a start state, one or more final states, and labeled edges connecting the states. A string is accepted if it ends in a final state after traversing through some path in the graph. For instance in the above DFA (deterministic finite automata), &lt;em&gt;a&lt;/em&gt;, &lt;em&gt;ac&lt;/em&gt;, and &lt;em&gt;ae&lt;/em&gt; are allowed.&lt;/p&gt;

&lt;p&gt;So an &lt;em&gt;acceptor&lt;/em&gt; maps any input string to a binary class {0,1} depending on whether or not the string is accepted. A &lt;em&gt;transducer&lt;/em&gt;, on the other hand, has 2 labels on each edge — an input label, and an output label. Furthermore, a &lt;em&gt;weighted&lt;/em&gt; finite state transducer has weights corresponding to each edge and every final state.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/17/wfst.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Therefore, a WFST is a mapping from a pair of strings to a weight sum. The pair is formed from the input/output labels along any path of the WFST. For pairs which are not possible in the graph, the corresponding weight is infinite.&lt;/p&gt;

&lt;p&gt;In practice, there are libraries available in every language to implement WFSTs. For C++, &lt;a href=&#34;http://www.openfst.org/twiki/bin/view/FST/WebHome&#34; target=&#34;_blank&#34;&gt;OpenFST&lt;/a&gt; is a popular library, which is also used in the &lt;a href=&#34;http://kaldi-asr.org/&#34; target=&#34;_blank&#34;&gt;Kaldi speech recognition toolkit&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In principle, it is possible to implement speech recognition algorithms without using WFSTs. However, these data structures have &lt;a href=&#34;https://cs.nyu.edu/~mohri/pub/csl01.pdf&#34; target=&#34;_blank&#34;&gt;several proven results&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; and algorithms which can directly be used in ASRs without having to worry about correctness and complexity. These advantages have made WFSTs almost omniscient in speech recognition. I’ll now summarize some algorithms on WFSTs.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;some-basic-algorithms-on-wfsts&#34;&gt;Some basic algorithms on WFSTs&lt;/h3&gt;

&lt;h4 id=&#34;composition&#34;&gt;Composition&lt;/h4&gt;

&lt;p&gt;Composition, as the name suggests, refers to the process of combining 2 WFSTs to form a single WFST. If we have transducers for pronunciation and word-level grammar, such an algorithm would enable us to form a phone-to-word level system easily.&lt;/p&gt;

&lt;p&gt;Composition is done using 3 rules:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Initial state in the new WFST are formed by combining the initial states of the old WFSTs into pairs.&lt;/li&gt;
&lt;li&gt;Similarly, final states are combined into pairs.&lt;/li&gt;
&lt;li&gt;For every pair of edges such that the o-label of the first WFST is the i-label of the second, we add an edge from the source pair to the destination pair. The edge weight is summed using the sum rules.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;An example of composition is shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/17/composition.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;At this point, it may be important to define what &amp;ldquo;sum&amp;rdquo; means for edge weights. Formally, the &amp;ldquo;languages&amp;rdquo; accepted by WFSTs are generalized through the notion of &lt;a href=&#34;https://en.wikipedia.org/wiki/Semiring&#34; target=&#34;_blank&#34;&gt;semirings&lt;/a&gt;. Basically, it is a set of elements with 2 operators, namely $\oplus$ and $\otimes$. Depending on the type of semiring, these operators can take on different definitions. For example, in a tropical semiring, $\oplus$ denotes &lt;strong&gt;$\min$&lt;/strong&gt;, and $\otimes$ denotes &lt;strong&gt;sum&lt;/strong&gt;. Furthermore, in any WFST, weights are $\otimes$-multiplied along paths (Note: here “multiplied” would mean summed for a tropical semiring) and $\oplus$-summed over paths with identical symbol sequence.&lt;/p&gt;

&lt;p&gt;See &lt;a href=&#34;http://www.openfst.org/twiki/bin/view/FST/ComposeDoc&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; for OpenFST implementation of composition.&lt;/p&gt;

&lt;h4 id=&#34;determinization&#34;&gt;Determinization&lt;/h4&gt;

&lt;p&gt;A deterministic automaton is one in which there is only one transition for each label in every state. By such a formulation, a deterministic WFST removes all redundancy and greatly reduces the complexity of the underlying grammar. But, are all WFSTs determinizable?&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The Twins Property:&lt;/em&gt; Let us consider an automaton A. Two states &lt;em&gt;p&lt;/em&gt; and &lt;em&gt;q&lt;/em&gt; in A are said to be siblings if both can be reached by string &lt;em&gt;x&lt;/em&gt; and both have cycles with label &lt;em&gt;y&lt;/em&gt;. Essentially, siblings are twins if the total weight for the paths until the states, as well as that including the cycle, are equal for both.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A WFST is determinizable if all its siblings are twins.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is an example of what I said earlier regarding WFSTs being an efficient implementation of the algorithms used in ASR. There are several methods to determinize a WFST. One such algorithm is shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/17/determinization.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In simpler steps, this algorithm does the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;At each state, for every outgoing label, if there are multiple outgoing edges for that label, replace them with a single edge with weight as the $\otimes$-sum of all edge weights containing that label.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since this is a local algorithm, it can be efficiently implemented in-memory. To see how to perform determinization in OpenFST, see &lt;a href=&#34;http://www.openfst.org/twiki/bin/view/FST/DeterminizeDoc&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&#34;minimization&#34;&gt;Minimization&lt;/h4&gt;

&lt;p&gt;Although minimization is not as essential as determinization, it is still a nice optimization technique. It refers to minimizing the number of states and transitions in a deterministic WFST.&lt;/p&gt;

&lt;p&gt;Minimization is carried out in 2 steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Weight pushing: All weights are pushed towards the start state. See the following example.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/17/pushing.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;After this is done, we combine those states which have identical paths to any final state. For example in the above WFST, states 1 and 2 have become identical after weight pushing, so they are combined into one state.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In OpenFST, the implementation details for minimization can be found &lt;a href=&#34;http://www.openfst.org/twiki/bin/view/FST/MinimizeDoc&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The following&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; shows the complete pipeline for a WFST reduction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/17/pipeline.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;wfsts-in-speech-recognition&#34;&gt;WFSTs in speech recognition&lt;/h4&gt;

&lt;p&gt;Several WFSTs are composed in sequence for use in speech recognition. These are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Grammar (&lt;strong&gt;G&lt;/strong&gt;): This is the language model trained on large text corpus.&lt;/li&gt;
&lt;li&gt;Lexicon (&lt;strong&gt;L&lt;/strong&gt;): This encodes information about the likelihood of phones without context.&lt;/li&gt;
&lt;li&gt;Context-dependent phonetics (&lt;strong&gt;C&lt;/strong&gt; ): This is similar to n-gram language modeling, except that it is for phones.&lt;/li&gt;
&lt;li&gt;HMM structure (&lt;strong&gt;H&lt;/strong&gt;): This is the model for the waveform.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In general, the composed transducer &lt;strong&gt;H&lt;/strong&gt;o&lt;strong&gt;C&lt;/strong&gt;o&lt;strong&gt;L&lt;/strong&gt;o&lt;strong&gt;G&lt;/strong&gt; represents the entire pipeline of speech recognition. Each of the components can individually be improved, so that the entire ASR system gets improved.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;This was just a brief introduction to WFSTs which are an important component in ASR systems. In further posts on speech, I hope to discuss things such as feature extraction, popular GMM-HMM models, and latest deep learning advances. I am also reading papers mentioned &lt;a href=&#34;http://jrmeyer.github.io/asr/2017/04/05/seminal-asr-papers.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; to get a good overview of how ASR has progressed over the years.&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Gales, Mark, and Steve Young. &amp;ldquo;The application of hidden Markov models in speech recognition.&amp;rdquo; Foundations and Trends in Signal Processing 1.3 (2008): 195–304.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Mohri, Mehryar, Fernando Pereira, and Michael Riley. &amp;ldquo;Weighted finite-state transducers in speech recognition.&amp;rdquo; Computer Speech &amp;amp; Language 16.1 (2002): 69–88.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;&lt;a href=&#34;https://wiki.eecs.yorku.ca/course_archive/2011-12/W/6328/_media/wfst-tutorial.pdf&#34; target=&#34;_blank&#34;&gt;Lecture slides&lt;/a&gt; from Prof. Hui Jiang (York University)
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How to Obtain Sentence Vectors</title>
      <link>https://abhilashasancheti.github.io/post/how-to-obtain-sentence-vectors/</link>
      <pubDate>Thu, 12 Apr 2018 13:41:14 +0530</pubDate>
      
      <guid>https://abhilashasancheti.github.io/post/how-to-obtain-sentence-vectors/</guid>
      <description>

&lt;p&gt;In several of my previous posts, I have discussed methods for obtaining word embeddings, such as SVD, word2vec, or GloVe. In this post, I will abstract a level higher and talk about 4 different methods that have been proposed to get embeddings for sentences.&lt;/p&gt;

&lt;p&gt;But first, some of you may ask why do we even need a different method for obtaining sentence vectors. Since sentences are essentially made up of words, it may be reasonable to argue that simply taking the sum or the average of the constituent word vectors should give a decent sentence representation. This is akin to a bag-of-words representation, and hence suffers from the same limitations, i.e.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It ignores the order of words in the sentence.&lt;/li&gt;
&lt;li&gt;It ignores the sentence semantics completely.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Other word vector based approaches are also similarly constrained. For instance, a weighted average technique again loses word order within the sentence. To remedy this issue, &lt;a href=&#34;https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf&#34; target=&#34;_blank&#34;&gt;Socher et al.&lt;/a&gt; combined the words in the order given by the parse tree of the sentence. While this technique may be suitable for complete sentences, it does not work for phrases or paragraphs.&lt;/p&gt;

&lt;p&gt;In an earlier &lt;a href=&#34;https://abhilashasancheti.github.io/post/last-3-years-in-text-classification/&#34; target=&#34;_blank&#34;&gt;post&lt;/a&gt;, I discussed several ways in which sentence representations are obtained as an intermediate step during text classification. Several approaches are used for this purpose, such as character to sentence level feature encoding, parse trees, regional (two-view) embeddings, and so on. However, the limitation with such an &amp;ldquo;intermediate&amp;rdquo; representation is that the vectors obtained are not generic in that they are closely tied to the classification objective. As such, vectors obtained through training on one objective may not be extrapolated for other tasks.&lt;/p&gt;

&lt;p&gt;In light of this discussion, I will now describe 4 recent methods that have been proposed to obtain general sentence vectors. Note that each of these belongs to either of 2 categories: (i) inter-sentence, wherein the vector of one sentence depends on its surrounding sentences, and (ii) intra-sentence, where a sentence vector only depends on that particular sentence in isolation.&lt;/p&gt;

&lt;h4 id=&#34;paragraph-vectors&#34;&gt;Paragraph Vectors&lt;/h4&gt;

&lt;p&gt;In this &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v32/le14.pdf&#34; target=&#34;_blank&#34;&gt;ICML’14 paper&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; from Mikolov (who also invented &lt;em&gt;word2vec&lt;/em&gt;), the authors propose the following solution: a sentence vector can be learned simply by assigning an index to each sentence, and then treating the index like any other word. This is shown in the following figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/16/doc2vec.png&#34; alt=&#34;Paragraph vectors model. Figure taken from paper&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Essentially, every paragraph (or sentence) is mapped to a unique vector, and the combined paragraph and word vectors are used to predict the next word. Through such a training, the paragraph vectors may start storing missing information, thus acting like a memory for the paragraph. For this reason, this method is called the Distributed Memory model (PV-DM).&lt;/p&gt;

&lt;p&gt;To obtain the embeddings for an unknown sentence, an inference step needs to be performed. A new column of randomly initialized values is added to the sentence embedding matrix. The inference step is performed keeping all the other parameters fixed to obtain the required vector.&lt;/p&gt;

&lt;p&gt;The PV-DM model requires a large amount of storage space since the paragraph vectors are concatenated with all the vectors in the context window at every training step. To solve this, the authors propose another model, called the Distributed BOW (PV-DBOW), which predicts random words in the context window. The downside is that this model does not use word order, and hence performs worse than PV-DM.&lt;/p&gt;

&lt;h4 id=&#34;skip-thoughts&#34;&gt;Skip-thoughts&lt;/h4&gt;

&lt;p&gt;While PV was an intra-sentence model, &lt;a href=&#34;https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf&#34; target=&#34;_blank&#34;&gt;skip-thoughts&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; is inter-sentence. The method uses continuity of text to predict the next sentence from the given sentence. This also solves the problem of the inference step that is present in the PV model. If you have read about the skip-gram algorithm in word2vec, skip-thoughts is essentially the same technique abstracted to the sentence level.&lt;/p&gt;

&lt;p&gt;In the paper, the authors propose an encoder-decoder framework for training, with an RNN used for both encoding and decoding. In addition to a sentence embedding matrix, this method also generates vectors for the words in the corpus vocabulary. Finally, the objective function to be maximized is as follows.&lt;/p&gt;

&lt;p&gt;$$ \sum_t \log P(w_{i+1}^t|w_{i+1}^{&amp;lt; t},\mathbf{h}_i) + \sum_t \log P(w_{i-1}^t|w_{i-1}^{&amp;lt; t},\mathbf{h}_i) $$&lt;/p&gt;

&lt;p&gt;Here, the indices $i+1$ and $i-1$ represent the next sentence and the previous sentence, respectively. Overall, the function represents the sum of log probabilities of correctly predicting the next sentence and the previous sentence, given the current sentence.&lt;/p&gt;

&lt;p&gt;Since word vectors are also precited at training time, a problem may arise at the time of inference if the new sentence contains an OOV word. To solve this, the authors present a simple solution for vocabulary expansion. We assume that any word, even if it is OOV, will definitely come from some vector space (say w2v), such that we have its vector representation in that space. As such, every known word has 2 representations, one in the RNN space and another in the w2v space. We can then identify a linear transformation matrix that transforms w2v space vectors into RNN space vectors, and this matrix may be used to obtain the RNN vectors for OOV words.&lt;/p&gt;

&lt;h4 id=&#34;fastsent&#34;&gt;FastSent&lt;/h4&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1602.03483.pdf&#34; target=&#34;_blank&#34;&gt;This model&lt;/a&gt;, proposed by Kyunghun Cho&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;, is also an inter-sentence technique, and is conceptually very similar to skip-thoughts. The only difference is that it uses a BOW representation of the sentence to predict the surrounding sentences, which makes it computationally much more efficient than skip-thoughts. The training hypothesis remains the same, i.e., rich sentence semantics can be inferred from the content of adjacent sentences. Since the details of the method are same as skip-thoughts, I will not repeat them here to avoid redundancy.&lt;/p&gt;

&lt;h4 id=&#34;sequential-denoising-autoencoders-sdae&#34;&gt;Sequential Denoising Autoencoders (SDAE)&lt;/h4&gt;

&lt;p&gt;This technique was also proposed in the &lt;a href=&#34;https://arxiv.org/pdf/1602.03483.pdf&#34; target=&#34;_blank&#34;&gt;same paper&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; as FastSent. However, it is essentially an intra-sentence method wherein the objective is to regenerate a sentence from a noisy version.&lt;/p&gt;

&lt;p&gt;In essence, in an SDAE, a high-dimensional input data is corrupted according to some noise function and the model is trained to recover the original data from the corrputed version.&lt;/p&gt;

&lt;p&gt;In the paper, the noise function $N$ uses 2 parameters as follows.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;For each word $w$ in the sentence $S$, $N$ deletes it according to some probability $p_0$.&lt;/li&gt;
&lt;li&gt;For each non-overlapping bigram in $S$, $N$ swaps the bigram tokens with probability $p_x$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These are inspired from the “word dropout” and “debagging” approaches, respectively, which have earlier been studied in some detail.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In the last paper&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;, the authors have performed detailed empirical evaluations of several sentence vector methods, including all of the above. From this analysis, the following observations can be drawn,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Task-dependency:&lt;/strong&gt; Although the methods intend to produce general sentence representations which work well across different tasks, it is found that some methods are more suitable from some tasks due to the inherent algorithm. For instance, skip-thoughts perform well on textual entailment tasks, whereas SDAEs perform much better on paraphrase detection.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inter vs. intra:&lt;/strong&gt; The inter-sentence models generate similar vectors in that their nearest neighbors are those sentences which have shared concepts. In contrast, for the intra-sentence models, these are sentences which have more overlapping words.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dependency on word order:&lt;/strong&gt; Although the widely held view is that word order is critical for sentence vectors, the average score for models which are sensitive to word order was found to be almost equal to those which are not. It was even lower for RNN models in unsupervised objectives, which is indeed surprising. One explanation for this may be that the sentences in the dataset, or the evaluation techniques, are not robust enough so as to sufficiently challenge simple word frequency based techniques.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Le, Quoc, and Tomas Mikolov. “Distributed representations of sentences and documents.” International Conference on Machine Learning. 2014.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Kiros, Ryan, et al. “Skip-thought vectors.” Advances in neural information processing systems. 2015.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Hill, Felix, Kyunghyun Cho, and Anna Korhonen. “Learning distributed representations of sentences from unlabelled data.” arXiv preprint arXiv:1602.03483 (2016).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Irony detection in tweets</title>
      <link>https://abhilashasancheti.github.io/project/irony-tweet/</link>
      <pubDate>Tue, 20 Mar 2018 17:00:16 +0530</pubDate>
      
      <guid>https://abhilashasancheti.github.io/project/irony-tweet/</guid>
      <description>&lt;p&gt;The task was to recognize whether a tweet has irony or not - binary classification. In essence, we identified 2 aspects that were essential to identify irony in tweets:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Semantic interaction between text and hashtags, modeled using &lt;a href=&#34;https://arxiv.org/pdf/1510.04935.pdf&#34; target=&#34;_blank&#34;&gt;holographic embeddings&lt;/a&gt; (or circular cross-correlations).&lt;/li&gt;
&lt;li&gt;World knowledge about irony in text, obtained through transfer learning from &lt;a href=&#34;https://deepmoji.mit.edu/&#34; target=&#34;_blank&#34;&gt;DeepMoji&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We were able to obtain a validation accuracy of 69%, although the model performed poorly in the final test phase. The code for the project is available &lt;a href=&#34;https://github.com/abhilashasancheti/tweet-irony-detection&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Online Learning of Word Embeddings</title>
      <link>https://abhilashasancheti.github.io/post/online-learning-word-embeddings/</link>
      <pubDate>Wed, 14 Mar 2018 13:40:57 +0530</pubDate>
      
      <guid>https://abhilashasancheti.github.io/post/online-learning-word-embeddings/</guid>
      <description>

&lt;p&gt;Word vectors have become the building blocks for all natural language processing systems. I have earlier written an overview of popular algorithms for learning word embeddings &lt;a href=&#34;https://abhilashasancheti.github.io/post/understanding-word-vectors/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. One limitation with all these methods (namely SVD, skip-gram, and GloVe) is that they are all “batch” techniques. In this post, I will discuss two recent papers (which are very similar but were developed independently) which aim to provide an online approximation for the skip-gram algorithm.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;But first, what do we mean by a “batch” algorithm?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Simply put, in a batch algorithm, the entire data set needs to be available before we begin the processing. In contrast, an “online” algorithm can process inputs on-the-fly, i.e., in a streaming fashion. Needless to say, such algorithms are also preferable when the available resources are not sufficient to process the entire dataset at once.&lt;/p&gt;

&lt;p&gt;Now that we have some idea about batch algorithms, I’ll explain why the existing methods for word representation learning are of this kind. First, in the case of the standard SVD and Stanford’s GloVe, the entire cooccurence matrix needs to be computed, and only then can the processing be started. If some additional data arrives later, the matrix would have to be recomputed, and training would have to be restarted (if at least one of the updates depends on a changed matrix element). Second, in the case of Mikolov’s &lt;em&gt;word2vec&lt;/em&gt; (skip-gram and CBOW), negative sampling is often used to make the computation more efficient. This sampling depends on the unigram probability distribution of the vocabulary words in the corpus. As such, before learning can happen, we need to compute the vocabulary as well as the unigram distribution.&lt;/p&gt;

&lt;p&gt;Recently, two very similar methods (developed independently) have been proposed to make the skip-gram with negative sampling (SGNS) algorithm learn in a streaming fashion. I’ll quickly review the SGNS algorithm first so that there is some context when we discuss the papers.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;batch-sgns-algorithm&#34;&gt;Batch SGNS algorithm&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/15/skipgram.png&#34; alt=&#34;Skip-gram objective. Image taken from [The Morning Paper](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/).&#34; /&gt;&lt;/p&gt;

&lt;p&gt;SGNS is a window-based method with the following training objective: Given the target word, predict all the context words in the window.&lt;/p&gt;

&lt;p&gt;Suppose we have a context window where $w$ is the target word and $c$ is one of the context words. Then, skip-gram’s objective is to compute $P(c|w)$, which is given as&lt;/p&gt;

&lt;p&gt;$$ p(c|w;\theta) = \frac{\exp(v_c \cdot v_w)}{\sum_{c^{\prime}\in C}\exp(v_{c^{\prime}}\cdot v_w)} $$&lt;/p&gt;

&lt;p&gt;Basically, it is just a softmax probability distribution over all the word-context pairs in the corpus, directed by the cosine similarity. However, the denominator term here is very expensive to compute since there may be a very large number of possible context words. To solve this problem, negative sampling is used.&lt;/p&gt;

&lt;p&gt;Goldberg and Levy have explained the derivation for the objective function in SGNS very clearly in their &lt;a href=&#34;https://arxiv.org/pdf/1402.3722.pdf&#34; target=&#34;_blank&#34;&gt;note&lt;/a&gt;. I will try to provide a little intuition here.&lt;/p&gt;

&lt;p&gt;For the word $w$, we are trying to predict the context word $c$. Since we are using softmax, this is essentially like a multi-class classification problem, where we are trying to classify the next word into one of $N$ classes (where $N$ is the number of words in the dictionary). Since $N$ may be quite large, this is a very difficult problem.&lt;/p&gt;

&lt;p&gt;What SGNS does is that it converts this multi-classification problem into binary classification. The new objective is to predict, for any given word-context pair $(w,c)$, whether the pair is in the window or not. For this, we try to increase the probability of a &amp;ldquo;positive&amp;rdquo; pair $(w,c)$, while at the same time reducing the probability of $k$ randomly chosen &amp;ldquo;negative samples&amp;rdquo; $(w,s)$ where $s$ is a word not found in $w$’s context. This leads to the following objective function which we try to maximize in SGNS:&lt;/p&gt;

&lt;p&gt;$$ J = \log \sigma(c\cdot w) + \sum_{i=1}^k \mathbb{E}_{w_i \sim p(w)}[\log \sigma (-w_i \cdot w)]  $$&lt;/p&gt;

&lt;p&gt;In other words, we push the target vector in the direction of the positive context vector, and pull it away from $k$ randomly chosen (w.r.t. the unigram probability distribution) negative vectors. Here &amp;ldquo;negative&amp;rdquo; means that these vectors are not actually present in the target’s context.&lt;/p&gt;

&lt;h4 id=&#34;what-do-we-need-to-make-sgns-online&#34;&gt;What do we need to make SGNS online?&lt;/h4&gt;

&lt;p&gt;As is evident from the above discussion, since SGNS is a window-based approach, the training itself is very much in an online paradigm. However, the constraints are in creating a vocabulary and a unigram distribution for negative sampling, which makes SGNS a two-pass method. Further, if additional data is seen later, the distribution and vocabulary would change, and the model would have to be retrained.&lt;/p&gt;

&lt;p&gt;Essentially, we need online alternatives for 2 aspects of the algorithms:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Dynamic vocabulary building&lt;/li&gt;
&lt;li&gt;Adaptive unigram distribution&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With this background, I will now discuss the two proposed methods for online SGNS.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;space-saving-word2vec&#34;&gt;Space-Saving word2vec&lt;/h3&gt;

&lt;p&gt;In &lt;a href=&#34;https://arxiv.org/pdf/1704.07463.pdf&#34; target=&#34;_blank&#34;&gt;this paper&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; from researchers at Johns Hopkins, the following solutions were proposed for the two problems mentioned above.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Space-saving algorithm for dynamic vocabulary building.&lt;/li&gt;
&lt;li&gt;Reservoir sampling for adaptive unigram distribution.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Space-saving algorithm:&lt;/strong&gt; It is a popular method to estimate the top-$k$ most frequent items in a streaming data.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We declare a structure V containing $k$ pairs of word and their counts, and initialize it to empty pairs.&lt;/li&gt;
&lt;li&gt;As word $w$ arrives, if $w \in V$, we increment its count.&lt;/li&gt;
&lt;li&gt;Otherwise, if $V$ has space, we append the pair $(w,1)$ to $V$.&lt;/li&gt;
&lt;li&gt;If not, the word with the lowest count is replaced by $w$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At any instant, the words in the structure V denote the dynamic vocabulary of the corpus.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reservoir sampling:&lt;/strong&gt; Reservoir sampling is a family of randomized algorithms for randomly choosing a sample of $k$ items from a list S containing $n$ items, where $n$ is either a very large or unknown number. (Wikipedia)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Similar to the SS algorithm, we declare a structure (called the reservoir) of $k$ empty elements (not pairs this time). In addition, we initialize a counter $c$ to 0.&lt;/li&gt;
&lt;li&gt;The first $k$ elements in the stream are filled into the reservoir. $c$ is incremented at every occurence.&lt;/li&gt;
&lt;li&gt;For the remaining items, we draw $j$ from $1,\ldots,c$ randomly. If $j &amp;lt; k$, the $j^{\text{th}}$ element of the reservoir is replaced with the new element.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At any instant, the samples present in the reservoir provide an approximate distribution of items in the entire data stream.&lt;/p&gt;

&lt;p&gt;While the algorithm itslelf is conceptually simple, the authors have mentioned several implementation choices which are important for training SGNS online. I list them here with some observations:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;When a word is ejected from a bin in the dynamic vocabulary, its embeddings are re-initialized. As such, every bin has its own learning rate which is reset when the word in the bin is changed.&lt;/li&gt;
&lt;li&gt;During sentence subsampling, all words not in $W$ are retained. Those in $W$ are retained with a probability which is inversely proportional to the square root of its count in the dictionary.&lt;/li&gt;
&lt;li&gt;Probably the most important deviation from the SGNS algorithm is that the reservoir sampling essentially generates an empirical distribution from which to sample negative context words. In contrast, in the original SGNS algorithm, a &lt;em&gt;smoothed&lt;/em&gt; empirical distribution is used. The authors have themselves allowed that “ smoothing the negative sampling distribution was (sic) shown to increase word embedding quality consistently.”&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;incremental-sgns&#34;&gt;Incremental SGNS&lt;/h3&gt;

&lt;p&gt;This &lt;a href=&#34;http://aclweb.org/anthology/D17-1037&#34; target=&#34;_blank&#34;&gt;EMNLP’17 paper&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; from researchers at Yahoo Japan proposes the following alternative solutions for the aforementioned problems.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Misra-Gries algorithm for dynamic vocabulary building.&lt;/li&gt;
&lt;li&gt;A modified reservoir sampling algorithm for adaptive unigram table.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Misra-Gries algorithm:&lt;/strong&gt; This was developed long before the space-saving algorithm (1982) and was the go-to technique for top-$k$ most frequent itemset estimation in streaming data, before the space-saving algorithm was developed. The method is very similar to SS except for one difference:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;When word $w$ is not in $V$ and there is no space to append, every element in $V$ is decremented until some element becomes 0, at which point it is replaced by the new word.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Modified reservoir sampling:&lt;/strong&gt; Here is the pseudocode from the paper.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/15/reservoir.png&#34; alt=&#34;Modified reservoir sampling. Image taken from original paper&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This algorithm differs from the conventional Reservoir Sampling in two important ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The counts used here are &lt;em&gt;smoothed&lt;/em&gt; (see line 4 to 6). This has been shown to be important for word vector quality, as discussed above.&lt;/li&gt;
&lt;li&gt;If the reservoir does not have enough space, we iterate over all existing words and replace them with some probability (which is proportional to the smoothed count of $w$). Contrast this with the earlier technique, where a $j$ was randomly sampled and word at that index was replaced. (&lt;strong&gt;Disclaimer&lt;/strong&gt;: &lt;em&gt;I am not sure how exactly this modification helps in learning. If I am allowed to venture a guess, I would say that it is a “soft” equivalent of the hard replacement in the original algorithm. This probably helps in the theoretical analysis of the algorithm.&lt;/em&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In addition, the authors have also provided theoretical justification for their algorithm and proved the following theorem: &lt;em&gt;The loss in case of incremental SGNS converges in probability to that of batch SGNS.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In summary, SGNS is probably the easiest batch word embedding algorithm to “streamify” because of its inherent window-based nature. The constraints of vocabulary and counts are addressed with approximation algorithms. I can think of several possible directions in which this work can be continued.&lt;/p&gt;

&lt;p&gt;First, there are several algorithms for estimating the top-$k$ most frequent items in a data stream. These are divided into count-based and sketch-based methods. The SS algorithm is probably the most efficient count-based technique, but it may be useful to look at other methods to see if they provide some edge. (Although I’m pretty sure the JHU researchers would have been thorough in their
selection of the algorithm.)&lt;/p&gt;

&lt;p&gt;Second, GloVe and SVD are yet to be addressed. In case of GloVe in particular, the problem would be to construct the co-occurence matrix in a online fashion. There should be some related work in statistics which can be leveraged for this, but I haven’t conducted much literature survey in this direction.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;May, Chandler, Kevin Duh, Benjamin Van Durme, and Ashwin Lall. &amp;ldquo;&lt;em&gt;Streaming word embeddings with the space-saving algorithm.&lt;/em&gt;&amp;rdquo; arXiv preprint arXiv:1704.07463 (2017).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Kaji, Nobuhiro, and Hayato Kobayashi. &amp;ldquo;&lt;em&gt;Incremental skip-gram model with negative sampling.&lt;/em&gt;&amp;rdquo; arXiv preprint arXiv:1704.03956 (2017).*
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Sparsity in Online Learning with Lasso Regularization</title>
      <link>https://abhilashasancheti.github.io/post/sparse-online-learning-lasso-regularization/</link>
      <pubDate>Sat, 24 Feb 2018 13:40:42 +0530</pubDate>
      
      <guid>https://abhilashasancheti.github.io/post/sparse-online-learning-lasso-regularization/</guid>
      <description>

&lt;p&gt;Sparse vectors have become popular recently for 2 reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Sparse matrices require much less storage since they can be stored using various space-saving methods.&lt;/li&gt;
&lt;li&gt;Sparse vectors are much more interpretable than dense vectors. For instance, the non-zero non-negative components of a sparse word vector may be taken to denote the weights for certain features. In contrast, there is no interpretation for a value like $-0.1347$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Sparsity is often induced through the use of L1 (or Lasso) regularization. There are 2 formulations of the Lasso: (i) convex constraint, and (ii) soft regularization.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Convex constraint&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As the name suggests, a convex constraint is added to the minimization problem so that the parameters do not exceed a certain value.&lt;/p&gt;

&lt;p&gt;$$ \min_{\beta \in \mathbb{R}^p}\lVert y - X\beta \rVert_2^2 \quad \text{s.t.} \quad \lVert \beta \rVert_1 \leq t $$&lt;/p&gt;

&lt;p&gt;The smaller the value of the tuning parameter $t$, fewer is the number of non-zero components in the solution.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Soft regularization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is just the Lagrange form the the convex constraint, and is used because it is easier to optimize. Note that it is equivalent to the convex constraint formulation for an appropriately chosen $g$.&lt;/p&gt;

&lt;p&gt;$$ \min_{\beta \in \mathbb{R}^p}\lVert y - X\beta \rVert_2^2 + g\lVert \beta \rVert_1 $$&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;There is a great theoretical explanation of sparsity with Lasso regularization by &lt;a href=&#34;http://www.stat.cmu.edu/~ryantibs/&#34; target=&#34;_blank&#34;&gt;Ryan Tibshirani&lt;/a&gt; and &lt;a href=&#34;http://www.stat.cmu.edu/~larry/&#34; target=&#34;_blank&#34;&gt;Larry Wasserman&lt;/a&gt; which you can find &lt;a href=&#34;http://www.stat.cmu.edu/~larry/=sml/sparsity.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. I will instead be focusing on some methods that have been introduced recently for inducing sparsity while learning online i.e., when the samples are obtained one at a time. In addition to such a scenario, online learning also comes into the picture when the data set is simply too large to be loaded in memory at once, and there are not sufficient resources for performing batch learning in a parallel fashion.&lt;/p&gt;

&lt;p&gt;In this post, I will summarize 3 such methods:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.jmlr.org/papers/volume10/langford09a/langford09a.pdf&#34; target=&#34;_blank&#34;&gt;Stochastic Truncated Gradient&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.jmlr.org/papers/volume10/duchi09a/duchi09a.pdf&#34; target=&#34;_blank&#34;&gt;Forward Backward Splitting&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.mendeley.com/viewer/?fileId=00e458de-d9ca-a697-5d67-a4c177759778&amp;amp;documentId=0e9eba78-0cbb-3cb2-a8ea-385a2afb64f5&#34; target=&#34;_blank&#34;&gt;Regularized Dual Averaging&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;But first, why a simple soft Lasso regularization won’t work? With the soft regularization method, we are essentially summing up 2 floating point values. As such, it is highly improbable that the sum will be zero, since very few pairs of floats add up to zero.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;stochastic-truncated-gradient-stg&#34;&gt;Stochastic Truncated Gradient (STG)&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://abhilashasancheti.github.io/img/14/stg.png&#34; alt=&#34;Simple round-off (T0) vs. Truncated Gradient (T1). Image taken from paper&#34; /&gt;&lt;/p&gt;

&lt;p&gt;STG combines ideas from 2 simple techniques:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Coefficient rounding&lt;/em&gt;: In this method, the coefficients are rounded to 0 if they are less than a value $\theta$. This is denoted in the figure above (left graph). The rounding is done after every $k$ steps. The problem with this approach is that if $k$ is small, the coefficients do not get an opportunity to reach a value above $\theta$ before they are pulled back to $0$. On the other hand, if $k$ is large, the intermediate steps in the algorithm need to store a large number of non-zero coefficients, which does not solve the storage issue.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Sub-gradient method&lt;/em&gt;: In this method, L1-regularization is performed by shifting the update in the opposite direction depending on the sign of the coefficient. The update equation is&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;$$ f(w_i) = w_i - \eta\nabla_1 L(w_i,z_i) - \eta g \text{sgn}(w_i) $$&lt;/p&gt;

&lt;p&gt;STG combines &lt;em&gt;rounding&lt;/em&gt; from (1) and &lt;em&gt;gravity&lt;/em&gt; from (2) so that (i) sparsity is achieved (unlike the sub-gradient method), and (ii) the rounding off is not too aggressive (unlike the direct rounding approach). The parameter update is then given by the function $T_1$ (shown in the right graph above).&lt;/p&gt;

&lt;p&gt;$$ T_1(v_j,\alpha,\theta) = \begin{cases} \max(0,v_j-\alpha) \quad &amp;amp;\text{if}~ v_j \in [0,\theta] \\\ \min(0,v_j+\alpha) \quad &amp;amp;\text{if}~ v_j \in [-\theta,0] \\\ 0 \quad &amp;amp;\text{otherwise}   \end{cases} $$&lt;/p&gt;

&lt;p&gt;The update rule is given using $T_1$ as&lt;/p&gt;

&lt;p&gt;$$ f(w_i) = T_1 (w_i - \nabla_1 L_1 (w_i,z_i,\eta g_i,\theta)) $$&lt;/p&gt;

&lt;p&gt;Here, $g$ may be called the gravity parameter, and $\theta$ is called the truncation parameter. In general, the larger these parameters are, the more sparsity is incurred. This can be understood easily from the definition of the truncation function.&lt;/p&gt;

&lt;p&gt;Furthermore, note that on setting $\theta = \infty$ in the truncation function yields a special case of the Sub-gradient method wherein &lt;strong&gt;max&lt;/strong&gt; and &lt;strong&gt;min&lt;/strong&gt; operations are performed after applying gravity pull.&lt;/p&gt;

&lt;p&gt;In the remainder of the paper, the authors prove a strong regret bound for the STG method, and also provide an efficient implementation for the same. Furthermore, they show the asymptotic solution of one instance of the algorithm is essentially equivalent to the Lasso regression, thus justifying the algorithm’s ability to produce sparse weight vectors when the number of features is intractably large.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;forward-backward-splitting-fobos&#34;&gt;Forward Backward Splitting (FOBOS)&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;Note: The method was named Forward Looking Subgradient (FOLOS) in the first draft and later renamed since it was essentially the same as an earlier proposed technique, the Forward Backward Splitting. The authors abbreviated it to FOBOS instead of FOBAS to avoid confusing readers of the first draft.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;First, a little background. Consider an objective function of the form $f(w) + r(w)$. In the case of a number of machine learning algorithms, the function $f$ denotes the empirical sum of some loss function (such as mean squared error), and the function $r$ is a regularizer (such as Lasso). If we use a simple gradient descent technique to minimize this objective function, the iterates would be of the form&lt;/p&gt;

&lt;p&gt;$$ w_{t+1} = w_t - \eta_t g_t^f - \eta_t g_t^r $$&lt;/p&gt;

&lt;p&gt;where the $g$’s are vectors from the subgradient sets of the corresponding functions. From the paper:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A common problem in subgradient methods is that if $r$ or $f$ is non-differentiable, the iterates of the subgradient method are very rarely at the points of non-differentiability. In the case of the Lasso regularization function, however, these points are often the true minima of the function.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In other words, the subgradient approach will result in neither a true minima nor a sparse solution if $r$ is the L1 regularizer.&lt;/p&gt;

&lt;p&gt;FOBOS, as the name suggests, splits every iteration into 2 steps — a forward step and a backward step, instead of minimizing both $f$ and $r$ simultaneously. The motivation for the method is that for L1 regularization functions, true minima is usually attained at the points of non-differentiability. For example, in the 2-D space, the function resembles a Diamond shape and the minima is obtained at one of the corner points. Each iteration of FOBOS consists of the following 2 steps:&lt;/p&gt;

&lt;p&gt;$$ w_{t+\frac{1}{2}} = w_t - \eta_t g_t^f \\\ w_{t+1} = \text{argmin}_w { \frac{1}{2}(w_t - w_{t+\frac{1}{2}})^2 + \eta_{t+\frac{1}{2}}r(w) } $$&lt;/p&gt;

&lt;p&gt;The first step is a simple unconstrained subgradient step with respect to the function $f$. In the second step, we try to achieve 2 objectives:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Stay close to the interim update vector. This is achieved by the first term.&lt;/li&gt;
&lt;li&gt;Attain a low complexity value as expressed by $r$. (Second term)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So the first step is a &lt;em&gt;forward&lt;/em&gt; step, where we update the coefficient in the direction of the subgradient, while the second is a &lt;em&gt;backward&lt;/em&gt; step where we pull the update back a little so as to obtain sparsity by moving in the direction of the non-differentiable points of $r$.&lt;/p&gt;

&lt;p&gt;Using the first equation in the second, taking derivative w.r.t $w$, and equating the derivative to $0$, we obtain the update scheme as&lt;/p&gt;

&lt;p&gt;$$ w_{t+1} = w_t - \eta_t g_t^f + \eta_{t+\frac{1}{2}} g_{t+1}^r $$&lt;/p&gt;

&lt;p&gt;(&lt;strong&gt;Note&lt;/strong&gt;: The equation above looks suspiciously similar to the &lt;strong&gt;&lt;em&gt;Nesterov Accelerated Gradient (NAG)&lt;/em&gt;&lt;/strong&gt; method for optimization. The authors have even cited Nesterov’s paper in related work. It might be interesting to  investigate this further.)&lt;/p&gt;

&lt;p&gt;This update scheme has 2 major advantages, according to the author.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;First, from an algorithmic standpoint, it enables sparse solutions at virtually no additional computational cost. Second, the forward-looking gradient allows us to build on existing analyses and show that the resulting framework enjoys the formal convergence properties of many existing gradient-based and online convex programming algorithms.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the paper, the authors also prove convergence of the method and show that on setting the intermediate learning rate properly, low regret bounds can be proved for both online as well as batch settings.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;regularized-dual-averaging-rda&#34;&gt;Regularized Dual Averaging (RDA)&lt;/h4&gt;

&lt;p&gt;Both of the above discussed techniques have one limitation — they perform updates depending only on the subgradients at a particular time step. In contrast, the RDA method “exploits the full regularization structure at each iteration.” Also, since the authors derive closed-form solutions for several popular optimization objectives, it follows that the computational complexity of such an approach is not worse than the methods which perform updates only based on current subgradients (both being $\mathcal{O}(n)$).&lt;/p&gt;

&lt;p&gt;RDA comprises of 3 steps in every iteration.&lt;/p&gt;

&lt;p&gt;In the first step, the subgradient is computed for that particular time step. This is the same as every other subgradient-based online optimization method.&lt;/p&gt;

&lt;p&gt;The second step consists of computing a running average of all past subgradients. This is done using the online approach as&lt;/p&gt;

&lt;p&gt;$$ \bar{g}_t = \frac{t-1}{t}\bar{g}_{t-1} + \frac{1}{t}g_t $$&lt;/p&gt;

&lt;p&gt;In the third step, the update is computed as&lt;/p&gt;

&lt;p&gt;$$ w_{t+1} = \text{argmin}_w { &amp;lt;\bar{g}_t,w&amp;gt; + \psi(w) + \frac{\beta}{t}h(w) } $$&lt;/p&gt;

&lt;p&gt;Let us try to understand this update scheme. First, the function $h(w)$ is a strongly convex function such that the update vector which minimizes it also minimizes the regularizer. In the case of Lasso regularization, $h(w)$ is chosen as follows.&lt;/p&gt;

&lt;p&gt;$$ h(w) = \frac{1}{2}\lVert w \rVert_2^2 + \rho \lVert w \rVert_1 $$&lt;/p&gt;

&lt;p&gt;where $\rho$ is a parameter called the sparsity enhancing parameter. $\beta$ is a predetermined non-negative and non-decreasing sequence.&lt;/p&gt;

&lt;p&gt;Now to solve the equation, we can just take the derivative of the argument of argmin and equate it to $0$. On solving this equation, we get an update of the form&lt;/p&gt;

&lt;p&gt;$$ w_{t+1} = \frac{t}{\beta_t}(\bar{g}_t + \rho) $$&lt;/p&gt;

&lt;p&gt;So the scheme ensures that the update is in the same convex space as the regularized dual average. Sparsity can further be controlled by tuning the value of the parameter $\rho$. The scaling factor can be regulated using the
non-decreasing sequence selected at the beginning of the algorithm. For the case when it is equal to the time step $t$, the new coefficient is simply the sum of the dual average and the sparsity parameter.&lt;/p&gt;

&lt;p&gt;The above is just my attempt at understanding the update scheme for RDA. I would be happy to discuss it further if you find something wrong with this explanation.&lt;/p&gt;

&lt;p&gt;Now the method itself would become extremely infeasible if this differentiation would have to be performed for every iteration. However, for most commonly used regularizers and loss functions, the update rule can be represented with a closed-form solution. For this reason, the overall algorithm has the same complexity as earlier algorithms which use only the current step subgradient for performing updates.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Langford, John, Lihong Li, and Tong Zhang. “Sparse online learning via truncated gradient.” &lt;em&gt;Journal of Machine Learning Research&lt;/em&gt; 10.Mar (2009): 777–801.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Duchi, John, and Yoram Singer. “Efficient online and batch learning using forward backward splitting.” &lt;em&gt;Journal of Machine Learning Research&lt;/em&gt; 10.Dec (2009): 2899–2934.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Xiao, Lin. “Dual averaging methods for regularized stochastic learning and online optimization.” &lt;em&gt;Journal of Machine Learning Research&lt;/em&gt; 11.Oct (2010): 2543–2596.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
